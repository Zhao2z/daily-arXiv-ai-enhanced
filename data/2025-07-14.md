<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 10]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [The State of Computational Science in Fission and Fusion Energy](https://arxiv.org/abs/2507.08061)
*Andrea Morales Coto,Aditi Verma*

Main category: cs.SE

TL;DR: 调查显示，核工程领域的计算科学家更倾向于使用现代编程语言、开源代码和模块化软件，预示着未来5到10年的行业趋势。


<details>
  <summary>Details</summary>
Motivation: 研究核工程中软件工具的重要性及其对工程设计的影响。

Method: 对103位从事核聚变和核裂变能源代码开发的科学家进行调查，了解其工具使用和开发体验。

Result: 趋势包括转向现代语言（如Python、C++）、模块化软件和多物理场代码，开发预算显著增加。

Conclusion: 核工程代码未来将更模块化、计算量更小，并受到更多组织重视。

Abstract: The tools used to engineer something are just as important as the thing that
is actually being engineered. In fact, in many cases, the tools can indeed
determine what is engineerable. In fusion and fission1 energy engineering,
software has become the dominant tool for design. For that reason, in 2024, for
the first time ever, we asked 103 computational scientists developing the codes
used in fusion and fission energy about the problems they are attempting to
solve with their codes, the tools available to them to solve them, and their
end to end developer experience with said tools.
  The results revealed a changing tide in software tools in fusion and fission,
with more and more computational scientists preferring modern programming
languages, open-source codes, and modular software. These trends represent a
peek into what will happen 5 to 10 years in the future of nuclear engineering.
Since the majority of our respondents belonged to US national labs and
universities, these results hint at the most cutting-edge trends in the
industry. The insights included in the State of Computational Science in
Fission and Fusion Energy indicate a dramatic shift toward multiphysics codes,
a drop-off in the use of FORTRAN in favor of more modern languages like Python
and C++, and ever-rising budgets for code development, at times reaching $50M
in a single organization.
  Our survey paints a future of nuclear engineering codes that is modular in
nature, small in terms of compute, and increasingly prioritized by
organizations. Access to our results in web form are available online.

</details>


### [2] [Code with Me or for Me? How Increasing AI Automation Transforms Developer Workflows](https://arxiv.org/abs/2507.08149)
*Valerie Chen,Ameet Talwalkar,Robert Brennan,Graham Neubig*

Main category: cs.SE

TL;DR: 研究探讨了开发者与自动化编码代理的互动，发现其潜力超越传统copilot，但也面临用户理解不足等挑战。


<details>
  <summary>Details</summary>
Motivation: 评估更自主的AI工具对开发者生产力和体验的影响，填补现有研究空白。

Method: 通过比较GitHub Copilot和OpenHands，招募常用前者的开发者进行实验。

Result: 代理能完成传统copilot无法完成的任务，减少用户工作量，但需解决用户理解问题。

Conclusion: 研究为开发者工作流变化提供见解，并为未来代理设计提出建议。

Abstract: Developers now have access to a growing array of increasingly autonomous AI
tools to support software development. While numerous studies have examined
developer use of copilots, which can provide chat assistance or code
completions, evaluations of coding agents, which can automatically write files
and run code, still largely rely on static benchmarks without
humans-in-the-loop. In this work, we conduct the first academic study to
explore developer interactions with coding agents and characterize how more
autonomous AI tools affect user productivity and experience, compared to
existing copilots. We evaluate two leading copilot and agentic coding
assistants, GitHub Copilot and OpenHands, recruiting participants who regularly
use the former. Our results show agents have the potential to assist developers
in ways that surpass copilots (e.g., completing tasks that humans might not
have accomplished before) and reduce the user effort required to complete
tasks. However, there are challenges involved in enabling their broader
adoption, including how to ensure users have an adequate understanding of agent
behaviors. Our results not only provide insights into how developer workflows
change as a result of coding agents but also highlight how user interactions
with agents differ from those with existing copilots, motivating a set of
recommendations for researchers building new agents. Given the broad set of
developers who still largely rely on copilot-like systems, our work highlights
key challenges of adopting more agentic systems into developer workflows.

</details>


### [3] [The Impact of Generative AI on Code Expertise Models: An Exploratory Study](https://arxiv.org/abs/2507.08160)
*Otávio Cury,Guilherme Avelino*

Main category: cs.SE

TL;DR: 论文探讨了生成式AI工具在代码生成中对开发者理解力的潜在影响，并通过模拟实验分析了其对代码知识模型和Truck Factor算法的敏感性。


<details>
  <summary>Details</summary>
Motivation: 生成式AI工具虽提升开发效率，但可能导致开发者对生成代码的理解不足，进而影响代码知识模型的准确性。

Method: 通过收集ChatGPT生成代码在GitHub项目中的统计数据，并模拟不同GenAI贡献程度的情景。

Result: 多数情景下，GenAI的使用对当前专业知识指标产生了可测量的影响。

Conclusion: 随着GenAI在开发流程中的深入应用，现有专业知识指标的可靠性可能下降。

Abstract: Generative Artificial Intelligence (GenAI) tools for source code generation
have significantly boosted productivity in software development. However, they
also raise concerns, particularly the risk that developers may rely heavily on
these tools, reducing their understanding of the generated code. We hypothesize
that this loss of understanding may be reflected in source code knowledge
models, which are used to identify developer expertise. In this work, we
present an exploratory analysis of how a knowledge model and a Truck Factor
algorithm built upon it can be affected by GenAI usage. To investigate this, we
collected statistical data on the integration of ChatGPT-generated code into
GitHub projects and simulated various scenarios by adjusting the degree of
GenAI contribution. Our findings reveal that most scenarios led to measurable
impacts, indicating the sensitivity of current expertise metrics. This suggests
that as GenAI becomes more integrated into development workflows, the
reliability of such metrics may decrease.

</details>


### [4] [Leveraging Large Language Models for Classifying App Users' Feedback](https://arxiv.org/abs/2507.08250)
*Yasaman Abedini,Abbas Heydarnoori*

Main category: cs.SE

TL;DR: 该论文研究了利用四种先进LLM（如GPT-3.5-Turbo、GPT-4等）提升用户反馈分类能力，解决标注数据不足的问题，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有监督学习方法依赖大量标注数据，但标注成本高，因此探索LLM在用户反馈分类中的潜力。

Method: 在八个标注数据集上测试四种LLM的分类性能，并利用LLM生成标注数据以增强BERT模型的训练。

Result: LLM在粗粒度分类中表现良好，且通过LLM标注数据增强训练集能显著提升分类器性能。

Conclusion: LLM可作为有效的标注工具，提升用户反馈分类性能，尤其在数据有限的情况下。

Abstract: In recent years, significant research has been conducted into classifying
application (app) user feedback, primarily relying on supervised machine
learning algorithms. However, fine-tuning more generalizable classifiers based
on existing labeled datasets remains an important challenge, as creating large
and accurately labeled datasets often requires considerable time and resources.
In this paper, we evaluate the capabilities of four advanced LLMs, including
GPT-3.5-Turbo, GPT-4, Flan-T5, and Llama3-70b, to enhance user feedback
classification and address the challenge of the limited labeled dataset. To
achieve this, we conduct several experiments on eight datasets that have been
meticulously labeled in prior research. These datasets include user reviews
from app stores, posts from the X platform, and discussions from the public
forums, widely recognized as representative sources of app user feedback. We
analyze the performance of various LLMs in identifying both fine-grained and
coarse-grained user feedback categories. Given the substantial volume of daily
user feedback and the computational limitations of LLMs, we leverage these
models as an annotation tool to augment labeled datasets with general and
app-specific data. This augmentation aims to enhance the performance of
state-of-the-art BERT-based classification models. Our findings indicate that
LLMs when guided by well-crafted prompts, can effectively classify user
feedback into coarse-grained categories. Moreover, augmenting the training
dataset with datasets labeled using the consensus of LLMs can significantly
enhance classifier performance.

</details>


### [5] [Computing Floating-Point Errors by Injecting Perturbations](https://arxiv.org/abs/2507.08467)
*Youshuai Tan,Zhanwei Zhang,Jinfu Chen,Zishuo Ding,Jifeng Xuan,Weiyi Shang*

Main category: cs.SE

TL;DR: PI-detector是一种新方法，用于高效准确地计算浮点程序中的误差，解决了现有工具ATOMU和FPCC的局限性。


<details>
  <summary>Details</summary>
Motivation: 浮点程序广泛应用于关键领域，但浮点误差可能导致严重后果。现有方法存在实现困难和执行时间长的问题，ATOMU有误报，FPCC速度慢。

Method: PI-detector通过观察浮点误差源于原子操作的大条件数，通过向操作数注入小扰动并比较结果来计算误差。

Result: 实验表明，PI-detector能高效准确地计算浮点误差。

Conclusion: PI-detector解决了现有工具的不足，为浮点误差检测提供了更优方案。

Abstract: Floating-point programs form the foundation of modern science and
engineering, providing the essential computational framework for a wide range
of applications, such as safety-critical systems, aerospace engineering, and
financial analysis. Floating-point errors can lead to severe consequences.
Although floating-point errors widely exist, only a subset of inputs may
trigger significant errors in floating-point programs. Therefore, it is crucial
to determine whether a given input could produce such errors. Researchers tend
to take the results of high-precision floating-point programs as oracles for
detecting floating-point errors, which introduces two main limitations: (1)
difficulty of implementation and (2) prolonged execution time. The two recent
tools, ATOMU and FPCC, can partially address these issues. However, ATOMU
suffers from false positives; while FPCC, though eliminating false positives,
operates at a considerably slower speed.
  To address these two challenges, we propose a novel approach named
PI-detector to computing floating-point errors effectively and efficiently. Our
approach is based on the observation that floating-point errors stem from large
condition numbers in atomic operations (such as addition and subtraction),
which then propagate and accumulate. PI-detector injects small perturbations
into the operands of individual atomic operations within the program and
compares the outcomes of the original program with the perturbed version to
compute floating-point errors. We evaluate PI-detector with datasets from ATOMU
and HSED, as well as a complex linear system-solving program. Experimental
results demonstrate that PI-detector can perform efficient and accurate
floating-point error computation.

</details>


### [6] [InferLog: Accelerating LLM Inference for Online Log Parsing via ICL-oriented Prefix Caching](https://arxiv.org/abs/2507.08523)
*Yilun Wang,Pengfei Chen,Haiyu Huang,Zilong He,Gou Tan,Chuanfu Zhang,Jingkai He,Zibin Zheng*

Main category: cs.SE

TL;DR: InferLog是一种针对在线日志解析的LLM推理优化方法，通过改进前缀缓存效率和动态配置调优，显著提升性能而不影响准确性。


<details>
  <summary>Details</summary>
Motivation: 现代软件系统生成大量运行时日志，需要高效准确的日志解析。现有LLM解析器因隐私和性能问题难以部署，InferLog旨在解决这些瓶颈。

Method: 设计前缀感知ICL优化策略和基于元学习的动态配置调优管道，提升LLM推理效率。

Result: 实验显示InferLog在Loghub数据集和vLLM上显著优于现有方法，加速了日志解析。

Conclusion: InferLog通过优化推理效率，解决了LLM在日志解析中的性能瓶颈，为实际部署提供了可行方案。

Abstract: Modern software systems generate massive volumes of runtime logs,
necessitating efficient and accurate log parsing to enable critical downstream
tasks such as anomaly detection and root cause analysis. Recently, large
language models (LLMs) have achieved advanced accuracy on log parsing, but
their deployment in production environments faces two major limitations: (1)
the privacy risks associated with commercial LLMs, driving the adoption of
local deployment, and (2) the stringent latency and throughput requirements
imposed by high-volume log streams, which existing LLM-based parsers fail to
meet. Although recent efforts have reduced the number of LLM queries, they
overlook the high latency of the LLM invocations, where concurrent log parsing
requests can cause serve performance degradation of LLM inference system.
  In this study, we present InferLog, the first LLM inference optimization
method for online log parsing. Our key insight is that the inference efficiency
emerges as the vital bottleneck in LLM-based online log parsing, rather than
parsing accuracy. InferLog accelerates inference by designing (1) A
Prefix-aware ICL Refinement policy to refine the examples and permutation of
in-context learning to improve the prefix caching efficiency. (2) A rapid and
task-specific configuration tuning pipeline based on meta-learning to find the
optimal LLM scheduling-related configuration for dynamic log parsing workloads.
The experimental results based on Loghub dataset and vLLM demonstrate that
InferLog significantly outperforms existing inference optimization methods and
markedly accelerates the state-of-the-art LLM-based log parser without
compromising parsing accuracy.

</details>


### [7] [Generating Proto-Personas through Prompt Engineering: A Case Study on Efficiency, Effectiveness and Empathy](https://arxiv.org/abs/2507.08594)
*Fernando Ayach,Vitor Lameirão,Raul Leão,Jerfferson Felizardo,Rafael Sobrinho,Vanessa Borges,Patrícia Matsubara,Awdren Fontão*

Main category: cs.SE

TL;DR: 提出了一种基于提示工程的生成式AI方法，用于自动生成原型人物角色，提高了效率和质量，但存在泛化和领域特异性问题。


<details>
  <summary>Details</summary>
Motivation: 解决手动创建原型人物角色的耗时、认知负担和偏见问题。

Method: 采用提示工程和生成式AI生成原型人物角色，并通过案例研究评估其效率、效果和用户接受度。

Result: 方法显著减少了时间和精力，提高了人物角色的质量和可重用性，但泛化和领域特异性仍有局限。

Conclusion: 生成式AI可有效支持产品发现实践，但需进一步解决泛化和情感共鸣问题。

Abstract: Proto-personas are commonly used during early-stage Product Discovery, such
as Lean Inception, to guide product definition and stakeholder alignment.
However, the manual creation of proto-personas is often time-consuming,
cognitively demanding, and prone to bias. In this paper, we propose and
empirically investigate a prompt engineering-based approach to generate
proto-personas with the support of Generative AI (GenAI). Our goal is to
evaluate the approach in terms of efficiency, effectiveness, user acceptance,
and the empathy elicited by the generated personas. We conducted a case study
with 19 participants embedded in a real Lean Inception, employing a qualitative
and quantitative methods design. The results reveal the approach's efficiency
by reducing time and effort and improving the quality and reusability of
personas in later discovery phases, such as Minimum Viable Product (MVP)
scoping and feature refinement. While acceptance was generally high, especially
regarding perceived usefulness and ease of use, participants noted limitations
related to generalization and domain specificity. Furthermore, although
cognitive empathy was strongly supported, affective and behavioral empathy
varied significantly across participants. These results contribute novel
empirical evidence on how GenAI can be effectively integrated into software
Product Discovery practices, while also identifying key challenges to be
addressed in future iterations of such hybrid design processes.

</details>


### [8] [NL in the Middle: Code Translation with LLMs and Intermediate Representations](https://arxiv.org/abs/2507.08627)
*Chi-en Amy Tai,Pengyu Nie,Lukasz Golab,Alexander Wong*

Main category: cs.SE

TL;DR: 研究发现，大语言模型（LLMs）在代码翻译中存在错误。通过自然语言（NL）和抽象语法树（ASTs）作为中间表示，结合链式思考（CoT）提示，能显著提升翻译准确性。


<details>
  <summary>Details</summary>
Motivation: 改进LLMs在代码翻译中的准确性，探索中间表示（如NL和ASTs）的作用。

Method: 使用Open Gpt4 8X7B、StarCoder和CodeGen模型，在CodeNet和AVATAR基准测试中，比较不同提示方法（从零样本到CoT）的效果。

Result: CoT结合NL中间表示效果最佳，Open Gpt4 8X7B的成功翻译率分别提高了13.8%和6.7%。

Conclusion: 中间表示（尤其是NL）结合CoT提示能有效提升LLMs的代码翻译性能。

Abstract: Studies show that large language models (LLMs) produce buggy code
translations. One avenue to improve translation accuracy is through
intermediate representations, which could provide structured insights to guide
the model's understanding. We explore whether code translation using LLMs can
benefit from intermediate representations via natural language (NL) and
abstract syntax trees (ASTs). Since prompt engineering greatly affects LLM
performance, we consider several ways to integrate these representations, from
one-shot to chain-of-thought (CoT) prompting. Using Open Gpt4 8X7B and
specialized StarCoder and CodeGen models on popular code translation benchmarks
(CodeNet and AVATAR), we find that CoT with an intermediate NL summary performs
best, with an increase of 13.8% and 6.7%, respectively, in successful
translations for the best-performing model (Open Gpt4 8X7B) compared to the
zero-shot prompt.

</details>


### [9] [LLMCup: Ranking-Enhanced Comment Updating with LLMs](https://arxiv.org/abs/2507.08671)
*Hua Ge,Juan Zhai,Minxue Pan,Fusen He,Ziyue Tan*

Main category: cs.SE

TL;DR: LLMCup是一个基于大型语言模型（LLM）的注释更新框架，通过多提示策略生成候选注释，并使用排名模型选择最佳注释，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 开发者倾向于更新代码而非注释，导致注释过时或不一致，影响代码理解和维护。现有方法（如CUP和HebCup）在复杂场景下表现不佳。

Method: 提出LLMCup框架，利用LLM生成多样候选注释，并通过排名模型CupRank选择最佳注释。

Result: 实验显示LLMCup在准确性、BLEU-4、METEOR、F1和SentenceBert相似度上显著优于基线方法，部分情况下甚至优于人工更新。

Conclusion: LLMCup展示了LLM在注释更新任务中的潜力，并强调了人工评估的重要性。

Abstract: While comments are essential for enhancing code readability and
maintainability in modern software projects, developers are often motivated to
update code but not comments, leading to outdated or inconsistent documentation
that hinders future understanding and maintenance. Recent approaches such as
CUP and HebCup have attempted automatic comment updating using neural
sequence-to-sequence models and heuristic rules, respectively. However, these
methods can miss or misinterpret crucial information during comment updating,
resulting in inaccurate comments, and they often struggle with complex update
scenarios. Given these challenges, a promising direction lies in leveraging
large language models (LLMs), which have shown impressive performance in
software engineering tasks such as comment generation, code synthesis, and
program repair. This suggests their strong potential to capture the logic
behind code modifications - an ability that is crucial for the task of comment
updating. Nevertheless, selecting an appropriate prompt strategy for an LLM on
each update case remains challenging. To address this, we propose a novel
comment updating framework, LLMCup, which first uses multiple prompt strategies
to provide diverse candidate updated comments via an LLM, and then employs a
ranking model, CupRank, to select the best candidate as final updated comment.
Experimental results demonstrate the effectiveness of LLMCup, with improvements
over state-of-the-art baselines (CUP and HebCup) by 49.0%-116.9% in Accuracy,
10.8%-20% in BLEU-4, 4.6% in METEOR, 0.9%-1.9% in F1, and 2.1%-3.4% in
SentenceBert similarity. Furthermore, a user study shows that comments updated
by LLMCup sometimes surpass human-written updates, highlighting the importance
of incorporating human evaluation in comment quality assessment.

</details>


### [10] [Dually Hierarchical Drift Adaptation for Online Configuration Performance Learning](https://arxiv.org/abs/2507.08730)
*Zezhen Xiang,Jingzhi Gong,Tao Chen*

Main category: cs.SE

TL;DR: DHDA是一个在线配置性能学习框架，通过双重层次适应机制处理动态环境中的全局和局部概念漂移，显著提高了准确性和适应性。


<details>
  <summary>Details</summary>
Motivation: 动态环境中的配置性能学习因全局和局部概念漂移而变得复杂，现有离线学习和迁移学习方法难以实时适应这些变化。

Method: DHDA采用双重层次适应机制：上层重新划分数据以处理全局漂移，下层局部模型异步检测和适应局部漂移，结合增量更新和定期全重训练以平衡效率。

Result: 在八个软件系统上的评估显示，DHDA准确性显著提升，适应漂移能力提高2倍，同时保持合理开销。

Conclusion: DHDA有效解决了动态环境中配置性能学习的挑战，为实时适应概念漂移提供了可行方案。

Abstract: Modern configurable software systems need to learn models that correlate
configuration and performance. However, when the system operates in dynamic
environments, the workload variations, hardware changes, and system updates
will inevitably introduce concept drifts at different levels - global drifts,
which reshape the performance landscape of the entire configuration space; and
local drifts, which only affect certain sub-regions of that space. As such,
existing offline and transfer learning approaches can struggle to adapt to
these implicit and unpredictable changes in real-time, rendering configuration
performance learning challenging. To address this, we propose DHDA, an online
configuration performance learning framework designed to capture and adapt to
these drifts at different levels. The key idea is that DHDA adapts to both the
local and global drifts using dually hierarchical adaptation: at the upper
level, we redivide the data into different divisions, within each of which the
local model is retrained, to handle global drifts only when necessary. At the
lower level, the local models of the divisions can detect local drifts and
adapt themselves asynchronously. To balance responsiveness and efficiency, DHDA
combines incremental updates with periodic full retraining to minimize
redundant computation when no drifts are detected. Through evaluating eight
software systems and against state-of-the-art approaches, we show that DHDA
achieves considerably better accuracy and can effectively adapt to drifts with
up to 2x improvements, while incurring reasonable overhead and is able to
improve different local models in handling concept drift.

</details>
