<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 16]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [AlgoTune: Can Language Models Speed Up General-Purpose Numerical Programs?](https://arxiv.org/abs/2507.15887)
*Ori Press,Brandon Amos,Haoyu Zhao,Yikai Wu,Samuel K. Ainsworth,Dominik Krupke,Patrick Kidger,Touqir Sajed,Bartolomeo Stellato,Jisun Park,Nathanael Bosch,Eli Meril,Albert Steppi,Arman Zharmagambetov,Fangzhao Zhang,David Perez-Pineiro,Alberto Mercurio,Ni Zhan,Talor Abramovich,Kilian Lieret,Hanlin Zhang,Shirley Huang,Matthias Bethge,Ofir Press*

Main category: cs.SE

TL;DR: 研究者提出了AlgoTune基准测试，评估语言模型设计和实现算法的能力，发现当前模型虽能实现1.72倍的性能提升，但缺乏算法创新能力，主要依赖表面优化。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型评估主要集中在人类已解决的任务上，缺乏对模型在开放性算法设计和实现方面创新能力的测试，需要评估模型是否能超越现有人类表现并展现创造性问题解决能力。

Method: 构建了包含155个编码任务的AlgoTune基准测试，任务来自领域专家，涵盖计算机科学、物理学和数学等领域；开发了验证和计时框架来评估语言模型生成的解决方案代码；创建了基线语言模型代理AlgoTuner并在多个前沿模型上进行评估。

Result: AlgoTuner相比使用SciPy、sk-learn和CVXPY等库的参考求解器平均实现了1.72倍的加速；但发现当前模型无法发现算法创新，更倾向于进行表面层次的优化而非深层算法改进。

Conclusion: 虽然语言模型在算法实现效率上有所提升，但在算法创新方面仍存在显著不足，希望AlgoTune基准能够推动开发出具有超越人类最先进表现的创造性问题解决能力的语言模型代理。

Abstract: Despite progress in language model (LM) capabilities, evaluations have thus
far focused on models' performance on tasks that humans have previously solved,
including in programming (Jimenez et al., 2024) and mathematics (Glazer et al.,
2024). We therefore propose testing models' ability to design and implement
algorithms in an open-ended benchmark: We task LMs with writing code that
efficiently solves computationally challenging problems in computer science,
physics, and mathematics. Our AlgoTune benchmark consists of 155 coding tasks
collected from domain experts and a framework for validating and timing
LM-synthesized solution code, which is compared to reference implementations
from popular open-source packages. In addition, we develop a baseline LM agent,
AlgoTuner, and evaluate its performance across a suite of frontier models.
AlgoTuner achieves an average 1.72x speedup against our reference solvers,
which use libraries such as SciPy, sk-learn and CVXPY. However, we find that
current models fail to discover algorithmic innovations, instead preferring
surface-level optimizations. We hope that AlgoTune catalyzes the development of
LM agents exhibiting creative problem solving beyond state-of-the-art human
performance.

</details>


### [2] [Dr. Boot: Bootstrapping Program Synthesis Language Models to Perform Repairing](https://arxiv.org/abs/2507.15889)
*Noah van der Vleuten*

Main category: cs.SE

TL;DR: 该论文提出了一种用于程序合成的自举算法，通过教授模型如何修复代码来解决现有编程数据集规模有限和语言模型与人类编程过程不匹配的问题。实验表明自举方法持续优于常规微调，且修复训练还能提升非修复性能。


<details>
  <summary>Details</summary>
Motivation: 现有程序合成语言模型面临两个主要问题：1）训练和评估数据集（MBPP、APPS）规模有限且质量不高，而语言模型需要大量数据；2）模型的程序合成过程与人类不匹配，人类通过编译器迭代开发代码，而大多数模型一次性生成代码。

Method: 提出了一种支持教授模型修复能力的程序合成自举算法。该方法通过自举过程训练模型学会修复代码，使其更接近人类的迭代编程过程。

Result: 自举方法持续优于常规微调；与其他工作相比，自举模型的性能与大68%的微调模型相当；带修复的自举训练还能提升推理时的非修复性能；但在推理时进行修复可能不如简单地采样相同数量的解决方案；发现APPS数据集训练部分的示例测试用例存在问题。

Conclusion: 自举算法是改进程序合成模型的有效方法，特别是结合修复能力的训练。虽然修复训练能提升整体性能，但推理时的修复策略仍需优化。此外，现有数据集的质量问题需要社区关注，特别是对依赖测试用例的修复和强化学习方法。

Abstract: Language models for program synthesis are usually trained and evaluated on
programming competition datasets (MBPP, APPS). However, these datasets are
limited in size and quality, while these language models are extremely data
hungry. Additionally, the language models have a misaligned program synthesis
process compared to humans. While humans iteratively develop code with the help
of a compiler, most program synthesis models currently produce code in one go.
To solve these issues, we introduce a bootstrapping algorithm for program
synthesis, that supports teaching models how to repair. We show that
bootstrapping consistently outperforms regular fine-tuning. Compared to other
work, our bootstrapped model performs on par with fine-tuned models that are
68\% larger. Notably, bootstrapping with repairing also improves non-repairing
performance compared to regular bootstrapping during inference. However, on our
models, repairing during inference is likely inferior to simply sampling the
same number of solutions. Furthermore, we find that there are issues with the
example test cases in the training portion of the APPS dataset that are
valuable to the community, as many repairing and reinforcement learning methods
rely on them.

</details>


### [3] [StaAgent: An Agentic Framework for Testing Static Analyzers](https://arxiv.org/abs/2507.15892)
*Elijah Nnorom,Md Basim Uddin Ahmed,Jiho Shin,Hung Viet Pham,Song Wang*

Main category: cs.SE

TL;DR: 本文提出了StaAgent，一个基于大语言模型的多智能体框架，用于系统性评估静态分析器规则的正确性。该框架包含四个专门的智能体，通过变异测试发现静态分析器实现中的缺陷，在五个主流静态分析器中发现了64个问题规则。


<details>
  <summary>Details</summary>
Motivation: 静态分析器在软件开发中起关键作用，但其规则实现往往测试不足且容易出现不一致性问题。现有方法难以系统性地评估静态分析器规则的正确性，需要一种可扩展且适应性强的解决方案来提高静态分析器的可靠性。

Method: 提出StaAgent多智能体框架，包含四个专门智能体：1）种子生成智能体：将漏洞检测规则转换为具体的包含漏洞的种子程序；2）代码验证智能体：确保种子程序的正确性；3）变异生成智能体：产生语义等价的变异体；4）分析器评估智能体：通过比较静态分析器在种子程序及其变异体上的行为进行变异测试，揭示不一致的行为模式。

Result: 使用五个最先进的大语言模型在五个广泛使用的静态分析器上进行评估，发现了64个问题规则（SpotBugs 28个，SonarQube 18个，ErrorProne 6个，Infer 4个，PMD 8个）。其中53个缺陷无法被现有最先进基线方法检测到。已向开发者报告所有缺陷，2个已修复，3个已确认，其余正在等待回复。

Conclusion: 实验结果证明了该方法的有效性，展示了基于智能体和大语言模型驱动的数据合成在推进软件工程方面的潜力。StaAgent为提高静态分析器可靠性提供了一种可扩展且适应性强的解决方案，为软件质量保证领域带来了新的技术突破。

Abstract: Static analyzers play a critical role in identifying bugs early in the
software development lifecycle, but their rule implementations are often
under-tested and prone to inconsistencies. To address this, we propose
StaAgent, an agentic framework that harnesses the generative capabilities of
Large Language Models (LLMs) to systematically evaluate static analyzer rules.
StaAgent comprises four specialized agents: a Seed Generation Agent that
translates bug detection rules into concrete, bug-inducing seed programs; a
Code Validation Agent that ensures the correctness of these seeds; a Mutation
Generation Agent that produces semantically equivalent mutants; and an Analyzer
Evaluation Agent that performs metamorphic testing by comparing the static
analyzer's behavior on seeds and their corresponding mutants. By revealing
inconsistent behaviors, StaAgent helps uncover flaws in rule implementations.
This LLM-driven, multi-agent framework offers a scalable and adaptable solution
to improve the reliability of static analyzers. We evaluated StaAgent with five
state-of-the-art LLMs (CodeL-lama, DeepSeek, Codestral, Qwen, and GPT-4o)
across five widely used static analyzers (SpotBugs, SonarQube, ErrorProne,
Infer, and PMD). The experimental results show that our approach can help
reveal 64 problematic rules in the latest versions of these five static
analyzers (i.e., 28 in SpotBugs, 18 in SonarQube, 6 in ErrorProne, 4 in Infer,
and 8 in PMD). In addition, 53 out of the 64 bugs cannot be detected by the
SOTA baseline. We have reported all the bugs to developers, with two of them
already fixed. Three more have been confirmed by developers, while the rest are
awaiting response. These results demonstrate the effectiveness of our approach
and underscore the promise of agentic, LLM-driven data synthesis to advance
software engineering.

</details>


### [4] [A Pilot Study on LLM-Based Agentic Translation from Android to iOS: Pitfalls and Insights](https://arxiv.org/abs/2507.16037)
*Zhili Zeng,Kimya Khakzad Shahandashti,Alvine Boaye Belle,Song Wang,Zhen Ming,Jiang*

Main category: cs.SE

TL;DR: 本研究评估了基于大语言模型的智能体方法在移动应用跨平台翻译中的性能，通过开发一个考虑依赖关系、规范、程序结构和控制流的智能体链，实现Android到iOS应用的自动化翻译，并分析了翻译失败的根本原因以提出改进指导方针。


<details>
  <summary>Details</summary>
Motivation: 传统的移动应用翻译方法依赖人工干预或基于规则的系统，耗时且劳动密集。虽然机器学习方法提供了自动化解决方案，但缺乏上下文理解和适应性。尽管大语言模型在代码翻译方面已有应用，但跨平台应用翻译（如Android与iOS之间的迁移）仍未得到充分探索，需要深入了解LLM在此领域的性能、优势和局限性。

Method: 开发了一个智能体链(chain of agents)，在将应用从Android翻译到iOS时考虑依赖关系、规范、程序结构和程序控制流。通过手动检查翻译代码的语法正确性、语义准确性和功能完整性来评估性能。对翻译失败案例进行详细的根本原因分析，以理解智能体翻译过程的潜在局限性。

Result: 通过评估基于LLM的智能体方法在移动应用翻译中的表现，识别了关键的失败点，并对翻译失败进行了根本原因分析。研究揭示了智能体翻译过程的潜在局限性，为改进翻译性能提供了见解。

Conclusion: 本研究填补了LLM在跨平台应用翻译领域的研究空白，通过系统评估基于智能体的翻译方法，识别了关键挑战并提出了改进指导方针。这为推进软件工程自动化，特别是移动应用跨平台迁移提供了重要参考，有助于开发更有效的自动化翻译工具和方法。

Abstract: The rapid advancement of mobile applications has led to a significant demand
for cross-platform compatibility, particularly between the Android and iOS
platforms. Traditional approaches to mobile application translation often rely
on manual intervention or rule-based systems, which are labor-intensive and
time-consuming. While recent advancements in machine learning have introduced
automated methods, they often lack contextual understanding and adaptability,
resulting in suboptimal translations. Large Language Models (LLMs) were
recently leveraged to enhance code translation at different granularities,
including the method, class, and repository levels. Researchers have
investigated common errors, limitations, and potential strategies to improve
these tasks. However, LLM-based application translation across different
platforms, such as migrating mobile applications between Android and iOS or
adapting software across diverse frameworks, remains underexplored.
Understanding the performance, strengths, and limitations of LLMs in
cross-platform application translation is critical for advancing software
engineering automation. This study aims to fill this gap by evaluating
LLM-based agentic approaches for mobile application translation, identifying
key failure points, and proposing guidelines to improve translation
performance. We developed a chain of agents that account for dependencies,
specifications, program structure, and program control flow when translating
applications from Android to iOS. To evaluate the performance, we manually
examined the translated code for syntactic correctness, semantic accuracy, and
functional completeness. For translation failures, we further conducted a
detailed root cause analysis to understand the underlying limitations of the
agentic translation process and identify opportunities for improvement.

</details>


### [5] [Making REST APIs Agent-Ready: From OpenAPI to Model Context Protocol Servers for Tool-Augmented LLMs](https://arxiv.org/abs/2507.16044)
*Meriem Mastouri,Emna Ksontini,Wael Kessentini*

Main category: cs.SE

TL;DR: 本文提出AutoMCP编译器，能够从OpenAPI规范自动生成MCP服务器，解决了手动构建MCP服务器繁琐重复的问题，在50个真实API的5066个端点测试中达到99.9%的成功率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型正从被动文本生成器转向主动调用外部工具的智能体，需要可扩展的工具集成协议。虽然Anthropic的模型上下文协议(MCP)提供了动态工具发现和调用的标准，但构建MCP服务器仍需大量手动工作，包括编写胶水代码、处理认证和手动配置模式，这与MCP旨在消除集成工作的目标相矛盾。

Method: 研究团队首先分析了MCP的采用趋势，发现22000+个MCP标记的GitHub仓库中只有不到5%包含服务器。随后提出AutoMCP编译器，该编译器能够解析OpenAPI 2.0/3.0规范中的REST API定义，自动生成完整的MCP服务器实现，包括模式注册和认证处理。

Result: 在50个真实世界API的5066个端点测试中，从1023个分层抽样的工具调用中，76.5%开箱即用成功。通过手动故障分析发现5个重复出现的问题，都归因于OpenAPI合约的不一致或遗漏。经过平均每个API 19行规范修改的轻微修复后，AutoMCP达到了99.9%的成功率。

Conclusion: 研究证明了尽管OpenAPI规范存在质量问题，但仍能实现近乎完全的MCP服务器自动化。该工作不仅分析了MCP采用情况并量化了手动服务器开发成本，还贡献了包含5066个可调用工具的语料库以及修复常见规范缺陷的见解，为LLM工具集成的自动化提供了有效解决方案。

Abstract: Large Language Models (LLMs) are evolving from passive text generators into
active agents that invoke external tools. To support this shift, scalable
protocols for tool integration are essential. The Model Context Protocol (MCP),
introduced by Anthropic in 2024, offers a schema-driven standard for dynamic
tool discovery and invocation. Yet, building MCP servers remains manual and
repetitive, requiring developers to write glue code, handle authentication, and
configure schemas by hand-replicating much of the integration effort MCP aims
to eliminate.
  This paper investigates whether MCP server construction can be meaningfully
automated. We begin by analyzing adoption trends: among 22,000+ MCP-tagged
GitHub repositories created within six months of release, fewer than 5% include
servers, typically small, single-maintainer projects dominated by repetitive
scaffolding. To address this gap, we present AutoMCP, a compiler that generates
MCP servers from OpenAPI 2.0/3.0 specifications. AutoMCP parses REST API
definitions and produces complete server implementations, including schema
registration and authentication handling.
  We evaluate AutoMCP on 50 real-world APIs spanning 5,066 endpoints across
over 10 domains. From a stratified sample of 1,023 tool calls, 76.5% succeeded
out of the box. Manual failure analysis revealed five recurring issues, all
attributable to inconsistencies or omissions in the OpenAPI contracts. After
minor fixes, averaging 19 lines of spec changes per API, AutoMCP achieved 99.9%
success.
  Our findings (i) analyze MCP adoption and quantify the cost of manual server
development, (ii) demonstrate that OpenAPI specifications, despite quality
issues, enable near-complete MCP server automation, and (iii) contribute a
corpus of 5,066 callable tools along with insights on repairing common
specification flaws.

</details>


### [6] [AI-Powered Commit Explorer (APCE)](https://arxiv.org/abs/2507.16063)
*Yousab Grees,Polina Iaremchuk,Ramtin Ehsani,Esteban Parra,Preetha Chatterjee,Sonia Haiduc*

Main category: cs.SE

TL;DR: 研究者开发了AI-Powered Commit Explorer (APCE)工具，用于支持开发者和研究人员使用和研究大语言模型生成的代码提交信息，该工具提供了提示存储、评估增强和自动化评估功能。


<details>
  <summary>Details</summary>
Motivation: 在版本控制系统中，提交信息为开发者提供了关于代码变更的宝贵信息，但在实践中高质量提交信息的编写经常被忽视。大语言模型生成的提交信息可以缓解这一问题，但需要专门的工具来支持其使用和研究。

Method: 开发了AI-Powered Commit Explorer (APCE)工具，该工具具有以下功能：1) 为大语言模型存储不同的提示；2) 提供额外的评估提示来进一步增强LLM生成的提交信息；3) 为研究人员提供自动化和人工评估LLM生成信息的直接机制。

Result: 成功开发了APCE工具，为开发者和研究人员提供了一个完整的平台来使用和研究LLM生成的提交信息，并提供了演示链接展示工具功能。

Conclusion: APCE工具为LLM生成提交信息的使用和研究提供了有效支持，通过提示管理、评估增强和自动化评估功能，帮助改善代码提交信息的质量和研究效率。

Abstract: Commit messages in a version control system provide valuable information for
developers regarding code changes in software systems. Commit messages can be
the only source of information left for future developers describing what was
changed and why. However, writing high-quality commit messages is often
neglected in practice. Large Language Model (LLM) generated commit messages
have emerged as a way to mitigate this issue. We introduce the AI-Powered
Commit Explorer (APCE), a tool to support developers and researchers in the use
and study of LLM-generated commit messages. APCE gives researchers the option
to store different prompts for LLMs and provides an additional evaluation
prompt that can further enhance the commit message provided by LLMs. APCE also
provides researchers with a straightforward mechanism for automated and human
evaluation of LLM-generated messages. Demo link https://youtu.be/zYrJ9s6sZvo

</details>


### [7] [Ten Essential Guidelines for Building High-Quality Research Software](https://arxiv.org/abs/2507.16166)
*Nasir U. Eisty,David E. Bernholdt,Alex Koufos,David J. Luet,Miranda Mundt*

Main category: cs.SE

TL;DR: 这篇论文提出了开发高质量科研软件的十个指导原则，涵盖软件开发全生命周期，旨在帮助研究人员创建稳健、可用且可持续的科研工具。


<details>
  <summary>Details</summary>
Motivation: 现代科学研究高度依赖高质量的研究软件来分析复杂数据、模拟现象和分享可重现的结果，但创建此类软件需要遵循确保稳健性、可用性和可持续性的最佳实践，因此需要系统性的指导原则。

Method: 提出涵盖软件开发全生命周期的十个指导原则，包括规划、编写清洁可读的代码、使用版本控制、实施全面的测试策略、模块化设计、可重现性、性能优化、长期维护、文档编写和社区参与等方面。

Result: 形成了一套完整的科研软件开发指导框架，涵盖了从项目规划到长期维护的各个关键环节，特别强调了文档编写和社区参与在提升软件可用性和影响力方面的重要作用。

Conclusion: 通过遵循这些指导原则，研究人员能够创建既推进其科学目标又为可靠、可重用研究工具的更广泛生态系统做出贡献的软件，为提升科研软件质量和影响力提供了实用资源。

Abstract: High-quality research software is a cornerstone of modern scientific
progress, enabling researchers to analyze complex data, simulate phenomena, and
share reproducible results. However, creating such software requires adherence
to best practices that ensure robustness, usability, and sustainability. This
paper presents ten guidelines for producing high-quality research software,
covering every stage of the development lifecycle. These guidelines emphasize
the importance of planning, writing clean and readable code, using version
control, and implementing thorough testing strategies. Additionally, they
address key principles such as modular design, reproducibility, performance
optimization, and long-term maintenance. The paper also highlights the role of
documentation and community engagement in enhancing software usability and
impact. By following these guidelines, researchers can create software that
advances their scientific objectives and contributes to a broader ecosystem of
reliable and reusable research tools. This work serves as a practical resource
for researchers and developers aiming to elevate the quality and impact of
their research software.

</details>


### [8] [LOCOFY Large Design Models -- Design to code conversion solution](https://arxiv.org/abs/2507.16208)
*Sohaib Muhammad,Ashwati Vipin,Karan Shetti,Honey Mittal*

Main category: cs.SE

TL;DR: 本文提出了大型设计模型(LDMs)范式，专门针对设计到代码转换进行训练，通过设计优化器、标记检测和自动组件提取三个模块，实现了比传统大语言模型更准确、可解释和可重复的设计到代码转换。


<details>
  <summary>Details</summary>
Motivation: 传统的大语言模型和多模态大语言模型在设计到代码转换应用中存在可解释性差、扩展性不足、资源需求高和可重复性低等挑战，需要专门针对设计领域的解决方案。

Method: 开发了专门的训练和推理管道，包括三个核心模块：1)设计优化器：使用专有真实数据集处理次优设计；2)标记和特征检测：使用预训练和微调模型准确检测和分类UI元素；3)自动组件：提取重复的UI结构为可重用组件，生成模块化代码。

Result: LDMs在端到端设计到代码转换中表现出色，使用新颖的预览匹配分数指标验证。与传统LLMs相比，在节点定位准确性、响应性和可重复性方面表现更优。定制训练的标记和特征检测模型在识别UI元素方面展现了高精度和一致性。

Conclusion: 提出的大型设计模型(LDMs)是理解设计并生成高效、可靠的生产就绪代码的优越解决方案，能够有效解决传统方法在设计到代码转换中的局限性。

Abstract: Despite rapid advances in Large Language Models and Multimodal Large Language
Models (LLMs), numerous challenges related to interpretability, scalability,
resource requirements and repeatability remain, related to their application in
the design-to-code space. To address this, we introduce the Large Design Models
(LDMs) paradigm specifically trained on designs and webpages to enable seamless
conversion from design-to-code. We have developed a training and inference
pipeline by incorporating data engineering and appropriate model architecture
modification. The training pipeline consists of the following: 1)Design
Optimiser: developed using a proprietary ground truth dataset and addresses
sub-optimal designs; 2)Tagging and feature detection: using pre-trained and
fine-tuned models, this enables the accurate detection and classification of UI
elements; and 3)Auto Components: extracts repeated UI structures into reusable
components to enable creation of modular code, thus reducing redundancy while
enhancing code reusability. In this manner, each model addresses distinct but
key issues for design-to-code conversion. Separately, our inference pipeline
processes real-world designs to produce precise and interpretable instructions
for code generation and ensures reliability. Additionally, our models
illustrated exceptional end-to-end design-to-code conversion accuracy using a
novel preview match score metric. Comparative experiments indicated superior
performance of LDMs against LLMs on accuracy of node positioning,
responsiveness and reproducibility. Moreover, our custom-trained tagging and
feature detection model demonstrated high precision and consistency in
identifying UI elements across a wide sample of test designs. Thus, our
proposed LDMs are a reliable and superior solution to understanding designs
that subsequently enable the generation of efficient and reliable
production-ready code.

</details>


### [9] [Search-based Generation of Waypoints for Triggering Self-Adaptations in Maritime Autonomous Vessels](https://arxiv.org/abs/2507.16327)
*Karoline Nylænder,Aitor Arrieta,Shaukat Ali,Paolo Arcaini*

Main category: cs.SE

TL;DR: 该论文提出了WPgen方法，通过多目标搜索算法生成微小修改的航路点来测试海事自主船舶的自适应导航系统，以验证其在异常情况下的适应能力


<details>
  <summary>Details</summary>
Motivation: 海事自主船舶需要在意外情况下自适应调整行为以维持可靠性要求，在设计阶段需要理解和识别触发自适应的设置条件，以便验证其实现的有效性

Method: 提出WPgen多目标搜索方法，使用NSGA-II算法对预定义航路点进行微小修改，保持尽可能接近原始航路点的同时导致船舶导航不当。采用三种种子策略生成初始种群，形成三个WPgen变体

Result: 在三种自主船舶（一艘水面油轮和两艘水下船舶）上评估了WPgen的三个变体，与随机搜索基线方法进行比较。实验结果显示不同变体的有效性因船舶类型而异

Conclusion: 基于实验结果，论文提出了WPgen方法的研究和实践意义，为海事自主船舶自适应导航系统的设计验证提供了有效的测试方法

Abstract: Self-adaptation in maritime autonomous vessels (AVs) enables them to adapt
their behaviors to address unexpected situations while maintaining
dependability requirements. During the design of such AVs, it is crucial to
understand and identify the settings that should trigger adaptations, enabling
validation of their implementation. To this end, we focus on the navigation
software of AVs, which must adapt their behavior during operation through
adaptations. AVs often rely on predefined waypoints to guide them along
designated routes, ensuring safe navigation. We propose a multiobjective
search-based approach, called WPgen, to generate minor modifications to the
predefined set of waypoints, keeping them as close as possible to the original
waypoints, while causing the AV to navigate inappropriately when navigating
with the generated waypoints. WPgen uses NSGA-II as the multi-objective search
algorithm with three seeding strategies for its initial population, resulting
in three variations of WPgen. We evaluated these variations on three AVs (one
overwater tanker and two underwater). We compared the three variations of WPgen
with Random Search as the baseline and with each other. Experimental results
showed that the effectiveness of these variations varied depending on the AV.
Based on the results, we present the research and practical implications of
WPgen.

</details>


### [10] [Improving Code LLM Robustness to Prompt Perturbations via Layer-Aware Model Editing](https://arxiv.org/abs/2507.16407)
*Shuhan Liu,Xing Hu,Kerui Huang,Xiaohu Yang,David Lo,Xin Xia*

Main category: cs.SE

TL;DR: 本文提出CREME方法，通过模型编辑技术提升大语言模型在代码生成任务中对提示扰动的鲁棒性，在扰动提示上将Pass@1准确率提升63%，同时保持原始输入的稳定性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在代码生成中对提示扰动高度敏感，微小的措辞、语法或格式修改都会显著降低生成代码的功能正确性。由于扰动在现实场景中频繁出现，提升LLMs对提示扰动的鲁棒性对确保实际代码生成的可靠性能至关重要。

Method: CREME（通过模型编辑增强代码鲁棒性）首先通过比较原始提示和扰动变体之间的隐藏状态来识别鲁棒性敏感层，然后在识别出的层上进行轻量级参数编辑以减少性能下降。

Result: 在HumanEval和MBPP两个代码生成基准及其扰动版本上的实验表明，CREME在扰动提示上将Pass@1准确率提升了63%，同时在干净输入上保持稳定性能，准确率偏差在1%以内。分析发现鲁棒性敏感层主要集中在网络的中间层和深层，且其位置在不同模型架构间有所变化。

Conclusion: CREME有效提升了大语言模型对提示扰动的鲁棒性，在显著改善扰动输入性能的同时保持了原始输入的稳定表现。研究揭示的鲁棒性敏感层分布规律为未来开发面向鲁棒性的编辑策略提供了宝贵基础。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in
code generation, where the natural language prompt plays a crucial role in
conveying user intent to the model. However, prior studies have shown that LLMs
are highly sensitive to prompt perturbations. Minor modifications in wording,
syntax, or formatting can significantly reduce the functional correctness of
generated code. As perturbations frequently occur in real-world scenarios,
improving the robustness of LLMs to prompt perturbations is essential for
ensuring reliable performance in practical code generation. In this paper, we
introduce CREME (Code Robustness Enhancement via Model Editing), a novel
approach that enhances LLM robustness through targeted parameter updates. CREME
first identifies robustness-sensitive layers by comparing hidden states between
an original prompt and its perturbed variant. Then, it performs lightweight
parameter editing at the identified layer to reduce performance degradation. We
evaluate CREME on two widely used code generation benchmarks (HumanEval and
MBPP) along with their perturbed counterparts. Experimental results show that
CREME improves Pass@1 accuracy by 63% on perturbed prompts while maintaining
stable performance on clean inputs, with accuracy deviations within 1%. Further
analysis reveals that robustness-sensitive layers are primarily concentrated in
the middle and deeper layers of the network, and their locations vary across
different model architectures. These insights provide a valuable foundation for
developing future robustness-oriented editing strategies.

</details>


### [11] [Exploring Large Language Models for Analyzing and Improving Method Names in Scientific Code](https://arxiv.org/abs/2507.16439)
*Gunnar Larsen,Carol Wong,Anthony Peruma*

Main category: cs.SE

TL;DR: 本研究评估了四个大型语言模型在分析和改进科学软件中方法名质量方面的能力，发现LLMs在遵循良好命名实践方面表现尚可，但在处理领域特定术语时存在不一致性，需要人工评估


<details>
  <summary>Details</summary>
Motivation: 随着研究科学家越来越依赖软件支持研究，而现有研究主要关注传统编程环境中标识符名称对程序理解的影响，对科学软件中方法名质量的研究有限。大型语言模型的进步为自动化代码分析任务提供了新机会

Method: 选择四个流行的大型语言模型，对从Python Jupyter Notebooks中提取的496个方法名进行语法模式分析和改进建议评估，并与人工标注进行对比

Result: LLMs在分析方法名方面具有一定效果，通常遵循良好的命名实践（如方法名以动词开头），但在处理领域特定术语时表现不一致，与人工标注的一致性中等

Conclusion: 自动化建议需要人工评估，本研究为通过AI自动化提高科学代码质量提供了基础性见解

Abstract: Research scientists increasingly rely on implementing software to support
their research. While previous research has examined the impact of identifier
names on program comprehension in traditional programming environments, limited
work has explored this area in scientific software, especially regarding the
quality of method names in the code. The recent advances in Large Language
Models (LLMs) present new opportunities for automating code analysis tasks,
such as identifier name appraisals and recommendations. Our study evaluates
four popular LLMs on their ability to analyze grammatical patterns and suggest
improvements for 496 method names extracted from Python-based Jupyter
Notebooks. Our findings show that the LLMs are somewhat effective in analyzing
these method names and generally follow good naming practices, like starting
method names with verbs. However, their inconsistent handling of
domain-specific terminology and only moderate agreement with human annotations
indicate that automated suggestions require human evaluation. This work
provides foundational insights for improving the quality of scientific code
through AI automation.

</details>


### [12] [On the Effectiveness of LLM-as-a-judge for Code Generation and Summarization](https://arxiv.org/abs/2507.16587)
*Giuseppe Crupi,Rosalia Tufano,Alejandro Velasco,Antonio Mastropaolo,Denys Poshyvanyk,Gabriele Bavota*

Main category: cs.SE

TL;DR: 本研究评估了大语言模型作为评判者在代码生成和代码摘要任务中的效果，发现GPT-4-turbo表现最佳，但即使是最好的模型也经常出现误判。


<details>
  <summary>Details</summary>
Motivation: 传统的定量指标（如BLEU）无法充分评估代码生成和摘要的质量，而大规模人工评估成本过高，因此需要探索使用大语言模型作为评判者的可行性，以实现自动化评估并支持多模型协作。

Method: 选择代码生成和代码摘要两个任务进行研究。对于代码生成，使用8个LLM评判1,405个Java方法和1,281个Python函数的正确性；对于代码摘要，将5个LLM的评判结果与9名人类专家对约1.2k个摘要的评判进行比较。

Result: GPT-4-turbo在两个任务中都表现出最佳的评判能力，而参数量为数百亿的"较小"LLM无法胜任评判任务。然而，即使是表现最好的LLM也经常对代码正确性和摘要质量产生误判。

Conclusion: 虽然GPT-4-turbo在LLM评判者中表现最佳，但目前的大语言模型在代码相关任务的评判上仍存在显著局限性，经常出现误判，表明LLM作为评判者的技术还需要进一步改进。

Abstract: Large Language Models have been recently exploited as judges for complex
natural language processing tasks, such as Q&A. The basic idea is to delegate
to an LLM the assessment of the "quality" of the output provided by an
automated technique for tasks for which: (i) quantitative metrics would only
tell part of the story, and; (ii) a large-scale human-based evaluation would be
too expensive. LLMs-as-a-judge, if proven effective for a specific task, can
also unlock new possibilities for automation, with several LLMs proposing a
solution for a given instance of the task and others judging and deciding what
is the best output to show the user. We study the effectiveness of
LLMs-as-a-judge for two code-related tasks, namely code generation and code
summarization. The rationale for choosing these tasks is two-fold. First,
quantitative metrics are usually not enough for the assessment of code
summarizers/generators. For example, it is well documented that metrics such as
BLEU are quite weak proxies for the quality of the generated summaries. Second,
even state-of-the-art techniques still struggle with handling complex instances
of these tasks, making them good candidates for benefiting from more advanced
solutions envisioning collaboration among LLMs. For code generation, we check
whether eight LLMs are able to judge the correctness of 1,405 Java methods and
1,281 Python functions generated by the same LLMs or implemented by humans. For
code summarization, we compare the judgment of five LLMs to those provided by
nine humans for ~1.2k summaries, related to both Java and Python functions. Our
findings show that GPT-4-turbo is the best LLM in terms of judging capabilities
for both tasks, with "smaller" LLMs featuring tens of billions parameters not
being able to cope with judging tasks. However, even the best-performing LLM
frequently misjudges the correctness of the code and summary quality.

</details>


### [13] [VulCoCo: A Simple Yet Effective Method for Detecting Vulnerable Code Clones](https://arxiv.org/abs/2507.16661)
*Tan Bui,Yan Naing Tun,Thanh Phuc Nguyen,Yindu Su,Ferdian Thung,Yikun Li,Han Wei Ang,Yide Yin,Frank Liauw,Lwin Khin Shar,Eng Lieh Ouh,Ting Zhang,David Lo*

Main category: cs.SE

TL;DR: 本文提出VulCoCo，一种结合嵌入式检索和大语言模型验证的轻量级方法，用于检测易受攻击的代码克隆(VCCs)。该方法在合成基准测试中优于现有最先进方法，并在真实项目中成功提交400个拉取请求，其中75个被合并，15个导致新发布的CVE。


<details>
  <summary>Details</summary>
Motivation: 现代软件开发中代码复用很常见，但当开发者无意中复制有风险的代码时也会传播漏洞。现有的易受攻击代码克隆检测工具往往依赖语法相似性或产生粗糙的漏洞预测且缺乏清晰解释，限制了实际应用价值。同时缺乏可复现的易受攻击代码克隆基准测试。

Method: 提出VulCoCo方法，结合嵌入式检索与大语言模型验证。从已知易受攻击函数集合开始，从大型语料库中检索语法或语义相似的候选函数，然后使用LLM评估候选函数是否保留了漏洞。构建了一个涵盖各种克隆类型的合成基准测试。

Result: 在基准测试中，VulCoCo在Precision@k和平均精度均值(MAP)方面优于先前的最先进方法。在真实世界项目中，向284个开源项目提交了400个拉取请求，其中75个被合并，15个导致新发布的CVE，证明了方法的有效性。

Conclusion: VulCoCo是一种有效的易受攻击代码克隆检测方法，在合成基准和真实项目中都表现出色。研究还为未来进一步提高易受攻击代码克隆检测精度的工作提供了见解。该方法具有轻量级和可扩展的特点，在实际应用中具有重要价值。

Abstract: Code reuse is common in modern software development, but it can also spread
vulnerabilities when developers unknowingly copy risky code. The code fragments
that preserve the logic of known vulnerabilities are known as vulnerable code
clones (VCCs). Detecting those VCCs is a critical but challenging task.
Existing VCC detection tools often rely on syntactic similarity or produce
coarse vulnerability predictions without clear explanations, limiting their
practical utility. In this paper, we propose VulCoCo, a lightweight and
scalable approach that combines embedding-based retrieval with large language
model (LLM) validation. Starting from a set of known vulnerable functions, we
retrieve syntactically or semantically similar candidate functions from a large
corpus and use an LLM to assess whether the candidates retain the
vulnerability. Given that there is a lack of reproducible vulnerable code clone
benchmarks, we first construct a synthetic benchmark that spans various clone
types.
  Our experiments on the benchmark show that VulCoCo outperforms prior
state-of-the-art methods in terms of Precision@k and mean average precision
(MAP). In addition, we also demonstrate VulCoCo's effectiveness in real-world
projects by submitting 400 pull requests (PRs) to 284 open-source projects.
Among them, 75 PRs were merged, and 15 resulted in newly published CVEs. We
also provide insights to inspire future work to further improve the precision
of vulnerable code clone detection.

</details>


### [14] [VulGuard: An Unified Tool for Evaluating Just-In-Time Vulnerability Prediction Models](https://arxiv.org/abs/2507.16685)
*Duong Nguyen,Manh Tran-Duc,Thanh Le-Cong,Triet Huynh Minh Le,M. Ali Babar,Quyet-Thang Huynh*

Main category: cs.SE

TL;DR: VulGuard是一个自动化工具，用于从GitHub仓库中提取、处理和分析提交记录，以支持即时漏洞预测（JIT-VP）研究，集成了多种先进的漏洞预测模型，并在FFmpeg和Linux内核项目中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决软件安全研究中即时漏洞预测（JIT-VP）面临的可重现性和可扩展性挑战，需要一个统一的框架来简化从GitHub仓库中提取和分析提交记录的过程。

Method: 开发了VulGuard自动化工具，该工具能够：1）自动挖掘提交历史；2）提取细粒度的代码变更、提交消息和软件工程指标；3）格式化数据用于下游分析；4）集成多个先进的漏洞预测模型；5）支持仓库级挖掘和模型级实验的统一框架；6）可轻松集成到CI/CD流水线中。

Result: 在两个具有影响力的开源项目FFmpeg和Linux内核上验证了工具的有效性，展示了其在真实世界JIT-VP研究中的潜力，并促进了标准化基准测试。

Conclusion: VulGuard成功地为即时漏洞预测研究提供了一个自动化、统一的解决方案，能够有效解决软件安全研究中的可重现性和可扩展性问题，加速真实世界的JIT-VP研究进展。

Abstract: We present VulGuard, an automated tool designed to streamline the extraction,
processing, and analysis of commits from GitHub repositories for Just-In-Time
vulnerability prediction (JIT-VP) research. VulGuard automatically mines commit
histories, extracts fine-grained code changes, commit messages, and software
engineering metrics, and formats them for downstream analysis. In addition, it
integrates several state-of-the-art vulnerability prediction models, allowing
researchers to train, evaluate, and compare models with minimal setup. By
supporting both repository-scale mining and model-level experimentation within
a unified framework, VulGuard addresses key challenges in reproducibility and
scalability in software security research. VulGuard can also be easily
integrated into the CI/CD pipeline. We demonstrate the effectiveness of the
tool in two influential open-source projects, FFmpeg and the Linux kernel,
highlighting its potential to accelerate real-world JIT-VP research and promote
standardized benchmarking. A demo video is available at:
https://youtu.be/j96096-pxbs

</details>


### [15] [Never Come Up Empty: Adaptive HyDE Retrieval for Improving LLM Developer Support](https://arxiv.org/abs/2507.16754)
*Fangjian Lei,Mariam El Mezouar,Shayan Noei,Ying Zou*

Main category: cs.SE

TL;DR: 本研究构建了包含300多万Java和Python相关Stack Overflow帖子的检索语料库，设计并评估了7种不同的RAG管道和63个变体，发现结合假设文档嵌入(HyDE)和完整答案上下文的RAG管道在回答开发者问题方面表现最佳，相比零样本基线在帮助性、正确性和细节方面都有显著提升。


<details>
  <summary>Details</summary>
Motivation: 大语言模型(LLMs)在协助开发者回答代码相关问题方面展现出潜力，但存在生成不可靠答案的风险。虽然检索增强生成(RAG)被提出来减少LLMs的幻觉问题，但由于设计选择众多，构建有效的RAG管道仍然具有挑战性。

Method: 构建了超过300万条Java和Python相关Stack Overflow帖子的检索语料库；设计并评估了7种不同的RAG管道和63个管道变体；针对历史上有相似匹配的问题进行评估；对于没有接近先验匹配的新问题，通过自动降低检索过程中的相似度阈值来提高覆盖率；将最优RAG管道应用于4个开源LLMs并与零样本性能进行比较。

Result: 结合假设文档嵌入(HyDE)和完整答案上下文的RAG管道在检索和回答Stack Overflow问题的相似内容方面表现最佳；最优RAG管道在所有模型上都持续优于零样本基线，在LLM作为评判者的评估中，在帮助性、正确性和细节方面获得了更高的分数。

Conclusion: 研究证明最优RAG管道能够稳健地提升各种开发者查询的答案质量，包括之前见过的和新颖的问题，并且在不同的LLMs上都有效果，为开发者问答系统的设计提供了有价值的指导。

Abstract: Large Language Models (LLMs) have shown promise in assisting developers with
code-related questions; however, LLMs carry the risk of generating unreliable
answers. To address this, Retrieval-Augmented Generation (RAG) has been
proposed to reduce the unreliability (i.e., hallucinations) of LLMs. However,
designing effective pipelines remains challenging due to numerous design
choices. In this paper, we construct a retrieval corpus of over 3 million Java
and Python related Stack Overflow posts with accepted answers, and explore
various RAG pipeline designs to answer developer questions, evaluating their
effectiveness in generating accurate and reliable responses. More specifically,
we (1) design and evaluate 7 different RAG pipelines and 63 pipeline variants
to answer questions that have historically similar matches, and (2) address new
questions without any close prior matches by automatically lowering the
similarity threshold during retrieval, thereby increasing the chance of finding
partially relevant context and improving coverage for unseen cases. We find
that implementing a RAG pipeline combining hypothetical-documentation-embedding
(HyDE) with the full-answer context performs best in retrieving and answering
similarcontent for Stack Overflow questions. Finally, we apply our optimal RAG
pipeline to 4 open-source LLMs and compare the results to their zero-shot
performance. Our findings show that RAG with our optimal RAG pipeline
consistently outperforms zero-shot baselines across models, achieving higher
scores for helpfulness, correctness, and detail with LLM-as-a-judge. These
findings demonstrate that our optimal RAG pipelines robustly enhance answer
quality for a wide range of developer queries including both previously seen
and novel questions across different LLMs

</details>


### [16] [Rethinking LLM-Based RTL Code Optimization Via Timing Logic Metamorphosis](https://arxiv.org/abs/2507.16808)
*Zhihao Xu,Bixin Li,Lulu Wang*

Main category: cs.SE

TL;DR: 本研究评估了大语言模型在寄存器传输级(RTL)代码优化中的效果，特别针对复杂时序逻辑，发现LLM在逻辑操作优化方面表现良好，但在复杂时序逻辑优化方面不如传统编译器方法。


<details>
  <summary>Details</summary>
Motivation: 传统RTL代码优化方法依赖手动调优和启发式算法，耗时且易出错。虽然有研究提出使用大语言模型辅助RTL代码优化，但现有方法未充分评估LLM在处理复杂时序逻辑RTL代码方面的有效性。

Method: 提出了一个包含四个子集的RTL优化评估基准，每个子集对应特定的RTL代码优化领域。引入基于变形测试的方法来系统评估LLM-Based RTL代码优化方法的有效性，核心思想是语义等价但更复杂的代码优化效果应保持一致。

Result: 通过大量实验发现：(1)基于LLM的RTL优化方法能有效优化逻辑操作，优于现有编译器方法；(2)在复杂时序逻辑的RTL代码上，特别是时序控制流优化和时钟域优化方面，LLM方法不如现有编译器方法，主要因为LLM难以理解RTL代码中的时序逻辑。

Conclusion: 研究揭示了LLM在RTL代码优化中的能力边界，为进一步利用LLM进行RTL代码优化的研究提供了见解和指导方向。

Abstract: Register Transfer Level(RTL) code optimization is crucial for achieving high
performance and low power consumption in digital circuit design. However,
traditional optimization methods often rely on manual tuning and heuristics,
which can be time-consuming and error-prone. Recent studies proposed to
leverage Large Language Models(LLMs) to assist in RTL code optimization. LLMs
can generate optimized code snippets based on natural language descriptions,
potentially speeding up the optimization process. However, existing approaches
have not thoroughly evaluated the effectiveness of LLM-Based code optimization
methods for RTL code with complex timing logic. To address this gap, we
conducted a comprehensive empirical investigation to assess the capability of
LLM-Based RTL code optimization methods in handling RTL code with complex
timing logic. In this study, we first propose a new benchmark for RTL
optimization evaluation. It comprises four subsets, each corresponding to a
specific area of RTL code optimization. Then we introduce a method based on
metamorphosis to systematically evaluate the effectiveness of LLM-Based RTL
code optimization methods.Our key insight is that the optimization
effectiveness should remain consistent for semantically equivalent but more
complex code. After intensive experiments, we revealed several key findings.
(1) LLM-Based RTL optimization methods can effectively optimize logic
operations and outperform existing compiler-based methods. (2) LLM-Based RTL
optimization methods do not perform better than existing compiler-based methods
on RTL code with complex timing logic, particularly in timing control flow
optimization and clock domain optimization. This is primarily attributed to the
challenges LLMs face in understanding timing logic in RTL code. Based on these
findings, we provide insights for further research in leveraging LLMs for RTL
code optimization.

</details>
