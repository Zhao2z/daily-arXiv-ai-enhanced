{"id": "2507.05269", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05269", "abs": "https://arxiv.org/abs/2507.05269", "authors": ["Danning Xie", "Mingwei Zheng", "Xuwei Liu", "Jiannan Wang", "Chengpeng Wang", "Lin Tan", "Xiangyu Zhang"], "title": "CORE: Benchmarking LLMs Code Reasoning Capabilities through Static Analysis Tasks", "comment": null, "summary": "Large language models (LLMs) have been widely adopted across diverse software\nengineering domains, such as code generation, program repair, and vulnerability\ndetection. These applications require understanding beyond surface-level code\npatterns: value propagation, control flow, and interdependence between program\nelements. However, existing benchmarks primarily evaluate end-to-end outcomes,\nsuch as whether code is correctly repaired or generated, leaving the models\nability for program semantic reasoning underexplored. This work presents CoRe,\na high-quality, human-verified benchmark designed to evaluate LLMs on\nfundamental static analysis tasks. CoRe includes 12,553 task instances spanning\ndata dependency, control dependency, and information flow across programs\nwritten in C/C++, Java, and Python. To ensure semantic diversity and reasoning\ncomplexity, we propose a semantics-aware diverse sampling strategy that selects\ntargets and task instances based on structural coverage and dependency depth.\nWe evaluate 10 mainstream LLMs and show that, while they perform well at\nidentifying dependencies, models still struggle with tasks that require deeper\nsemantic understanding and multi-step reasoning. We further conduct qualitative\nanalyses to uncover key challenges, such as complex control structures and\nbackward dependency patterns, offering insights into improving LLMs code\nreasoning capabilities.", "AI": {"tldr": "CoRe\u662f\u4e00\u4e2a\u9ad8\u8d28\u91cf\u3001\u4eba\u5de5\u9a8c\u8bc1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9759\u6001\u5206\u6790\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u6df1\u5c42\u8bed\u4e49\u7406\u89e3\u548c\u591a\u6b65\u63a8\u7406\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u7aef\u5230\u7aef\u7ed3\u679c\uff08\u5982\u4ee3\u7801\u4fee\u590d\u6216\u751f\u6210\uff09\uff0c\u800c\u5ffd\u7565\u4e86\u6a21\u578b\u5bf9\u7a0b\u5e8f\u8bed\u4e49\u63a8\u7406\u7684\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u4e13\u95e8\u7684\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u63d0\u51faCoRe\u57fa\u51c6\uff0c\u5305\u542b12,553\u4e2a\u4efb\u52a1\u5b9e\u4f8b\uff0c\u6db5\u76d6C/C++\u3001Java\u548cPython\u4e2d\u7684\u6570\u636e\u4f9d\u8d56\u3001\u63a7\u5236\u4f9d\u8d56\u548c\u4fe1\u606f\u6d41\u4efb\u52a1\uff0c\u91c7\u7528\u8bed\u4e49\u611f\u77e5\u7684\u591a\u6837\u5316\u91c7\u6837\u7b56\u7565\u3002", "result": "\u8bc4\u4f3010\u79cd\u4e3b\u6d41LLM\uff0c\u53d1\u73b0\u5b83\u4eec\u5728\u4f9d\u8d56\u8bc6\u522b\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u6df1\u5c42\u8bed\u4e49\u7406\u89e3\u548c\u591a\u6b65\u63a8\u7406\u4efb\u52a1\u4e2d\u4ecd\u6709\u56f0\u96be\u3002", "conclusion": "CoRe\u63ed\u793a\u4e86LLM\u5728\u590d\u6742\u63a7\u5236\u7ed3\u6784\u548c\u53cd\u5411\u4f9d\u8d56\u6a21\u5f0f\u7b49\u6311\u6218\uff0c\u4e3a\u63d0\u5347\u5176\u4ee3\u7801\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2507.05270", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.05270", "abs": "https://arxiv.org/abs/2507.05270", "authors": ["Boyuan Li", "Chengwei Liu", "Lingling Fan", "Sen Chen", "Zhenlin Zhang", "Zheli Liu"], "title": "Open Source, Hidden Costs: A Systematic Literature Review on OSS License Management", "comment": null, "summary": "Integrating third-party software components is a common practice in modern\nsoftware development, offering significant advantages in terms of efficiency\nand innovation. However, this practice is fraught with risks related to\nsoftware licensing. A lack of understanding may lead to disputes, which can\npose serious legal and operational challenges. To these ends, both academia and\nindustry have conducted various investigations and proposed solutions and tools\nto deal with these challenges. However, significant limitations still remain.\nMoreover, the rapid evolution of open-source software (OSS) licenses, as well\nas the rapidly incorporated generative software engineering techniques, such as\nlarge language models for code (CodeLLMs), are placing greater demands on the\nsystematic management of software license risks. To unveil the severe\nchallenges and explore possible future directions, we conduct the first\nsystematic literature review (SLR) on 80 carefully selected OSS license-related\npapers, classifying existing research into three key categories, i.e., license\nidentification, license risk assessment, and license risk mitigation. Based on\nthese, we discuss challenges in existing solutions, conclude the opportunities\nto shed light on future research directions and offer practical recommendations\nfor practitioners. We hope this thorough review will help bridge the gaps\nbetween academia and industry and accelerate the ecosystem-wide governance of\nlegitimate software risks within the software engineering community.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\uff08SLR\uff09\u5206\u6790\u4e8680\u7bc7\u5f00\u6e90\u8f6f\u4ef6\uff08OSS\uff09\u8bb8\u53ef\u8bc1\u76f8\u5173\u7814\u7a76\uff0c\u5206\u7c7b\u4e3a\u8bb8\u53ef\u8bc1\u8bc6\u522b\u3001\u98ce\u9669\u8bc4\u4f30\u548c\u7f13\u89e3\u4e09\u7c7b\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u7684\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u548c\u5b9e\u8df5\u5efa\u8bae\u3002", "motivation": "\u73b0\u4ee3\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u96c6\u6210\u7b2c\u4e09\u65b9\u7ec4\u4ef6\u867d\u9ad8\u6548\u521b\u65b0\uff0c\u4f46\u8bb8\u53ef\u8bc1\u98ce\u9669\u53ef\u80fd\u5bfc\u81f4\u6cd5\u5f8b\u548c\u8fd0\u8425\u95ee\u9898\u3002\u73b0\u6709\u7814\u7a76\u548c\u5de5\u5177\u4ecd\u6709\u5c40\u9650\uff0c\u4e14\u5f00\u6e90\u8bb8\u53ef\u8bc1\u548c\u751f\u6210\u5f0f\u8f6f\u4ef6\u5de5\u7a0b\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\u5bf9\u98ce\u9669\u7ba1\u7406\u63d0\u51fa\u66f4\u9ad8\u8981\u6c42\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\uff08SLR\uff09\u5206\u679080\u7bc7\u5f00\u6e90\u8bb8\u53ef\u8bc1\u76f8\u5173\u8bba\u6587\uff0c\u5206\u7c7b\u4e3a\u8bb8\u53ef\u8bc1\u8bc6\u522b\u3001\u98ce\u9669\u8bc4\u4f30\u548c\u7f13\u89e3\u4e09\u7c7b\u3002", "result": "\u63ed\u793a\u4e86\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u7684\u4e0d\u8db3\uff0c\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff08\u5982\u7ed3\u5408\u751f\u6210\u5f0f\u6280\u672f\uff09\u548c\u5b9e\u8df5\u5efa\u8bae\uff0c\u4ee5\u4fc3\u8fdb\u5b66\u672f\u754c\u4e0e\u5de5\u4e1a\u754c\u7684\u5408\u4f5c\u3002", "conclusion": "\u8bba\u6587\u4e3a\u5f00\u6e90\u8bb8\u53ef\u8bc1\u98ce\u9669\u7ba1\u7406\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u603b\u7ed3\uff0c\u65e8\u5728\u63a8\u52a8\u8f6f\u4ef6\u5de5\u7a0b\u793e\u533a\u5bf9\u5408\u6cd5\u98ce\u9669\u7684\u751f\u6001\u6cbb\u7406\u3002"}}
{"id": "2507.05272", "categories": ["cs.SE", "cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.05272", "abs": "https://arxiv.org/abs/2507.05272", "authors": ["Daragh King", "Vasileios Koutavas", "Laura Kovacs"], "title": "FuzzFeed: An Automatic Approach to Weakest Precondition Generation using LLMs and Fuzzing", "comment": null, "summary": "The weakest precondition (WP) of a program describes the largest set of\ninitial states from which all terminating executions of the program satisfy a\ngiven postcondition. The generation of WPs is an important task with practical\napplications in areas ranging from verification to run-time error checking.\n  This paper proposes the combination of Large Language Models (LLMs) and fuzz\ntesting for generating WPs. In pursuit of this goal, we introduce Fuzzing\nGuidance (FG); FG acts as a means of directing LLMs towards correct WPs using\nprogram execution feedback. FG utilises fuzz testing for approximately checking\nthe validity and weakness of candidate WPs, this information is then fed back\nto the LLM as a means of context refinement.\n  We demonstrate the effectiveness of our approach on a comprehensive benchmark\nset of deterministic array programs in Java. Our experiments indicate that LLMs\nare capable of producing viable candidate WPs, and that this ability can be\npractically enhanced through FG.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u6a21\u7cca\u6d4b\u8bd5\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u7a0b\u5e8f\u7684\u6700\u5f31\u524d\u7f6e\u6761\u4ef6\uff08WP\uff09\uff0c\u5e76\u901a\u8fc7\u6a21\u7cca\u6307\u5bfc\uff08FG\uff09\u4f18\u5316LLMs\u7684\u8f93\u51fa\u3002", "motivation": "\u751f\u6210\u7a0b\u5e8f\u7684\u6700\u5f31\u524d\u7f6e\u6761\u4ef6\uff08WP\uff09\u5728\u9a8c\u8bc1\u548c\u8fd0\u884c\u65f6\u9519\u8bef\u68c0\u67e5\u7b49\u9886\u57df\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u6548\u7387\u6709\u9650\u3002", "method": "\u7ed3\u5408LLMs\u548c\u6a21\u7cca\u6d4b\u8bd5\uff0c\u5f15\u5165\u6a21\u7cca\u6307\u5bfc\uff08FG\uff09\u901a\u8fc7\u7a0b\u5e8f\u6267\u884c\u53cd\u9988\u4f18\u5316LLMs\u751f\u6210\u7684WP\u5019\u9009\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLLMs\u80fd\u751f\u6210\u53ef\u884c\u7684WP\u5019\u9009\uff0c\u4e14FG\u80fd\u663e\u8457\u63d0\u5347\u5176\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "LLMs\u7ed3\u5408FG\u662f\u4e00\u79cd\u6709\u6548\u751f\u6210WP\u7684\u65b9\u6cd5\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.05279", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.05279", "abs": "https://arxiv.org/abs/2507.05279", "authors": ["Virgile Boraud", "Yannis Bendi-Ouis", "Paul Bernard", "Xavier Hinaut"], "title": "ReservoirChat: Interactive Documentation Enhanced with LLM and Knowledge Graph for ReservoirPy", "comment": null, "summary": "We introduce a tool designed to improve the capabilities of Large Language\nModels (LLMs) in assisting with code development using the ReservoirPy library,\nas well as in answering complex questions in the field of Reservoir Computing.\nBy incorporating external knowledge through Retrieval-Augmented Generation\n(RAG) and knowledge graphs, our approach aims to reduce hallucinations and\nincrease the factual accuracy of generated responses. The system provides an\ninteractive experience similar to ChatGPT, tailored specifically for\nReservoirPy, enabling users to write, debug, and understand Python code while\naccessing reliable domain-specific insights. In our evaluation, while\nproprietary models such as ChatGPT-4o and NotebookLM performed slightly better\non general knowledge questions, our model outperformed them on coding tasks and\nshowed a significant improvement over its base model, Codestral-22B.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u5de5\u5177\uff0c\u901a\u8fc7\u7ed3\u5408\u5916\u90e8\u77e5\u8bc6\uff08\u5982RAG\u548c\u77e5\u8bc6\u56fe\u8c31\uff09\u63d0\u5347LLM\u5728\u4ee3\u7801\u5f00\u53d1\u548cReservoir Computing\u9886\u57df\u7684\u80fd\u529b\uff0c\u51cf\u5c11\u5e7b\u89c9\u5e76\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3LLM\u5728\u4ee3\u7801\u5f00\u53d1\u548cReservoir Computing\u9886\u57df\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u63d0\u9ad8\u751f\u6210\u5185\u5bb9\u7684\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528Retrieval-Augmented Generation (RAG)\u548c\u77e5\u8bc6\u56fe\u8c31\u6280\u672f\uff0c\u7ed3\u5408ReservoirPy\u5e93\uff0c\u63d0\u4f9b\u4ea4\u4e92\u5f0f\u4f53\u9a8c\u3002", "result": "\u5728\u7f16\u7801\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8eChatGPT-4o\u548cNotebookLM\uff0c\u4e14\u663e\u8457\u4f18\u4e8e\u57fa\u7840\u6a21\u578bCodestral-22B\u3002", "conclusion": "\u8be5\u5de5\u5177\u5728\u7279\u5b9a\u9886\u57df\uff08\u5982ReservoirPy\uff09\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u4ee3\u7801\u5f00\u53d1\u4efb\u52a1\u4e0a\u5177\u6709\u4f18\u52bf\u3002"}}
{"id": "2507.05281", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.05281", "abs": "https://arxiv.org/abs/2507.05281", "authors": ["Lingyue Fu", "Hao Guan", "Bolun Zhang", "Haowei Yuan", "Yaoming Zhu", "Jun Xu", "Zongyu Wang", "Lin Qiu", "Xunliang Cai", "Xuezhi Cao", "Weiwen Liu", "Weinan Zhang", "Yong Yu"], "title": "CoreCodeBench: A Configurable Multi-Scenario Repository-Level Benchmark", "comment": null, "summary": "As Large Language Models (LLMs) demonstrate increasingly sophisticated code\nprocessing capabilities, evaluating their performance on engineering-level code\nremains challenging. Existing repository-level benchmarks primarily focus on\nsingle scenarios, such as code generation or bug fixing, without adequately\ncapturing the diversity and complexity of real-world software or project\nengineering workflows. Furthermore, these benchmarks suffer from limited\ncontrollability in question positioning and reliability issues in their\ngenerated test cases. To address these limitations, we present CorePipe, a\nfully automated pipeline that converts repositories into comprehensive test\ncases, and introduce CoreCodeBench, a configurable multi-scenario\nrepository-level benchmark. To simulate real engineering scenarios, CorePipe\ngenerates three types of atomic questions (Development, BugFix, and Test-Driven\nDevelopment) specifically targeting core code segments. These atomic questions\nare further combined into three types of composite questions, with difficulty\nlevels flexibly adjusted through hyperparameter tuning. CoreCodeBench provides\na comprehensive and extensive repository-level benchmark to investigate the\napplicability of LLMs in real-world engineering projects. Experiments with 16\nLLMs across diverse scenarios reveal varying capabilities and offer\nmulti-dimensional insights into LLM performance in engineering contexts. The\ncode for CorePipe is available at\nhttps://github.com/AGI-Eval-Official/CoreCodeBench, and the data for\nCoreCodeBench can be accessed at\nhttps://huggingface.co/collections/tubehhh/corecodebench-68256d2faabf4b1610a08caa.", "AI": {"tldr": "CorePipe\u548cCoreCodeBench\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u4ee3\u7801\u8bc4\u4f30\u57fa\u51c6\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u751f\u6210\u591a\u6837\u5316\u7684\u6d4b\u8bd5\u7528\u4f8b\uff0c\u4e3aLLMs\u5728\u5de5\u7a0b\u573a\u666f\u4e2d\u7684\u8868\u73b0\u63d0\u4f9b\u5168\u9762\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u8bc4\u4f30\u57fa\u51c6\u5728\u591a\u6837\u6027\u548c\u53ef\u63a7\u6027\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u65e0\u6cd5\u5168\u9762\u53cd\u6620LLMs\u5728\u771f\u5b9e\u5de5\u7a0b\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u63d0\u51faCorePipe\u81ea\u52a8\u5316\u7ba1\u9053\u751f\u6210\u6d4b\u8bd5\u7528\u4f8b\uff0c\u5e76\u6784\u5efaCoreCodeBench\u591a\u573a\u666f\u53ef\u914d\u7f6e\u57fa\u51c6\uff0c\u5305\u542b\u539f\u5b50\u95ee\u9898\u548c\u590d\u5408\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u4e8616\u79cdLLMs\uff0c\u63ed\u793a\u4e86\u5b83\u4eec\u5728\u5de5\u7a0b\u573a\u666f\u4e2d\u7684\u4e0d\u540c\u80fd\u529b\u548c\u591a\u7ef4\u8868\u73b0\u3002", "conclusion": "CoreCodeBench\u4e3aLLMs\u5728\u771f\u5b9e\u5de5\u7a0b\u9879\u76ee\u4e2d\u7684\u9002\u7528\u6027\u63d0\u4f9b\u4e86\u5168\u9762\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2507.05289", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.05289", "abs": "https://arxiv.org/abs/2507.05289", "authors": ["Igor Regis da Silva Simoes", "Elaine Venson"], "title": "Measuring how changes in code readability attributes affect code quality evaluation by Large Language Models", "comment": null, "summary": "Code readability is one of the main aspects of code quality, influenced by\nvarious properties like identifier names, comments, code structure, and\nadherence to standards. However, measuring this attribute poses challenges in\nboth industry and academia. While static analysis tools assess attributes such\nas code smells and comment percentage, code reviews introduce an element of\nsubjectivity. This paper explores using Large Language Models (LLMs) to\nevaluate code quality attributes related to its readability in a standardized,\nreproducible, and consistent manner. We conducted a quasi-experiment study to\nmeasure the effects of code changes on Large Language Model (LLM)s\ninterpretation regarding its readability quality attribute. Nine LLMs were\ntested, undergoing three interventions: removing comments, replacing identifier\nnames with obscure names, and refactoring to remove code smells. Each\nintervention involved 10 batch analyses per LLM, collecting data on response\nvariability. We compared the results with a known reference model and tool. The\nresults showed that all LLMs were sensitive to the interventions, with\nagreement with the reference classifier being high for the original and\nrefactored code scenarios. The LLMs demonstrated a strong semantic sensitivity\nthat the reference model did not fully capture. A thematic analysis of the LLMs\nreasoning confirmed their evaluations directly reflected the nature of each\nintervention. The models also exhibited response variability, with 9.37% to\n14.58% of executions showing a standard deviation greater than zero, indicating\nresponse oscillation, though this did not always compromise the statistical\nsignificance of the results. LLMs demonstrated potential for evaluating\nsemantic quality aspects, such as coherence between identifier names, comments,\nand documentation with code purpose.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8bc4\u4f30\u4ee3\u7801\u53ef\u8bfb\u6027\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86LLM\u5bf9\u4ee3\u7801\u5e72\u9884\u7684\u654f\u611f\u6027\u53ca\u5176\u4e0e\u53c2\u8003\u6a21\u578b\u7684\u5bf9\u6bd4\u7ed3\u679c\u3002", "motivation": "\u4ee3\u7801\u53ef\u8bfb\u6027\u662f\u4ee3\u7801\u8d28\u91cf\u7684\u91cd\u8981\u65b9\u9762\uff0c\u4f46\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u4e3b\u89c2\u6027\u548c\u4e0d\u4e00\u81f4\u6027\uff0c\u9700\u8981\u4e00\u79cd\u6807\u51c6\u5316\u3001\u53ef\u91cd\u590d\u7684\u8bc4\u4f30\u65b9\u5f0f\u3002", "method": "\u901a\u8fc7\u51c6\u5b9e\u9a8c\u7814\u7a76\uff0c\u6d4b\u8bd59\u79cdLLM\u5bf9\u4e09\u79cd\u4ee3\u7801\u5e72\u9884\uff08\u5220\u9664\u6ce8\u91ca\u3001\u66ff\u6362\u6807\u8bc6\u7b26\u540d\u79f0\u3001\u91cd\u6784\u4ee3\u7801\uff09\u7684\u53cd\u5e94\uff0c\u5e76\u4e0e\u53c2\u8003\u6a21\u578b\u5bf9\u6bd4\u3002", "result": "LLM\u5bf9\u5e72\u9884\u654f\u611f\uff0c\u4e0e\u53c2\u8003\u6a21\u578b\u5728\u539f\u59cb\u548c\u91cd\u6784\u4ee3\u7801\u573a\u666f\u4e2d\u4e00\u81f4\u6027\u9ad8\uff0c\u4e14\u8868\u73b0\u51fa\u8bed\u4e49\u654f\u611f\u6027\u3002\u54cd\u5e94\u5b58\u5728\u4e00\u5b9a\u53d8\u5f02\u6027\uff0c\u4f46\u7ed3\u679c\u4ecd\u5177\u7edf\u8ba1\u663e\u8457\u6027\u3002", "conclusion": "LLM\u5728\u8bc4\u4f30\u4ee3\u7801\u8bed\u4e49\u8d28\u91cf\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u5c24\u5176\u5728\u6807\u8bc6\u7b26\u540d\u79f0\u3001\u6ce8\u91ca\u4e0e\u4ee3\u7801\u76ee\u7684\u7684\u8fde\u8d2f\u6027\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2507.05294", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.05294", "abs": "https://arxiv.org/abs/2507.05294", "authors": ["William Law"], "title": "zkSDK: Streamlining zero-knowledge proof development through automated trace-driven ZK-backend selection", "comment": "undergrad thesis", "summary": "The rapid advancement of creating Zero-Knowledge (ZK) programs has led to the\ndevelopment of numerous tools designed to support developers. Popular options\ninclude being able to write in general-purpose programming languages like Rust\nfrom Risc Zero. Other languages exist like Circom, Lib-snark, and Cairo.\nHowever, developers entering the ZK space are faced with many different ZK\nbackends to choose from, leading to a steep learning curve and a fragmented\ndeveloper experience across different platforms. As a result, many developers\ntend to select a single ZK backend and remain tied to it. This thesis\nintroduces zkSDK, a modular framework that streamlines ZK application\ndevelopment by abstracting the backend complexities. At the core of zkSDK is\nPresto, a custom Python-like programming language that enables the profiling\nand analysis of a program to assess its computational workload intensity.\nCombined with user-defined criteria, zkSDK employs a dynamic selection\nalgorithm to automatically choose the optimal ZK-proving backend. Through an\nin-depth analysis and evaluation of real-world workloads, we demonstrate that\nzkSDK effectively selects the best-suited backend from a set of supported ZK\nbackends, delivering a seamless and user-friendly development experience.", "AI": {"tldr": "zkSDK\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u62bd\u8c61\u540e\u7aef\u590d\u6742\u6027\u7b80\u5316ZK\u5e94\u7528\u5f00\u53d1\uff0c\u6838\u5fc3\u662fPresto\u8bed\u8a00\uff0c\u52a8\u6001\u9009\u62e9\u6700\u4f18ZK\u540e\u7aef\u3002", "motivation": "ZK\u5f00\u53d1\u8005\u9762\u4e34\u4f17\u591a\u540e\u7aef\u9009\u62e9\uff0c\u5b66\u4e60\u66f2\u7ebf\u9661\u5ced\u4e14\u4f53\u9a8c\u788e\u7247\u5316\uff0czkSDK\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5f00\u53d1zkSDK\u6846\u67b6\uff0c\u96c6\u6210Presto\u8bed\u8a00\u548c\u52a8\u6001\u9009\u62e9\u7b97\u6cd5\uff0c\u81ea\u52a8\u8bc4\u4f30\u5e76\u9009\u62e9\u6700\u4f18ZK\u540e\u7aef\u3002", "result": "\u901a\u8fc7\u5b9e\u9645\u5de5\u4f5c\u8d1f\u8f7d\u5206\u6790\uff0czkSDK\u80fd\u6709\u6548\u9009\u62e9\u6700\u9002\u5408\u7684\u540e\u7aef\uff0c\u63d0\u5347\u5f00\u53d1\u4f53\u9a8c\u3002", "conclusion": "zkSDK\u4e3aZK\u5f00\u53d1\u63d0\u4f9b\u4e86\u65e0\u7f1d\u4e14\u7528\u6237\u53cb\u597d\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f18\u5316\u4e86\u540e\u7aef\u9009\u62e9\u6d41\u7a0b\u3002"}}
{"id": "2507.05307", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05307", "abs": "https://arxiv.org/abs/2507.05307", "authors": ["Xuanqi Gao", "Juan Zhai", "Shiqing Ma", "Siyi Xie", "Chao Shen"], "title": "ASSURE: Metamorphic Testing for AI-powered Browser Extensions", "comment": null, "summary": "The integration of Large Language Models (LLMs) into browser extensions has\nrevolutionized web browsing, enabling sophisticated functionalities like\ncontent summarization, intelligent translation, and context-aware writing\nassistance. However, these AI-powered extensions introduce unprecedented\nchallenges in testing and reliability assurance. Traditional browser extension\ntesting approaches fail to address the non-deterministic behavior,\ncontext-sensitivity, and complex web environment integration inherent to\nLLM-powered extensions. Similarly, existing LLM testing methodologies operate\nin isolation from browser-specific contexts, creating a critical gap in\neffective evaluation frameworks. To bridge this gap, we present ASSURE, a\nmodular automated testing framework specifically designed for AI-powered\nbrowser extensions. ASSURE comprises three principal components: (1) a modular\ntest case generation engine that supports plugin-based extension of testing\nscenarios, (2) an automated execution framework that orchestrates the complex\ninteractions between web content, extension processing, and AI model behavior,\nand (3) a configurable validation pipeline that systematically evaluates\nbehavioral consistency and security invariants rather than relying on exact\noutput matching. Our evaluation across six widely-used AI browser extensions\ndemonstrates ASSURE's effectiveness, identifying 531 distinct issues spanning\nsecurity vulnerabilities, metamorphic relation violations, and content\nalignment problems. ASSURE achieves 6.4x improved testing throughput compared\nto manual approaches, detecting critical security vulnerabilities within 12.4\nminutes on average. This efficiency makes ASSURE practical for integration into\ndevelopment pipelines, offering a comprehensive solution to the unique\nchallenges of testing AI-powered browser extensions.", "AI": {"tldr": "ASSURE\u662f\u4e00\u4e2a\u4e13\u4e3aAI\u6d4f\u89c8\u5668\u6269\u5c55\u8bbe\u8ba1\u7684\u81ea\u52a8\u5316\u6d4b\u8bd5\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u5e94\u5bf9\u7684\u975e\u786e\u5b9a\u6027\u884c\u4e3a\u548c\u590d\u6742\u96c6\u6210\u95ee\u9898\u3002", "motivation": "AI\u6d4f\u89c8\u5668\u6269\u5c55\u7684\u6d4b\u8bd5\u548c\u53ef\u9760\u6027\u4fdd\u969c\u9762\u4e34\u524d\u6240\u672a\u6709\u7684\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u548c\u73b0\u6709LLM\u6d4b\u8bd5\u65b9\u6cd5\u5747\u65e0\u6cd5\u6709\u6548\u89e3\u51b3\u3002", "method": "ASSURE\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff1a\u6a21\u5757\u5316\u6d4b\u8bd5\u7528\u4f8b\u751f\u6210\u5f15\u64ce\u3001\u81ea\u52a8\u5316\u6267\u884c\u6846\u67b6\u548c\u53ef\u914d\u7f6e\u9a8c\u8bc1\u7ba1\u9053\u3002", "result": "\u5728\u516d\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684AI\u6d4f\u89c8\u5668\u6269\u5c55\u4e2d\uff0cASSURE\u8bc6\u522b\u4e86531\u4e2a\u95ee\u9898\uff0c\u6d4b\u8bd5\u541e\u5410\u91cf\u6bd4\u624b\u52a8\u65b9\u6cd5\u63d0\u9ad8\u4e866.4\u500d\u3002", "conclusion": "ASSURE\u4e3aAI\u6d4f\u89c8\u5668\u6269\u5c55\u7684\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u5408\u96c6\u6210\u5230\u5f00\u53d1\u6d41\u7a0b\u4e2d\u3002"}}
{"id": "2507.05316", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05316", "abs": "https://arxiv.org/abs/2507.05316", "authors": ["Koren Lazar", "Matan Vetzler", "Kiran Kate", "Jason Tsay", "David Boaz Himanshu Gupta", "Avraham Shinnar", "Rohith D Vallam", "David Amid Esther Goldbraich", "Guy Uziel", "Jim Laredo", "Ateret Anaby Tavor"], "title": "OASBuilder: Generating OpenAPI Specifications from Online API Documentation with Large Language Models", "comment": null, "summary": "AI agents and business automation tools interacting with external web\nservices require standardized, machine-readable information about their APIs in\nthe form of API specifications. However, the information about APIs available\nonline is often presented as unstructured, free-form HTML documentation,\nrequiring external users to spend significant time manually converting it into\na structured format. To address this, we introduce OASBuilder, a novel\nframework that transforms long and diverse API documentation pages into\nconsistent, machine-readable API specifications. This is achieved through a\ncarefully crafted pipeline that integrates large language models and rule-based\nalgorithms which are guided by domain knowledge of the structure of\ndocumentation webpages. Our experiments demonstrate that OASBuilder generalizes\nwell across hundreds of APIs, and produces valid OpenAPI specifications that\nencapsulate most of the information from the original documentation. OASBuilder\nhas been successfully implemented in an enterprise environment, saving\nthousands of hours of manual effort and making hundreds of complex enterprise\nAPIs accessible as tools for LLMs.", "AI": {"tldr": "OASBuilder\u662f\u4e00\u4e2a\u6846\u67b6\uff0c\u5c06\u975e\u7ed3\u6784\u5316\u7684API\u6587\u6863\u8f6c\u6362\u4e3a\u673a\u5668\u53ef\u8bfb\u7684OpenAPI\u89c4\u8303\uff0c\u7ed3\u5408\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u7b97\u6cd5\u3002", "motivation": "\u5728\u7ebfAPI\u6587\u6863\u591a\u4e3a\u975e\u7ed3\u6784\u5316HTML\uff0c\u624b\u52a8\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u683c\u5f0f\u8017\u65f6\u8d39\u529b\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u7b97\u6cd5\uff0c\u5229\u7528\u6587\u6863\u7f51\u9875\u7ed3\u6784\u77e5\u8bc6\uff0c\u6784\u5efa\u8f6c\u6362\u7ba1\u9053\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cOASBuilder\u80fd\u5904\u7406\u6570\u767e\u79cdAPI\uff0c\u751f\u6210\u6709\u6548\u7684OpenAPI\u89c4\u8303\uff0c\u6db5\u76d6\u5927\u90e8\u5206\u539f\u59cb\u6587\u6863\u4fe1\u606f\u3002", "conclusion": "OASBuilder\u5728\u4f01\u4e1a\u73af\u5883\u4e2d\u6210\u529f\u5e94\u7528\uff0c\u8282\u7701\u5927\u91cf\u624b\u52a8\u65f6\u95f4\uff0c\u4f7f\u590d\u6742API\u66f4\u6613\u4e8eLLMs\u4f7f\u7528\u3002"}}
{"id": "2507.05325", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.05325", "abs": "https://arxiv.org/abs/2507.05325", "authors": ["Lidiany Cerqueira", "Jo\u00e3o Pedro Bastos", "Danilo Neves", "Glauco Carneiro", "Rodrigo Sp\u00ednola", "S\u00e1vio Freire", "Jos\u00e9 Amancio Macedo Santos", "Manoel Mendon\u00e7a"], "title": "Exploring Empathy in Software Engineering: Insights from a Grey Literature Analysis of Practitioners' Perspectives", "comment": "This is the author's version of the paper accepted for publication in\n  ACM Transactions on Software Engineering and Methodology. The final version\n  will be available via the ACM Digital Library. The HTML preview may not\n  render some formatting correctly. Please refer to the PDF version for\n  accurate presentation", "summary": "Context. Empathy, a key social skill, is essential for communication and\ncollaboration in SE but remains an under-researched topic. Aims. This study\ninvestigates empathy in SE from practitioners' perspectives, aiming to\ncharacterize its meaning, identify barriers, discuss practices to overcome\nthem, and explore its effects. Method. A qualitative content analysis was\nconducted on 55 web articles from DEV and Medium, two communities widely used\nby practitioners. To strengthen our findings, we conducted a follow-up survey\nwith empathy experts. Results. The study proposes a definition of empathy in\nSE, identifies barriers such as toxic culture and excessive technical focus,\npractices to foster empathy in teams, and outcomes, including improved\ncollaboration, communication, and reduced anxiety, frustration, and stress.\nThese findings are synthesized into a conceptual framework. Conclusion. Survey\nresults indicate the framework is clear, valuable, and raises empathy\nawareness, with suggestions for improvements and integration into training.\nThis study paves the way for improving team dynamics by addressing barriers and\noffering strategies to cultivate empathy. Future work will explore empathy's\nbroader implications in SE practice.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5171\u60c5\uff0c\u5b9a\u4e49\u4e86\u5176\u542b\u4e49\uff0c\u8bc6\u522b\u4e86\u969c\u788d\uff0c\u63d0\u51fa\u4e86\u5b9e\u8df5\u65b9\u6cd5\uff0c\u5e76\u6784\u5efa\u4e86\u6982\u5ff5\u6846\u67b6\u3002", "motivation": "\u5171\u60c5\u662f\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u672a\u88ab\u5145\u5206\u7814\u7a76\u7684\u5173\u952e\u793e\u4ea4\u6280\u80fd\uff0c\u5bf9\u6c9f\u901a\u548c\u534f\u4f5c\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u5206\u679055\u7bc7DEV\u548cMedium\u6587\u7ae0\uff0c\u5e76\u8f85\u4ee5\u4e13\u5bb6\u8c03\u67e5\uff0c\u8fdb\u884c\u5b9a\u6027\u5185\u5bb9\u5206\u6790\u3002", "result": "\u63d0\u51fa\u4e86\u5171\u60c5\u7684\u5b9a\u4e49\uff0c\u8bc6\u522b\u4e86\u969c\u788d\uff08\u5982\u6bd2\u6027\u6587\u5316\u548c\u6280\u672f\u8fc7\u5ea6\u5173\u6ce8\uff09\uff0c\u63d0\u51fa\u4e86\u4fc3\u8fdb\u5171\u60c5\u7684\u5b9e\u8df5\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u79ef\u6781\u6548\u679c\uff08\u5982\u6539\u5584\u534f\u4f5c\u548c\u51cf\u5c11\u538b\u529b\uff09\u3002", "conclusion": "\u6784\u5efa\u7684\u6982\u5ff5\u6846\u67b6\u88ab\u8ba4\u4e3a\u6e05\u6670\u4e14\u6709\u4ef7\u503c\uff0c\u672a\u6765\u5c06\u63a2\u7d22\u5171\u60c5\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u66f4\u5e7f\u6cdb\u5f71\u54cd\u3002"}}
{"id": "2507.05504", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.05504", "abs": "https://arxiv.org/abs/2507.05504", "authors": ["Alex Kleijwegt", "Sinem Getir Yaman", "Radu Calinescu"], "title": "Tool for Supporting Debugging and Understanding of Normative Requirements Using LLMs", "comment": null, "summary": "Normative requirements specify social, legal, ethical, empathetic, and\ncultural (SLEEC) norms that must be observed by a system. To support the\nidentification of SLEEC requirements, numerous standards and regulations have\nbeen developed. These requirements are typically defined by stakeholders in the\nnon-technical system with diverse expertise (e.g., ethicists, lawyers, social\nscientists). Hence, ensuring their consistency and managing the requirement\nelicitation process are complex and error-prone tasks. Recent research has\naddressed this challenge using domain-specific languages to specify normative\nrequirements as rules, whose consistency can then be analyzed with formal\nmethods. Nevertheless, these approaches often present the results from formal\nverification tools in a way that is inaccessible to non-technical users. This\nhinders understanding and makes the iterative process of eliciting and\nvalidating these requirements inefficient in terms of both time and effort. To\naddress this problem, we introduce SLEEC-LLM, a tool that uses large language\nmodels (LLMs) to provide natural-language interpretations for model-checking\ncounterexamples corresponding to SLEEC rule inconsistencies. SLEEC-LLM improves\nthe efficiency and explainability of normative requirements elicitation and\nconsistency analysis. To demonstrate its effectiveness, we summarise its use in\ntwo real-world case studies involving non-technical stakeholders.", "AI": {"tldr": "SLEEC-LLM\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e3aSLEEC\u89c4\u5219\u4e0d\u4e00\u81f4\u6027\u63d0\u4f9b\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\uff0c\u63d0\u5347\u89c4\u8303\u9700\u6c42\u83b7\u53d6\u548c\u4e00\u81f4\u6027\u5206\u6790\u7684\u6548\u7387\u4e0e\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u89c4\u8303\u9700\u6c42\uff08SLEEC\uff09\u7684\u8bc6\u522b\u548c\u7ba1\u7406\u590d\u6742\u4e14\u6613\u9519\uff0c\u73b0\u6709\u65b9\u6cd5\u7684\u6280\u672f\u6027\u7ed3\u679c\u5bf9\u975e\u6280\u672f\u7528\u6237\u4e0d\u53cb\u597d\uff0c\u963b\u788d\u4e86\u7406\u89e3\u4e0e\u8fed\u4ee3\u6548\u7387\u3002", "method": "\u63d0\u51faSLEEC-LLM\u5de5\u5177\uff0c\u5229\u7528LLMs\u5c06\u5f62\u5f0f\u9a8c\u8bc1\u7ed3\u679c\u8f6c\u5316\u4e3a\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u3002", "result": "\u901a\u8fc7\u4e24\u4e2a\u771f\u5b9e\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86SLEEC-LLM\u5728\u63d0\u5347\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "SLEEC-LLM\u4e3a\u975e\u6280\u672f\u7528\u6237\u63d0\u4f9b\u4e86\u66f4\u76f4\u89c2\u7684\u5de5\u5177\uff0c\u4f18\u5316\u4e86\u89c4\u8303\u9700\u6c42\u7684\u83b7\u53d6\u4e0e\u5206\u6790\u8fc7\u7a0b\u3002"}}
{"id": "2507.05565", "categories": ["cs.SE", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.05565", "abs": "https://arxiv.org/abs/2507.05565", "authors": ["Sangwon Hyun", "Shaukat Ali", "M. Ali Babar"], "title": "Search-based Selection of Metamorphic Relations for Optimized Robustness Testing of Large Language Models", "comment": null, "summary": "Assessing the trustworthiness of Large Language Models (LLMs), such as\nrobustness, has garnered significant attention. Recently, metamorphic testing\nthat defines Metamorphic Relations (MRs) has been widely applied to evaluate\nthe robustness of LLM executions. However, the MR-based robustness testing\nstill requires a scalable number of MRs, thereby necessitating the optimization\nof selecting MRs. Most extant LLM testing studies are limited to automatically\ngenerating test cases (i.e., MRs) to enhance failure detection. Additionally,\nmost studies only considered a limited test space of single perturbation MRs in\ntheir evaluation of LLMs. In contrast, our paper proposes a search-based\napproach for optimizing the MR groups to maximize failure detection and\nminimize the LLM execution cost. Moreover, our approach covers the\ncombinatorial perturbations in MRs, facilitating the expansion of test space in\nthe robustness assessment. We have developed a search process and implemented\nfour search algorithms: Single-GA, NSGA-II, SPEA2, and MOEA/D with novel\nencoding to solve the MR selection problem in the LLM robustness testing. We\nconducted comparative experiments on the four search algorithms along with a\nrandom search, using two major LLMs with primary Text-to-Text tasks. Our\nstatistical and empirical investigation revealed two key findings: (1) the\nMOEA/D algorithm performed the best in optimizing the MR space for LLM\nrobustness testing, and (2) we identified silver bullet MRs for the LLM\nrobustness testing, which demonstrated dominant capabilities in confusing LLMs\nacross different Text-to-Text tasks. In LLM robustness assessment, our research\nsheds light on the fundamental problem for optimized testing and provides\ninsights into search-based solutions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u641c\u7d22\u7684\u65b9\u6cd5\uff0c\u4f18\u5316MR\u7ec4\u4ee5\u6700\u5927\u5316\u5931\u8d25\u68c0\u6d4b\u5e76\u6700\u5c0f\u5316LLM\u6267\u884c\u6210\u672c\uff0c\u540c\u65f6\u8986\u76d6\u7ec4\u5408\u6270\u52a8\u3002MOEA/D\u7b97\u6cd5\u8868\u73b0\u6700\u4f73\uff0c\u5e76\u53d1\u73b0\u4e86\u94f6\u5f39MR\u3002", "motivation": "\u73b0\u6709LLM\u6d4b\u8bd5\u7814\u7a76\u591a\u9650\u4e8e\u81ea\u52a8\u751f\u6210\u6d4b\u8bd5\u7528\u4f8b\uff08MRs\uff09\uff0c\u4e14\u4ec5\u8003\u8651\u5355\u4e00\u6270\u52a8MRs\uff0c\u7f3a\u4e4f\u5bf9MR\u9009\u62e9\u7684\u4f18\u5316\u548c\u7ec4\u5408\u6270\u52a8\u7684\u8986\u76d6\u3002", "method": "\u63d0\u51fa\u641c\u7d22\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u56db\u79cd\u641c\u7d22\u7b97\u6cd5\uff08Single-GA\u3001NSGA-II\u3001SPEA2\u3001MOEA/D\uff09\u4f18\u5316MR\u9009\u62e9\uff0c\u8986\u76d6\u7ec4\u5408\u6270\u52a8\u3002", "result": "MOEA/D\u7b97\u6cd5\u5728\u4f18\u5316MR\u7a7a\u95f4\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u5e76\u53d1\u73b0\u94f6\u5f39MRs\u80fd\u6709\u6548\u6df7\u6dc6LLM\u3002", "conclusion": "\u7814\u7a76\u4e3aLLM\u7a33\u5065\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4f18\u5316\u6d4b\u8bd5\u7684\u57fa\u7840\u95ee\u9898\u548c\u57fa\u4e8e\u641c\u7d22\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.05932", "categories": ["cs.SE", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.05932", "abs": "https://arxiv.org/abs/2507.05932", "authors": ["You Lu", "Dingji Wang", "Kaifeng Huang", "Bihuan Chen", "Xin Peng"], "title": "TigAug: Data Augmentation for Testing Traffic Light Detection in Autonomous Driving Systems", "comment": null, "summary": "Autonomous vehicle technology has been developed in the last decades with\nrecent advances in sensing and computing technology. There is an urgent need to\nensure the reliability and robustness of autonomous driving systems (ADSs).\nDespite the recent achievements in testing various ADS modules, little\nattention has been paid on the automated testing of traffic light detection\nmodels in ADSs. A common practice is to manually collect and label traffic\nlight data. However, it is labor-intensive, and even impossible to collect\ndiverse data under different driving environments.\n  To address these problems, we propose and implement TigAug to automatically\naugment labeled traffic light images for testing traffic light detection models\nin ADSs. We construct two families of metamorphic relations and three families\nof transformations based on a systematic understanding of weather environments,\ncamera properties, and traffic light properties. We use augmented images to\ndetect erroneous behaviors of traffic light detection models by\ntransformation-specific metamorphic relations, and to improve the performance\nof traffic light detection models by retraining. Large-scale experiments with\nfour state-of-the-art traffic light detection models and two traffic light\ndatasets have demonstrated that i) TigAug is effective in testing traffic light\ndetection models, ii) TigAug is efficient in synthesizing traffic light images,\nand iii) TigAug generates traffic light images with acceptable naturalness.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faTigAug\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u52a8\u589e\u5f3a\u6807\u8bb0\u7684\u4ea4\u901a\u706f\u56fe\u50cf\u6765\u6d4b\u8bd5\u548c\u6539\u8fdb\u4ea4\u901a\u706f\u68c0\u6d4b\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u624b\u52a8\u6570\u636e\u6536\u96c6\u7684\u4e0d\u8db3\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\uff08ADS\uff09\u4e2d\u4ea4\u901a\u706f\u68c0\u6d4b\u6a21\u578b\u7684\u6d4b\u8bd5\u7f3a\u4e4f\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u624b\u52a8\u6570\u636e\u6536\u96c6\u6548\u7387\u4f4e\u4e14\u96be\u4ee5\u8986\u76d6\u591a\u6837\u73af\u5883\u3002", "method": "\u6784\u5efa\u57fa\u4e8e\u5929\u6c14\u3001\u76f8\u673a\u548c\u4ea4\u901a\u706f\u7279\u6027\u7684\u8715\u53d8\u5173\u7cfb\u548c\u53d8\u6362\u5bb6\u65cf\uff0c\u81ea\u52a8\u751f\u6210\u589e\u5f3a\u56fe\u50cf\u7528\u4e8e\u6d4b\u8bd5\u548c\u6a21\u578b\u6539\u8fdb\u3002", "result": "\u5b9e\u9a8c\u8868\u660eTigAug\u80fd\u6709\u6548\u6d4b\u8bd5\u6a21\u578b\u3001\u9ad8\u6548\u5408\u6210\u56fe\u50cf\uff0c\u4e14\u751f\u6210\u56fe\u50cf\u5177\u6709\u53ef\u63a5\u53d7\u7684\u81ea\u7136\u5ea6\u3002", "conclusion": "TigAug\u4e3a\u4ea4\u901a\u706f\u68c0\u6d4b\u6a21\u578b\u7684\u6d4b\u8bd5\u548c\u6539\u8fdb\u63d0\u4f9b\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.05981", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.05981", "abs": "https://arxiv.org/abs/2507.05981", "authors": ["Marc Oriol", "Quim Motger", "Jordi Marco", "Xavier Franch"], "title": "Multi-Agent Debate Strategies to Enhance Requirements Engineering with Large Language Models", "comment": null, "summary": "Context: Large Language Model (LLM) agents are becoming widely used for\nvarious Requirements Engineering (RE) tasks. Research on improving their\naccuracy mainly focuses on prompt engineering, model fine-tuning, and retrieval\naugmented generation. However, these methods often treat models as isolated\nblack boxes - relying on single-pass outputs without iterative refinement or\ncollaboration, limiting robustness and adaptability. Objective: We propose\nthat, just as human debates enhance accuracy and reduce bias in RE tasks by\nincorporating diverse perspectives, different LLM agents debating and\ncollaborating may achieve similar improvements. Our goal is to investigate\nwhether Multi-Agent Debate (MAD) strategies can enhance RE performance. Method:\nWe conducted a systematic study of existing MAD strategies across various\ndomains to identify their key characteristics. To assess their applicability in\nRE, we implemented and tested a preliminary MAD-based framework for RE\nclassification. Results: Our study identified and categorized several MAD\nstrategies, leading to a taxonomy outlining their core attributes. Our\npreliminary evaluation demonstrated the feasibility of applying MAD to RE\nclassification. Conclusions: MAD presents a promising approach for improving\nLLM accuracy in RE tasks. This study provides a foundational understanding of\nMAD strategies, offering insights for future research and refinements in RE\napplications.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\uff08MAD\uff09\u7b56\u7565\u5982\u4f55\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u9700\u6c42\u5de5\u7a0b\uff08RE\uff09\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u521d\u6b65\u7684MAD\u6846\u67b6\u5e76\u9a8c\u8bc1\u5176\u53ef\u884c\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u63d0\u793a\u5de5\u7a0b\u3001\u6a21\u578b\u5fae\u8c03\uff09\u5c06LLM\u89c6\u4e3a\u5b64\u7acb\u9ed1\u7bb1\uff0c\u7f3a\u4e4f\u8fed\u4ee3\u4f18\u5316\u548c\u534f\u4f5c\uff0c\u9650\u5236\u4e86\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002\u53d7\u4eba\u7c7b\u8fa9\u8bba\u542f\u53d1\uff0c\u7814\u7a76\u63a2\u7d22MAD\u7b56\u7565\u662f\u5426\u80fd\u63d0\u5347RE\u4efb\u52a1\u6027\u80fd\u3002", "method": "\u7cfb\u7edf\u7814\u7a76\u4e86\u73b0\u6709MAD\u7b56\u7565\uff0c\u63d0\u51fa\u5206\u7c7b\u6cd5\uff0c\u5e76\u521d\u6b65\u5b9e\u73b0\u4e86\u4e00\u4e2a\u57fa\u4e8eMAD\u7684RE\u5206\u7c7b\u6846\u67b6\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u7814\u7a76\u603b\u7ed3\u4e86MAD\u7b56\u7565\u7684\u6838\u5fc3\u7279\u5f81\uff0c\u521d\u6b65\u8bc4\u4f30\u8868\u660eMAD\u5728RE\u5206\u7c7b\u4e2d\u5177\u6709\u53ef\u884c\u6027\u3002", "conclusion": "MAD\u4e3a\u63d0\u5347LLM\u5728RE\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u6027\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.05995", "categories": ["cs.SE", "68Nxx", "D.2.0; D.2.8"], "pdf": "https://arxiv.org/pdf/2507.05995", "abs": "https://arxiv.org/abs/2507.05995", "authors": ["Pengzhou Chen", "Tao Chen"], "title": "PromiseTune: Unveiling Causally Promising and Explainable Configuration Tuning", "comment": "This paper has been accepted by ICSE26", "summary": "The high configurability of modern software systems has made configuration\ntuning a crucial step for assuring system performance, e.g., latency or\nthroughput. However, given the expensive measurements, large configuration\nspace, and rugged configuration landscape, existing tuners suffer\nineffectiveness due to the difficult balance of budget utilization between\nexploring uncertain regions (for escaping from local optima) and exploiting\nguidance of known good configurations (for fast convergence). The root cause is\nthat we lack knowledge of where the promising regions lay, which also causes\nchallenges in the explainability of the results.\n  In this paper, we propose PromiseTune that tunes configuration guided by\ncausally purified rules. PromiseTune is unique in the sense that we learn\nrules, which reflect certain regions in the configuration landscape, and purify\nthem with causal inference. The remaining rules serve as approximated\nreflections of the promising regions, bounding the tuning to emphasize these\nplaces in the landscape. This, as we demonstrate, can effectively mitigate the\nimpact of the exploration and exploitation trade-off. Those purified regions\ncan then be paired with the measured configurations to provide spatial\nexplainability at the landscape level. Comparing with 11 state-of-the-art\ntuners on 12 systems and varying budgets, we show that PromiseTune performs\nsignificantly better than the others with $42\\%$ superior rank to the overall\nsecond best while providing richer information to explain the hidden system\ncharacteristics.", "AI": {"tldr": "PromiseTune\u901a\u8fc7\u56e0\u679c\u63a8\u65ad\u51c0\u5316\u89c4\u5219\uff0c\u6307\u5bfc\u914d\u7f6e\u8c03\u4f18\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u63d0\u4f9b\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u4ee3\u8f6f\u4ef6\u7cfb\u7edf\u7684\u9ad8\u53ef\u914d\u7f6e\u6027\u4f7f\u914d\u7f6e\u8c03\u4f18\u6210\u4e3a\u5173\u952e\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u63a2\u7d22\u4e0e\u5229\u7528\u95f4\u96be\u4ee5\u5e73\u8861\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u6f5c\u5728\u4f18\u5316\u533a\u57df\u7684\u4e86\u89e3\u3002", "method": "\u63d0\u51faPromiseTune\uff0c\u901a\u8fc7\u5b66\u4e60\u5e76\u56e0\u679c\u51c0\u5316\u89c4\u5219\uff0c\u8fd1\u4f3c\u53cd\u6620\u6f5c\u5728\u4f18\u5316\u533a\u57df\uff0c\u6307\u5bfc\u8c03\u4f18\u5e76\u7f13\u89e3\u63a2\u7d22\u4e0e\u5229\u7528\u7684\u6743\u8861\u95ee\u9898\u3002", "result": "\u572812\u4e2a\u7cfb\u7edf\u548c\u4e0d\u540c\u9884\u7b97\u4e0b\uff0cPromiseTune\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u5176\u4ed611\u79cd\u65b9\u6cd5\uff0c\u6392\u540d\u63d0\u534742%\uff0c\u5e76\u63d0\u4f9b\u66f4\u4e30\u5bcc\u7684\u89e3\u91ca\u4fe1\u606f\u3002", "conclusion": "PromiseTune\u901a\u8fc7\u56e0\u679c\u89c4\u5219\u6307\u5bfc\u8c03\u4f18\uff0c\u6709\u6548\u63d0\u5347\u6027\u80fd\u5e76\u589e\u5f3a\u7ed3\u679c\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2507.06014", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.06014", "abs": "https://arxiv.org/abs/2507.06014", "authors": ["Tim Puhlf\u00fcr\u00df", "Julia Butzke", "Walid Maalej"], "title": "Model Cards Revisited: Bridging the Gap Between Theory and Practice for Ethical AI Requirements", "comment": "Accepted for publication at the 33rd IEEE International Requirements\n  Engineering 2025 conference", "summary": "Model cards are the primary documentation framework for developers of\nartificial intelligence (AI) models to communicate critical information to\ntheir users. Those users are often developers themselves looking for relevant\ndocumentation to ensure that their AI systems comply with the ethical\nrequirements of existing laws, guidelines, and standards. Recent studies\nindicate inadequate model documentation practices, suggesting a gap between AI\nrequirements and current practices in model documentation. To understand this\ngap and provide actionable guidance to bridge it, we conducted a thematic\nanalysis of 26 guidelines on ethics and AI, three AI documentation frameworks,\nthree quantitative studies of model cards, and ten actual model cards. We\nidentified a total of 43 ethical requirements relevant to model documentation\nand organized them into a taxonomy featuring four themes and twelve sub-themes\nrepresenting ethical principles. Our findings indicate that model developers\npredominantly emphasize model capabilities and reliability in the documentation\nwhile overlooking other ethical aspects, such as explainability, user autonomy,\nand fairness. This underscores the need for enhanced support in documenting\nethical AI considerations. Our taxonomy serves as a foundation for a revised\nmodel card framework that holistically addresses ethical AI requirements.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86AI\u6a21\u578b\u6587\u6863\uff08\u6a21\u578b\u5361\uff09\u7684\u73b0\u72b6\uff0c\u53d1\u73b0\u5f00\u53d1\u8005\u4e3b\u8981\u5173\u6ce8\u6a21\u578b\u80fd\u529b\u548c\u53ef\u9760\u6027\uff0c\u800c\u5ffd\u7565\u4e86\u5176\u4ed6\u4f26\u7406\u8981\u6c42\uff08\u5982\u53ef\u89e3\u91ca\u6027\u3001\u7528\u6237\u81ea\u4e3b\u6743\u548c\u516c\u5e73\u6027\uff09\u3002\u901a\u8fc7\u5206\u679026\u4efd\u4f26\u7406\u6307\u5357\u30013\u4e2a\u6587\u6863\u6846\u67b6\u548c10\u4e2a\u5b9e\u9645\u6a21\u578b\u5361\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b43\u9879\u4f26\u7406\u9700\u6c42\u7684\u5206\u7c7b\u6cd5\uff0c\u4e3a\u6539\u8fdb\u6a21\u578b\u5361\u6846\u67b6\u63d0\u4f9b\u57fa\u7840\u3002", "motivation": "\u5f53\u524dAI\u6a21\u578b\u6587\u6863\u5b9e\u8df5\u4e0d\u8db3\uff0c\u672a\u80fd\u6ee1\u8db3\u4f26\u7406\u6cd5\u5f8b\u548c\u6807\u51c6\u7684\u8981\u6c42\uff0c\u9700\u586b\u8865\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u5bf926\u4efd\u4f26\u7406\u6307\u5357\u30013\u4e2aAI\u6587\u6863\u6846\u67b6\u30013\u9879\u5b9a\u91cf\u7814\u7a76\u548c10\u4e2a\u5b9e\u9645\u6a21\u578b\u5361\u8fdb\u884c\u4e3b\u9898\u5206\u6790\uff0c\u8bc6\u522b\u5e76\u5206\u7c7b43\u9879\u4f26\u7406\u9700\u6c42\u3002", "result": "\u5f00\u53d1\u8005\u4e3b\u8981\u5173\u6ce8\u6a21\u578b\u80fd\u529b\u548c\u53ef\u9760\u6027\uff0c\u5ffd\u89c6\u4e86\u53ef\u89e3\u91ca\u6027\u3001\u7528\u6237\u81ea\u4e3b\u6743\u548c\u516c\u5e73\u6027\u7b49\u4f26\u7406\u65b9\u9762\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u7c7b\u6cd5\uff0c\u4e3a\u5168\u9762\u6ee1\u8db3\u4f26\u7406\u9700\u6c42\u7684\u6a21\u578b\u5361\u6846\u67b6\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2507.05269", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05269", "abs": "https://arxiv.org/abs/2507.05269", "authors": ["Danning Xie", "Mingwei Zheng", "Xuwei Liu", "Jiannan Wang", "Chengpeng Wang", "Lin Tan", "Xiangyu Zhang"], "title": "CORE: Benchmarking LLMs Code Reasoning Capabilities through Static Analysis Tasks", "comment": null, "summary": "Large language models (LLMs) have been widely adopted across diverse software\nengineering domains, such as code generation, program repair, and vulnerability\ndetection. These applications require understanding beyond surface-level code\npatterns: value propagation, control flow, and interdependence between program\nelements. However, existing benchmarks primarily evaluate end-to-end outcomes,\nsuch as whether code is correctly repaired or generated, leaving the models\nability for program semantic reasoning underexplored. This work presents CoRe,\na high-quality, human-verified benchmark designed to evaluate LLMs on\nfundamental static analysis tasks. CoRe includes 12,553 task instances spanning\ndata dependency, control dependency, and information flow across programs\nwritten in C/C++, Java, and Python. To ensure semantic diversity and reasoning\ncomplexity, we propose a semantics-aware diverse sampling strategy that selects\ntargets and task instances based on structural coverage and dependency depth.\nWe evaluate 10 mainstream LLMs and show that, while they perform well at\nidentifying dependencies, models still struggle with tasks that require deeper\nsemantic understanding and multi-step reasoning. We further conduct qualitative\nanalyses to uncover key challenges, such as complex control structures and\nbackward dependency patterns, offering insights into improving LLMs code\nreasoning capabilities.", "AI": {"tldr": "CoRe\u662f\u4e00\u4e2a\u9ad8\u8d28\u91cf\u3001\u4eba\u5de5\u9a8c\u8bc1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u9759\u6001\u5206\u6790\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0LLMs\u5728\u6df1\u5c42\u8bed\u4e49\u7406\u89e3\u548c\u591a\u6b65\u63a8\u7406\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u7aef\u5230\u7aef\u7ed3\u679c\uff08\u5982\u4ee3\u7801\u4fee\u590d\u6216\u751f\u6210\uff09\uff0c\u800c\u5ffd\u7565\u4e86LLMs\u5bf9\u7a0b\u5e8f\u8bed\u4e49\u63a8\u7406\u80fd\u529b\u7684\u8bc4\u4f30\u3002", "method": "\u63d0\u51faCoRe\u57fa\u51c6\uff0c\u5305\u542b12,553\u4e2a\u4efb\u52a1\u5b9e\u4f8b\uff0c\u6db5\u76d6\u6570\u636e\u4f9d\u8d56\u3001\u63a7\u5236\u4f9d\u8d56\u548c\u4fe1\u606f\u6d41\uff0c\u91c7\u7528\u8bed\u4e49\u611f\u77e5\u7684\u591a\u6837\u5316\u91c7\u6837\u7b56\u7565\u3002", "result": "\u8bc4\u4f3010\u79cd\u4e3b\u6d41LLMs\uff0c\u53d1\u73b0\u5176\u5728\u4f9d\u8d56\u8bc6\u522b\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u6df1\u5c42\u8bed\u4e49\u7406\u89e3\u548c\u591a\u6b65\u63a8\u7406\u4efb\u52a1\u4e2d\u4ecd\u6709\u56f0\u96be\u3002", "conclusion": "\u63ed\u793a\u4e86LLMs\u5728\u590d\u6742\u63a7\u5236\u7ed3\u6784\u548c\u53cd\u5411\u4f9d\u8d56\u6a21\u5f0f\u7b49\u6311\u6218\uff0c\u4e3a\u63d0\u5347\u5176\u4ee3\u7801\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2507.05270", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.05270", "abs": "https://arxiv.org/abs/2507.05270", "authors": ["Boyuan Li", "Chengwei Liu", "Lingling Fan", "Sen Chen", "Zhenlin Zhang", "Zheli Liu"], "title": "Open Source, Hidden Costs: A Systematic Literature Review on OSS License Management", "comment": null, "summary": "Integrating third-party software components is a common practice in modern\nsoftware development, offering significant advantages in terms of efficiency\nand innovation. However, this practice is fraught with risks related to\nsoftware licensing. A lack of understanding may lead to disputes, which can\npose serious legal and operational challenges. To these ends, both academia and\nindustry have conducted various investigations and proposed solutions and tools\nto deal with these challenges. However, significant limitations still remain.\nMoreover, the rapid evolution of open-source software (OSS) licenses, as well\nas the rapidly incorporated generative software engineering techniques, such as\nlarge language models for code (CodeLLMs), are placing greater demands on the\nsystematic management of software license risks. To unveil the severe\nchallenges and explore possible future directions, we conduct the first\nsystematic literature review (SLR) on 80 carefully selected OSS license-related\npapers, classifying existing research into three key categories, i.e., license\nidentification, license risk assessment, and license risk mitigation. Based on\nthese, we discuss challenges in existing solutions, conclude the opportunities\nto shed light on future research directions and offer practical recommendations\nfor practitioners. We hope this thorough review will help bridge the gaps\nbetween academia and industry and accelerate the ecosystem-wide governance of\nlegitimate software risks within the software engineering community.", "AI": {"tldr": "\u8bba\u6587\u7efc\u8ff0\u4e86\u7b2c\u4e09\u65b9\u8f6f\u4ef6\u7ec4\u4ef6\u96c6\u6210\u4e2d\u7684\u8bb8\u53ef\u8bc1\u98ce\u9669\u95ee\u9898\uff0c\u901a\u8fc7\u7cfb\u7edf\u6587\u732e\u56de\u987e\uff08SLR\uff09\u5206\u6790\u4e8680\u7bc7\u76f8\u5173\u7814\u7a76\uff0c\u5206\u7c7b\u4e3a\u8bb8\u53ef\u8bc1\u8bc6\u522b\u3001\u98ce\u9669\u8bc4\u4f30\u548c\u7f13\u89e3\uff0c\u5e76\u63a2\u8ba8\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u73b0\u4ee3\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u5e7f\u6cdb\u4f7f\u7528\u7b2c\u4e09\u65b9\u7ec4\u4ef6\uff0c\u4f46\u8bb8\u53ef\u8bc1\u95ee\u9898\u53ef\u80fd\u5bfc\u81f4\u6cd5\u5f8b\u548c\u8fd0\u8425\u98ce\u9669\uff0c\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u4ecd\u6709\u5c40\u9650\uff0c\u9700\u7cfb\u7edf\u6027\u7ba1\u7406\u3002", "method": "\u5bf980\u7bc7\u5f00\u6e90\u8f6f\u4ef6\u8bb8\u53ef\u8bc1\u76f8\u5173\u8bba\u6587\u8fdb\u884c\u7cfb\u7edf\u6587\u732e\u56de\u987e\uff08SLR\uff09\uff0c\u5206\u7c7b\u4e3a\u8bb8\u53ef\u8bc1\u8bc6\u522b\u3001\u98ce\u9669\u8bc4\u4f30\u548c\u7f13\u89e3\u3002", "result": "\u603b\u7ed3\u4e86\u73b0\u6709\u7814\u7a76\u7684\u4e0d\u8db3\uff0c\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u548c\u5b9e\u8df5\u5efa\u8bae\u3002", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u6027\u7efc\u8ff0\uff0c\u4e3a\u5b66\u672f\u754c\u548c\u5de5\u4e1a\u754c\u63d0\u4f9b\u8bb8\u53ef\u8bc1\u98ce\u9669\u7ba1\u7406\u7684\u53c2\u8003\uff0c\u63a8\u52a8\u5408\u6cd5\u8f6f\u4ef6\u98ce\u9669\u7684\u751f\u6001\u6cbb\u7406\u3002"}}
{"id": "2507.05272", "categories": ["cs.SE", "cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.05272", "abs": "https://arxiv.org/abs/2507.05272", "authors": ["Daragh King", "Vasileios Koutavas", "Laura Kovacs"], "title": "FuzzFeed: An Automatic Approach to Weakest Precondition Generation using LLMs and Fuzzing", "comment": null, "summary": "The weakest precondition (WP) of a program describes the largest set of\ninitial states from which all terminating executions of the program satisfy a\ngiven postcondition. The generation of WPs is an important task with practical\napplications in areas ranging from verification to run-time error checking.\n  This paper proposes the combination of Large Language Models (LLMs) and fuzz\ntesting for generating WPs. In pursuit of this goal, we introduce Fuzzing\nGuidance (FG); FG acts as a means of directing LLMs towards correct WPs using\nprogram execution feedback. FG utilises fuzz testing for approximately checking\nthe validity and weakness of candidate WPs, this information is then fed back\nto the LLM as a means of context refinement.\n  We demonstrate the effectiveness of our approach on a comprehensive benchmark\nset of deterministic array programs in Java. Our experiments indicate that LLMs\nare capable of producing viable candidate WPs, and that this ability can be\npractically enhanced through FG.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u6a21\u7cca\u6d4b\u8bd5\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u7a0b\u5e8f\u7684\u6700\u5f31\u524d\u7f6e\u6761\u4ef6\uff08WP\uff09\uff0c\u5e76\u901a\u8fc7Fuzzing Guidance\uff08FG\uff09\u5229\u7528\u7a0b\u5e8f\u6267\u884c\u53cd\u9988\u6307\u5bfcLLMs\u751f\u6210\u6b63\u786e\u7684WP\u3002", "motivation": "\u751f\u6210\u7a0b\u5e8f\u7684\u6700\u5f31\u524d\u7f6e\u6761\u4ef6\uff08WP\uff09\u5728\u9a8c\u8bc1\u548c\u8fd0\u884c\u65f6\u9519\u8bef\u68c0\u67e5\u7b49\u9886\u57df\u5177\u6709\u91cd\u8981\u5e94\u7528\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u7ed3\u5408LLMs\u548c\u6a21\u7cca\u6d4b\u8bd5\uff0c\u5f15\u5165FG\u901a\u8fc7\u7a0b\u5e8f\u6267\u884c\u53cd\u9988\u6307\u5bfcLLMs\u751f\u6210WP\uff0c\u5e76\u5229\u7528\u6a21\u7cca\u6d4b\u8bd5\u9a8c\u8bc1\u5019\u9009WP\u7684\u6709\u6548\u6027\u548c\u5f31\u6027\u3002", "result": "\u5728Java\u786e\u5b9a\u6027\u6570\u7ec4\u7a0b\u5e8f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5b9e\u9a8c\u8868\u660eLLMs\u80fd\u591f\u751f\u6210\u53ef\u884c\u7684\u5019\u9009WP\uff0c\u4e14FG\u80fd\u6709\u6548\u63d0\u5347\u5176\u80fd\u529b\u3002", "conclusion": "LLMs\u7ed3\u5408FG\u7684\u65b9\u6cd5\u5728\u751f\u6210WP\u65b9\u9762\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.05279", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.05279", "abs": "https://arxiv.org/abs/2507.05279", "authors": ["Virgile Boraud", "Yannis Bendi-Ouis", "Paul Bernard", "Xavier Hinaut"], "title": "ReservoirChat: Interactive Documentation Enhanced with LLM and Knowledge Graph for ReservoirPy", "comment": null, "summary": "We introduce a tool designed to improve the capabilities of Large Language\nModels (LLMs) in assisting with code development using the ReservoirPy library,\nas well as in answering complex questions in the field of Reservoir Computing.\nBy incorporating external knowledge through Retrieval-Augmented Generation\n(RAG) and knowledge graphs, our approach aims to reduce hallucinations and\nincrease the factual accuracy of generated responses. The system provides an\ninteractive experience similar to ChatGPT, tailored specifically for\nReservoirPy, enabling users to write, debug, and understand Python code while\naccessing reliable domain-specific insights. In our evaluation, while\nproprietary models such as ChatGPT-4o and NotebookLM performed slightly better\non general knowledge questions, our model outperformed them on coding tasks and\nshowed a significant improvement over its base model, Codestral-22B.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u5de5\u5177\uff0c\u901a\u8fc7\u7ed3\u5408\u5916\u90e8\u77e5\u8bc6\uff08RAG\u548c\u77e5\u8bc6\u56fe\u8c31\uff09\u63d0\u5347LLM\u5728ReservoirPy\u5e93\u4ee3\u7801\u5f00\u53d1\u548c\u50a8\u5c42\u8ba1\u7b97\u9886\u57df\u95ee\u9898\u89e3\u7b54\u4e2d\u7684\u80fd\u529b\uff0c\u51cf\u5c11\u5e7b\u89c9\u5e76\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "motivation": "\u63d0\u5347LLM\u5728\u7279\u5b9a\u9886\u57df\uff08\u5982ReservoirPy\u548c\u50a8\u5c42\u8ba1\u7b97\uff09\u7684\u8f85\u52a9\u80fd\u529b\uff0c\u51cf\u5c11\u751f\u6210\u5185\u5bb9\u4e2d\u7684\u9519\u8bef\u3002", "method": "\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u548c\u77e5\u8bc6\u56fe\u8c31\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u4f18\u5316\u6a21\u578b\u5728\u4ee3\u7801\u5f00\u53d1\u548c\u9886\u57df\u95ee\u9898\u89e3\u7b54\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u5728\u7f16\u7801\u4efb\u52a1\u4e0a\u4f18\u4e8eChatGPT-4o\u548cNotebookLM\uff0c\u4e14\u663e\u8457\u4f18\u4e8e\u5176\u57fa\u7840\u6a21\u578bCodestral-22B\u3002", "conclusion": "\u8be5\u5de5\u5177\u5728\u7279\u5b9a\u9886\u57df\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u4ee3\u7801\u5f00\u53d1\u548c\u9886\u57df\u77e5\u8bc6\u89e3\u7b54\u65b9\u9762\u5177\u6709\u4f18\u52bf\u3002"}}
{"id": "2507.05281", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.05281", "abs": "https://arxiv.org/abs/2507.05281", "authors": ["Lingyue Fu", "Hao Guan", "Bolun Zhang", "Haowei Yuan", "Yaoming Zhu", "Jun Xu", "Zongyu Wang", "Lin Qiu", "Xunliang Cai", "Xuezhi Cao", "Weiwen Liu", "Weinan Zhang", "Yong Yu"], "title": "CoreCodeBench: A Configurable Multi-Scenario Repository-Level Benchmark", "comment": null, "summary": "As Large Language Models (LLMs) demonstrate increasingly sophisticated code\nprocessing capabilities, evaluating their performance on engineering-level code\nremains challenging. Existing repository-level benchmarks primarily focus on\nsingle scenarios, such as code generation or bug fixing, without adequately\ncapturing the diversity and complexity of real-world software or project\nengineering workflows. Furthermore, these benchmarks suffer from limited\ncontrollability in question positioning and reliability issues in their\ngenerated test cases. To address these limitations, we present CorePipe, a\nfully automated pipeline that converts repositories into comprehensive test\ncases, and introduce CoreCodeBench, a configurable multi-scenario\nrepository-level benchmark. To simulate real engineering scenarios, CorePipe\ngenerates three types of atomic questions (Development, BugFix, and Test-Driven\nDevelopment) specifically targeting core code segments. These atomic questions\nare further combined into three types of composite questions, with difficulty\nlevels flexibly adjusted through hyperparameter tuning. CoreCodeBench provides\na comprehensive and extensive repository-level benchmark to investigate the\napplicability of LLMs in real-world engineering projects. Experiments with 16\nLLMs across diverse scenarios reveal varying capabilities and offer\nmulti-dimensional insights into LLM performance in engineering contexts. The\ncode for CorePipe is available at\nhttps://github.com/AGI-Eval-Official/CoreCodeBench, and the data for\nCoreCodeBench can be accessed at\nhttps://huggingface.co/collections/tubehhh/corecodebench-68256d2faabf4b1610a08caa.", "AI": {"tldr": "CorePipe\u548cCoreCodeBench\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u7ba1\u9053\u548c\u53ef\u914d\u7f6e\u7684\u591a\u573a\u666f\u4ed3\u5e93\u7ea7\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u771f\u5b9e\u5de5\u7a0b\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5c40\u9650\u4e8e\u5355\u4e00\u573a\u666f\uff0c\u65e0\u6cd5\u5168\u9762\u53cd\u6620\u771f\u5b9e\u8f6f\u4ef6\u5de5\u7a0b\u7684\u591a\u6837\u6027\u548c\u590d\u6742\u6027\uff0c\u4e14\u5b58\u5728\u53ef\u63a7\u6027\u548c\u53ef\u9760\u6027\u95ee\u9898\u3002", "method": "CorePipe\u5c06\u4ed3\u5e93\u8f6c\u6362\u4e3a\u7efc\u5408\u6d4b\u8bd5\u7528\u4f8b\uff0c\u751f\u6210\u4e09\u7c7b\u539f\u5b50\u95ee\u9898\uff08\u5f00\u53d1\u3001\u4fee\u590d\u3001\u6d4b\u8bd5\u9a71\u52a8\u5f00\u53d1\uff09\uff0c\u5e76\u901a\u8fc7\u8d85\u53c2\u6570\u8c03\u6574\u7ec4\u5408\u4e3a\u590d\u5408\u95ee\u9898\u3002CoreCodeBench\u4f5c\u4e3a\u57fa\u51c6\u6d4b\u8bd5\u5de5\u5177\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u4e8616\u79cdLLM\uff0c\u63ed\u793a\u4e86\u5b83\u4eec\u5728\u5de5\u7a0b\u573a\u666f\u4e2d\u7684\u4e0d\u540c\u80fd\u529b\uff0c\u63d0\u4f9b\u4e86\u591a\u7ef4\u5ea6\u7684\u6027\u80fd\u6d1e\u5bdf\u3002", "conclusion": "CoreCodeBench\u4e3aLLM\u5728\u771f\u5b9e\u5de5\u7a0b\u9879\u76ee\u7684\u9002\u7528\u6027\u63d0\u4f9b\u4e86\u5168\u9762\u8bc4\u4f30\u5de5\u5177\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.05289", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.05289", "abs": "https://arxiv.org/abs/2507.05289", "authors": ["Igor Regis da Silva Simoes", "Elaine Venson"], "title": "Measuring how changes in code readability attributes affect code quality evaluation by Large Language Models", "comment": null, "summary": "Code readability is one of the main aspects of code quality, influenced by\nvarious properties like identifier names, comments, code structure, and\nadherence to standards. However, measuring this attribute poses challenges in\nboth industry and academia. While static analysis tools assess attributes such\nas code smells and comment percentage, code reviews introduce an element of\nsubjectivity. This paper explores using Large Language Models (LLMs) to\nevaluate code quality attributes related to its readability in a standardized,\nreproducible, and consistent manner. We conducted a quasi-experiment study to\nmeasure the effects of code changes on Large Language Model (LLM)s\ninterpretation regarding its readability quality attribute. Nine LLMs were\ntested, undergoing three interventions: removing comments, replacing identifier\nnames with obscure names, and refactoring to remove code smells. Each\nintervention involved 10 batch analyses per LLM, collecting data on response\nvariability. We compared the results with a known reference model and tool. The\nresults showed that all LLMs were sensitive to the interventions, with\nagreement with the reference classifier being high for the original and\nrefactored code scenarios. The LLMs demonstrated a strong semantic sensitivity\nthat the reference model did not fully capture. A thematic analysis of the LLMs\nreasoning confirmed their evaluations directly reflected the nature of each\nintervention. The models also exhibited response variability, with 9.37% to\n14.58% of executions showing a standard deviation greater than zero, indicating\nresponse oscillation, though this did not always compromise the statistical\nsignificance of the results. LLMs demonstrated potential for evaluating\nsemantic quality aspects, such as coherence between identifier names, comments,\nand documentation with code purpose.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8bc4\u4f30\u4ee3\u7801\u53ef\u8bfb\u6027\uff0c\u901a\u8fc7\u5b9e\u9a8c\u53d1\u73b0LLMs\u5bf9\u4ee3\u7801\u5e72\u9884\u654f\u611f\uff0c\u5e76\u663e\u793a\u51fa\u8bc4\u4f30\u8bed\u4e49\u8d28\u91cf\u7684\u6f5c\u529b\u3002", "motivation": "\u4ee3\u7801\u53ef\u8bfb\u6027\u662f\u4ee3\u7801\u8d28\u91cf\u7684\u91cd\u8981\u65b9\u9762\uff0c\u4f46\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u4e3b\u89c2\u6027\u548c\u4e0d\u4e00\u81f4\u6027\u3002\u8bba\u6587\u65e8\u5728\u63a2\u7d22LLMs\u80fd\u5426\u63d0\u4f9b\u6807\u51c6\u5316\u3001\u53ef\u590d\u73b0\u7684\u8bc4\u4f30\u65b9\u5f0f\u3002", "method": "\u901a\u8fc7\u51c6\u5b9e\u9a8c\u7814\u7a76\uff0c\u6d4b\u8bd59\u79cdLLMs\u5bf9\u4e09\u79cd\u4ee3\u7801\u5e72\u9884\uff08\u5220\u9664\u6ce8\u91ca\u3001\u66ff\u6362\u6807\u8bc6\u7b26\u540d\u79f0\u3001\u91cd\u6784\u4ee3\u7801\uff09\u7684\u53cd\u5e94\uff0c\u5e76\u4e0e\u53c2\u8003\u6a21\u578b\u5bf9\u6bd4\u3002", "result": "LLMs\u5bf9\u5e72\u9884\u654f\u611f\uff0c\u4e0e\u53c2\u8003\u6a21\u578b\u5728\u539f\u59cb\u548c\u91cd\u6784\u4ee3\u7801\u573a\u666f\u4e2d\u9ad8\u5ea6\u4e00\u81f4\uff0c\u4e14\u663e\u793a\u51fa\u8bed\u4e49\u654f\u611f\u6027\u3002\u54cd\u5e94\u5b58\u5728\u4e00\u5b9a\u53d8\u5f02\u6027\uff0c\u4f46\u4e0d\u5f71\u54cd\u7edf\u8ba1\u663e\u8457\u6027\u3002", "conclusion": "LLMs\u5728\u8bc4\u4f30\u4ee3\u7801\u8bed\u4e49\u8d28\u91cf\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u6807\u8bc6\u7b26\u540d\u79f0\u3001\u6ce8\u91ca\u4e0e\u4ee3\u7801\u76ee\u7684\u7684\u8fde\u8d2f\u6027\u65b9\u9762\u3002"}}
{"id": "2507.05294", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.05294", "abs": "https://arxiv.org/abs/2507.05294", "authors": ["William Law"], "title": "zkSDK: Streamlining zero-knowledge proof development through automated trace-driven ZK-backend selection", "comment": "undergrad thesis", "summary": "The rapid advancement of creating Zero-Knowledge (ZK) programs has led to the\ndevelopment of numerous tools designed to support developers. Popular options\ninclude being able to write in general-purpose programming languages like Rust\nfrom Risc Zero. Other languages exist like Circom, Lib-snark, and Cairo.\nHowever, developers entering the ZK space are faced with many different ZK\nbackends to choose from, leading to a steep learning curve and a fragmented\ndeveloper experience across different platforms. As a result, many developers\ntend to select a single ZK backend and remain tied to it. This thesis\nintroduces zkSDK, a modular framework that streamlines ZK application\ndevelopment by abstracting the backend complexities. At the core of zkSDK is\nPresto, a custom Python-like programming language that enables the profiling\nand analysis of a program to assess its computational workload intensity.\nCombined with user-defined criteria, zkSDK employs a dynamic selection\nalgorithm to automatically choose the optimal ZK-proving backend. Through an\nin-depth analysis and evaluation of real-world workloads, we demonstrate that\nzkSDK effectively selects the best-suited backend from a set of supported ZK\nbackends, delivering a seamless and user-friendly development experience.", "AI": {"tldr": "zkSDK\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u6846\u67b6\uff0c\u65e8\u5728\u7b80\u5316\u96f6\u77e5\u8bc6\uff08ZK\uff09\u5e94\u7528\u5f00\u53d1\uff0c\u901a\u8fc7\u62bd\u8c61\u540e\u7aef\u590d\u6742\u6027\u5e76\u63d0\u4f9b\u52a8\u6001\u9009\u62e9\u6700\u4f18ZK\u540e\u7aef\u7684\u529f\u80fd\u3002", "motivation": "\u5f53\u524dZK\u5f00\u53d1\u5de5\u5177\u4f17\u591a\u4e14\u5206\u6563\uff0c\u5f00\u53d1\u8005\u9762\u4e34\u5b66\u4e60\u66f2\u7ebf\u9661\u5ced\u548c\u5e73\u53f0\u788e\u7247\u5316\u95ee\u9898\uff0c\u5bfc\u81f4\u4ed6\u4eec\u901a\u5e38\u53ea\u80fd\u9009\u62e9\u5e76\u4f9d\u8d56\u5355\u4e00\u540e\u7aef\u3002", "method": "zkSDK\u5f15\u5165Presto\uff08\u4e00\u79cd\u7c7b\u4f3cPython\u7684\u81ea\u5b9a\u4e49\u8bed\u8a00\uff09\u7528\u4e8e\u7a0b\u5e8f\u5206\u6790\u548c\u8ba1\u7b97\u8d1f\u8f7d\u8bc4\u4f30\uff0c\u7ed3\u5408\u7528\u6237\u5b9a\u4e49\u6807\u51c6\uff0c\u52a8\u6001\u9009\u62e9\u6700\u4f18ZK\u540e\u7aef\u3002", "result": "\u901a\u8fc7\u5b9e\u9645\u5de5\u4f5c\u8d1f\u8f7d\u5206\u6790\uff0czkSDK\u80fd\u6709\u6548\u9009\u62e9\u6700\u9002\u5408\u7684\u540e\u7aef\uff0c\u63d0\u4f9b\u65e0\u7f1d\u4e14\u7528\u6237\u53cb\u597d\u7684\u5f00\u53d1\u4f53\u9a8c\u3002", "conclusion": "zkSDK\u901a\u8fc7\u6a21\u5757\u5316\u548c\u81ea\u52a8\u5316\u540e\u7aef\u9009\u62e9\uff0c\u663e\u8457\u6539\u5584\u4e86ZK\u5f00\u53d1\u7684\u788e\u7247\u5316\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u5f00\u53d1\u6548\u7387\u3002"}}
{"id": "2507.05307", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05307", "abs": "https://arxiv.org/abs/2507.05307", "authors": ["Xuanqi Gao", "Juan Zhai", "Shiqing Ma", "Siyi Xie", "Chao Shen"], "title": "ASSURE: Metamorphic Testing for AI-powered Browser Extensions", "comment": null, "summary": "The integration of Large Language Models (LLMs) into browser extensions has\nrevolutionized web browsing, enabling sophisticated functionalities like\ncontent summarization, intelligent translation, and context-aware writing\nassistance. However, these AI-powered extensions introduce unprecedented\nchallenges in testing and reliability assurance. Traditional browser extension\ntesting approaches fail to address the non-deterministic behavior,\ncontext-sensitivity, and complex web environment integration inherent to\nLLM-powered extensions. Similarly, existing LLM testing methodologies operate\nin isolation from browser-specific contexts, creating a critical gap in\neffective evaluation frameworks. To bridge this gap, we present ASSURE, a\nmodular automated testing framework specifically designed for AI-powered\nbrowser extensions. ASSURE comprises three principal components: (1) a modular\ntest case generation engine that supports plugin-based extension of testing\nscenarios, (2) an automated execution framework that orchestrates the complex\ninteractions between web content, extension processing, and AI model behavior,\nand (3) a configurable validation pipeline that systematically evaluates\nbehavioral consistency and security invariants rather than relying on exact\noutput matching. Our evaluation across six widely-used AI browser extensions\ndemonstrates ASSURE's effectiveness, identifying 531 distinct issues spanning\nsecurity vulnerabilities, metamorphic relation violations, and content\nalignment problems. ASSURE achieves 6.4x improved testing throughput compared\nto manual approaches, detecting critical security vulnerabilities within 12.4\nminutes on average. This efficiency makes ASSURE practical for integration into\ndevelopment pipelines, offering a comprehensive solution to the unique\nchallenges of testing AI-powered browser extensions.", "AI": {"tldr": "ASSURE\u662f\u4e00\u4e2a\u4e13\u4e3aAI\u6d4f\u89c8\u5668\u6269\u5c55\u8bbe\u8ba1\u7684\u81ea\u52a8\u5316\u6d4b\u8bd5\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u7684\u975e\u786e\u5b9a\u6027\u884c\u4e3a\u548c\u590d\u6742\u73af\u5883\u96c6\u6210\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u6d4f\u89c8\u5668\u6269\u5c55\u6d4b\u8bd5\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406LLM\u9a71\u52a8\u7684\u6269\u5c55\u7684\u975e\u786e\u5b9a\u6027\u548c\u4e0a\u4e0b\u6587\u654f\u611f\u6027\uff0c\u73b0\u6709LLM\u6d4b\u8bd5\u65b9\u6cd5\u4e5f\u7f3a\u4e4f\u6d4f\u89c8\u5668\u73af\u5883\u96c6\u6210\uff0c\u4e9f\u9700\u65b0\u6846\u67b6\u3002", "method": "ASSURE\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff1a\u6a21\u5757\u5316\u6d4b\u8bd5\u7528\u4f8b\u751f\u6210\u5f15\u64ce\u3001\u81ea\u52a8\u5316\u6267\u884c\u6846\u67b6\u548c\u53ef\u914d\u7f6e\u9a8c\u8bc1\u7ba1\u9053\uff0c\u7cfb\u7edf\u8bc4\u4f30\u884c\u4e3a\u4e00\u81f4\u6027\u548c\u5b89\u5168\u6027\u3002", "result": "\u8bc4\u4f30\u663e\u793aASSURE\u5728\u516d\u4e2a\u5e38\u7528AI\u6269\u5c55\u4e2d\u53d1\u73b0531\u4e2a\u95ee\u9898\uff0c\u6d4b\u8bd5\u541e\u5410\u91cf\u6bd4\u624b\u52a8\u65b9\u6cd5\u9ad86.4\u500d\uff0c\u5e73\u574712.4\u5206\u949f\u68c0\u6d4b\u5230\u5173\u952e\u6f0f\u6d1e\u3002", "conclusion": "ASSURE\u4e3aAI\u6d4f\u89c8\u5668\u6269\u5c55\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u5408\u96c6\u6210\u5230\u5f00\u53d1\u6d41\u7a0b\u4e2d\u3002"}}
{"id": "2507.05316", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05316", "abs": "https://arxiv.org/abs/2507.05316", "authors": ["Koren Lazar", "Matan Vetzler", "Kiran Kate", "Jason Tsay", "David Boaz Himanshu Gupta", "Avraham Shinnar", "Rohith D Vallam", "David Amid Esther Goldbraich", "Guy Uziel", "Jim Laredo", "Ateret Anaby Tavor"], "title": "OASBuilder: Generating OpenAPI Specifications from Online API Documentation with Large Language Models", "comment": null, "summary": "AI agents and business automation tools interacting with external web\nservices require standardized, machine-readable information about their APIs in\nthe form of API specifications. However, the information about APIs available\nonline is often presented as unstructured, free-form HTML documentation,\nrequiring external users to spend significant time manually converting it into\na structured format. To address this, we introduce OASBuilder, a novel\nframework that transforms long and diverse API documentation pages into\nconsistent, machine-readable API specifications. This is achieved through a\ncarefully crafted pipeline that integrates large language models and rule-based\nalgorithms which are guided by domain knowledge of the structure of\ndocumentation webpages. Our experiments demonstrate that OASBuilder generalizes\nwell across hundreds of APIs, and produces valid OpenAPI specifications that\nencapsulate most of the information from the original documentation. OASBuilder\nhas been successfully implemented in an enterprise environment, saving\nthousands of hours of manual effort and making hundreds of complex enterprise\nAPIs accessible as tools for LLMs.", "AI": {"tldr": "OASBuilder\u662f\u4e00\u4e2a\u6846\u67b6\uff0c\u5c06\u975e\u7ed3\u6784\u5316\u7684API\u6587\u6863\u8f6c\u6362\u4e3a\u673a\u5668\u53ef\u8bfb\u7684OpenAPI\u89c4\u8303\uff0c\u7ed3\u5408\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u7b97\u6cd5\u3002", "motivation": "\u5728\u7ebfAPI\u6587\u6863\u591a\u4e3a\u975e\u7ed3\u6784\u5316HTML\uff0c\u9700\u8981\u624b\u52a8\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u683c\u5f0f\uff0c\u8017\u65f6\u4e14\u4f4e\u6548\u3002", "method": "\u901a\u8fc7\u6574\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u7b97\u6cd5\uff0c\u7ed3\u5408\u6587\u6863\u7f51\u9875\u7ed3\u6784\u7684\u9886\u57df\u77e5\u8bc6\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u8f6c\u6362\u7ba1\u9053\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cOASBuilder\u80fd\u6709\u6548\u5904\u7406\u6570\u767e\u79cdAPI\uff0c\u751f\u6210\u5305\u542b\u5927\u90e8\u5206\u539f\u59cb\u6587\u6863\u4fe1\u606f\u7684\u6709\u6548OpenAPI\u89c4\u8303\u3002", "conclusion": "OASBuilder\u5728\u4f01\u4e1a\u73af\u5883\u4e2d\u6210\u529f\u5e94\u7528\uff0c\u8282\u7701\u4e86\u5927\u91cf\u624b\u52a8\u65f6\u95f4\uff0c\u4f7f\u590d\u6742API\u66f4\u6613\u4e8eLLM\u4f7f\u7528\u3002"}}
{"id": "2507.05325", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.05325", "abs": "https://arxiv.org/abs/2507.05325", "authors": ["Lidiany Cerqueira", "Jo\u00e3o Pedro Bastos", "Danilo Neves", "Glauco Carneiro", "Rodrigo Sp\u00ednola", "S\u00e1vio Freire", "Jos\u00e9 Amancio Macedo Santos", "Manoel Mendon\u00e7a"], "title": "Exploring Empathy in Software Engineering: Insights from a Grey Literature Analysis of Practitioners' Perspectives", "comment": "This is the author's version of the paper accepted for publication in\n  ACM Transactions on Software Engineering and Methodology. The final version\n  will be available via the ACM Digital Library. The HTML preview may not\n  render some formatting correctly. Please refer to the PDF version for\n  accurate presentation", "summary": "Context. Empathy, a key social skill, is essential for communication and\ncollaboration in SE but remains an under-researched topic. Aims. This study\ninvestigates empathy in SE from practitioners' perspectives, aiming to\ncharacterize its meaning, identify barriers, discuss practices to overcome\nthem, and explore its effects. Method. A qualitative content analysis was\nconducted on 55 web articles from DEV and Medium, two communities widely used\nby practitioners. To strengthen our findings, we conducted a follow-up survey\nwith empathy experts. Results. The study proposes a definition of empathy in\nSE, identifies barriers such as toxic culture and excessive technical focus,\npractices to foster empathy in teams, and outcomes, including improved\ncollaboration, communication, and reduced anxiety, frustration, and stress.\nThese findings are synthesized into a conceptual framework. Conclusion. Survey\nresults indicate the framework is clear, valuable, and raises empathy\nawareness, with suggestions for improvements and integration into training.\nThis study paves the way for improving team dynamics by addressing barriers and\noffering strategies to cultivate empathy. Future work will explore empathy's\nbroader implications in SE practice.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u540c\u7406\u5fc3\u7684\u5b9a\u4e49\u3001\u969c\u788d\u3001\u5b9e\u8df5\u53ca\u6548\u679c\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u6982\u5ff5\u6846\u67b6\u3002", "motivation": "\u540c\u7406\u5fc3\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7814\u7a76\u4e0d\u8db3\uff0c\u9700\u4ece\u5b9e\u8df5\u8005\u89d2\u5ea6\u6df1\u5165\u63a2\u8ba8\u3002", "method": "\u5bf955\u7bc7DEV\u548cMedium\u6587\u7ae0\u8fdb\u884c\u5b9a\u6027\u5185\u5bb9\u5206\u6790\uff0c\u5e76\u8f85\u4ee5\u4e13\u5bb6\u8c03\u67e5\u9a8c\u8bc1\u3002", "result": "\u63d0\u51fa\u4e86\u540c\u7406\u5fc3\u7684\u5b9a\u4e49\uff0c\u8bc6\u522b\u4e86\u969c\u788d\uff08\u5982\u6bd2\u6027\u6587\u5316\u3001\u8fc7\u5ea6\u6280\u672f\u5bfc\u5411\uff09\uff0c\u5e76\u603b\u7ed3\u4e86\u63d0\u5347\u540c\u7406\u5fc3\u7684\u5b9e\u8df5\u53ca\u5176\u79ef\u6781\u6548\u679c\u3002", "conclusion": "\u6846\u67b6\u6e05\u6670\u4e14\u6709\u4ef7\u503c\uff0c\u672a\u6765\u5c06\u63a2\u7d22\u540c\u7406\u5fc3\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u66f4\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2507.05504", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.05504", "abs": "https://arxiv.org/abs/2507.05504", "authors": ["Alex Kleijwegt", "Sinem Getir Yaman", "Radu Calinescu"], "title": "Tool for Supporting Debugging and Understanding of Normative Requirements Using LLMs", "comment": null, "summary": "Normative requirements specify social, legal, ethical, empathetic, and\ncultural (SLEEC) norms that must be observed by a system. To support the\nidentification of SLEEC requirements, numerous standards and regulations have\nbeen developed. These requirements are typically defined by stakeholders in the\nnon-technical system with diverse expertise (e.g., ethicists, lawyers, social\nscientists). Hence, ensuring their consistency and managing the requirement\nelicitation process are complex and error-prone tasks. Recent research has\naddressed this challenge using domain-specific languages to specify normative\nrequirements as rules, whose consistency can then be analyzed with formal\nmethods. Nevertheless, these approaches often present the results from formal\nverification tools in a way that is inaccessible to non-technical users. This\nhinders understanding and makes the iterative process of eliciting and\nvalidating these requirements inefficient in terms of both time and effort. To\naddress this problem, we introduce SLEEC-LLM, a tool that uses large language\nmodels (LLMs) to provide natural-language interpretations for model-checking\ncounterexamples corresponding to SLEEC rule inconsistencies. SLEEC-LLM improves\nthe efficiency and explainability of normative requirements elicitation and\nconsistency analysis. To demonstrate its effectiveness, we summarise its use in\ntwo real-world case studies involving non-technical stakeholders.", "AI": {"tldr": "SLEEC-LLM\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e3aSLEEC\u89c4\u5219\u4e0d\u4e00\u81f4\u6027\u63d0\u4f9b\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\uff0c\u63d0\u5347\u89c4\u8303\u9700\u6c42\u83b7\u53d6\u548c\u4e00\u81f4\u6027\u5206\u6790\u7684\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u89c4\u8303\u9700\u6c42\uff08SLEEC\uff09\u7684\u8bc6\u522b\u548c\u7ba1\u7406\u590d\u6742\u4e14\u6613\u9519\uff0c\u73b0\u6709\u65b9\u6cd5\u5bf9\u975e\u6280\u672f\u7528\u6237\u4e0d\u53cb\u597d\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u5f00\u53d1SLEEC-LLM\u5de5\u5177\uff0c\u5229\u7528LLM\u5c06\u5f62\u5f0f\u9a8c\u8bc1\u7ed3\u679c\u8f6c\u5316\u4e3a\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u3002", "result": "SLEEC-LLM\u5728\u771f\u5b9e\u6848\u4f8b\u4e2d\u6709\u6548\u63d0\u5347\u4e86\u9700\u6c42\u83b7\u53d6\u548c\u9a8c\u8bc1\u7684\u6548\u7387\u3002", "conclusion": "SLEEC-LLM\u4e3a\u975e\u6280\u672f\u7528\u6237\u63d0\u4f9b\u4e86\u66f4\u6613\u7406\u89e3\u7684\u5de5\u5177\uff0c\u4f18\u5316\u4e86\u89c4\u8303\u9700\u6c42\u7684\u7ba1\u7406\u6d41\u7a0b\u3002"}}
{"id": "2507.05565", "categories": ["cs.SE", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.05565", "abs": "https://arxiv.org/abs/2507.05565", "authors": ["Sangwon Hyun", "Shaukat Ali", "M. Ali Babar"], "title": "Search-based Selection of Metamorphic Relations for Optimized Robustness Testing of Large Language Models", "comment": null, "summary": "Assessing the trustworthiness of Large Language Models (LLMs), such as\nrobustness, has garnered significant attention. Recently, metamorphic testing\nthat defines Metamorphic Relations (MRs) has been widely applied to evaluate\nthe robustness of LLM executions. However, the MR-based robustness testing\nstill requires a scalable number of MRs, thereby necessitating the optimization\nof selecting MRs. Most extant LLM testing studies are limited to automatically\ngenerating test cases (i.e., MRs) to enhance failure detection. Additionally,\nmost studies only considered a limited test space of single perturbation MRs in\ntheir evaluation of LLMs. In contrast, our paper proposes a search-based\napproach for optimizing the MR groups to maximize failure detection and\nminimize the LLM execution cost. Moreover, our approach covers the\ncombinatorial perturbations in MRs, facilitating the expansion of test space in\nthe robustness assessment. We have developed a search process and implemented\nfour search algorithms: Single-GA, NSGA-II, SPEA2, and MOEA/D with novel\nencoding to solve the MR selection problem in the LLM robustness testing. We\nconducted comparative experiments on the four search algorithms along with a\nrandom search, using two major LLMs with primary Text-to-Text tasks. Our\nstatistical and empirical investigation revealed two key findings: (1) the\nMOEA/D algorithm performed the best in optimizing the MR space for LLM\nrobustness testing, and (2) we identified silver bullet MRs for the LLM\nrobustness testing, which demonstrated dominant capabilities in confusing LLMs\nacross different Text-to-Text tasks. In LLM robustness assessment, our research\nsheds light on the fundamental problem for optimized testing and provides\ninsights into search-based solutions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u641c\u7d22\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u9c81\u68d2\u6027\u6d4b\u8bd5\u4e2d\u7684\u53d8\u5f62\u5173\u7cfb\uff08MR\uff09\u9009\u62e9\uff0c\u4ee5\u6700\u5927\u5316\u6545\u969c\u68c0\u6d4b\u5e76\u6700\u5c0f\u5316\u6267\u884c\u6210\u672c\u3002\u5b9e\u9a8c\u8868\u660eMOEA/D\u7b97\u6cd5\u8868\u73b0\u6700\u4f73\uff0c\u5e76\u53d1\u73b0\u4e86\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u8d8a\u7684MR\u3002", "motivation": "\u73b0\u6709LLM\u9c81\u68d2\u6027\u6d4b\u8bd5\u65b9\u6cd5\u9700\u8981\u5927\u91cfMR\u4e14\u6d4b\u8bd5\u7a7a\u95f4\u6709\u9650\uff0c\u4e9f\u9700\u4f18\u5316MR\u9009\u62e9\u548c\u6269\u5c55\u6d4b\u8bd5\u7a7a\u95f4\u3002", "method": "\u63d0\u51fa\u641c\u7d22\u65b9\u6cd5\uff0c\u5b9e\u73b0\u56db\u79cd\u641c\u7d22\u7b97\u6cd5\uff08Single-GA\u3001NSGA-II\u3001SPEA2\u3001MOEA/D\uff09\u4f18\u5316MR\u9009\u62e9\uff0c\u8986\u76d6\u7ec4\u5408\u6270\u52a8\u3002", "result": "MOEA/D\u7b97\u6cd5\u8868\u73b0\u6700\u4f73\uff0c\u5e76\u53d1\u73b0\u4e86\u4e00\u4e9b\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u8d8a\u7684MR\u3002", "conclusion": "\u7814\u7a76\u4e3aLLM\u9c81\u68d2\u6027\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u4f18\u5316\u65b9\u6cd5\u548c\u641c\u7d22\u89e3\u51b3\u65b9\u6848\u7684\u89c1\u89e3\u3002"}}
{"id": "2507.05932", "categories": ["cs.SE", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.05932", "abs": "https://arxiv.org/abs/2507.05932", "authors": ["You Lu", "Dingji Wang", "Kaifeng Huang", "Bihuan Chen", "Xin Peng"], "title": "TigAug: Data Augmentation for Testing Traffic Light Detection in Autonomous Driving Systems", "comment": null, "summary": "Autonomous vehicle technology has been developed in the last decades with\nrecent advances in sensing and computing technology. There is an urgent need to\nensure the reliability and robustness of autonomous driving systems (ADSs).\nDespite the recent achievements in testing various ADS modules, little\nattention has been paid on the automated testing of traffic light detection\nmodels in ADSs. A common practice is to manually collect and label traffic\nlight data. However, it is labor-intensive, and even impossible to collect\ndiverse data under different driving environments.\n  To address these problems, we propose and implement TigAug to automatically\naugment labeled traffic light images for testing traffic light detection models\nin ADSs. We construct two families of metamorphic relations and three families\nof transformations based on a systematic understanding of weather environments,\ncamera properties, and traffic light properties. We use augmented images to\ndetect erroneous behaviors of traffic light detection models by\ntransformation-specific metamorphic relations, and to improve the performance\nof traffic light detection models by retraining. Large-scale experiments with\nfour state-of-the-art traffic light detection models and two traffic light\ndatasets have demonstrated that i) TigAug is effective in testing traffic light\ndetection models, ii) TigAug is efficient in synthesizing traffic light images,\nand iii) TigAug generates traffic light images with acceptable naturalness.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faTigAug\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u52a8\u589e\u5f3a\u6807\u8bb0\u7684\u4ea4\u901a\u706f\u56fe\u50cf\u6765\u6d4b\u8bd5\u548c\u6539\u8fdb\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u7684\u4ea4\u901a\u706f\u68c0\u6d4b\u6a21\u578b\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\uff08ADS\uff09\u4e2d\u4ea4\u901a\u706f\u68c0\u6d4b\u6a21\u578b\u7684\u81ea\u52a8\u5316\u6d4b\u8bd5\u7814\u7a76\u4e0d\u8db3\uff0c\u4f20\u7edf\u624b\u52a8\u6570\u636e\u6536\u96c6\u65b9\u6cd5\u6548\u7387\u4f4e\u4e14\u96be\u4ee5\u8986\u76d6\u591a\u6837\u73af\u5883\u3002", "method": "\u57fa\u4e8e\u5929\u6c14\u3001\u76f8\u673a\u548c\u4ea4\u901a\u706f\u7279\u6027\uff0c\u6784\u5efa\u4e86\u4e24\u7c7b\u53d8\u5f62\u5173\u7cfb\u548c\u4e09\u7c7b\u53d8\u6362\uff0c\u901a\u8fc7\u589e\u5f3a\u56fe\u50cf\u68c0\u6d4b\u6a21\u578b\u9519\u8bef\u884c\u4e3a\u5e76\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660eTigAug\u80fd\u6709\u6548\u6d4b\u8bd5\u6a21\u578b\u3001\u9ad8\u6548\u5408\u6210\u56fe\u50cf\uff0c\u4e14\u751f\u6210\u56fe\u50cf\u5177\u6709\u53ef\u63a5\u53d7\u7684\u81ea\u7136\u5ea6\u3002", "conclusion": "TigAug\u4e3a\u4ea4\u901a\u706f\u68c0\u6d4b\u6a21\u578b\u7684\u6d4b\u8bd5\u548c\u6027\u80fd\u63d0\u5347\u63d0\u4f9b\u4e86\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.05981", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.05981", "abs": "https://arxiv.org/abs/2507.05981", "authors": ["Marc Oriol", "Quim Motger", "Jordi Marco", "Xavier Franch"], "title": "Multi-Agent Debate Strategies to Enhance Requirements Engineering with Large Language Models", "comment": null, "summary": "Context: Large Language Model (LLM) agents are becoming widely used for\nvarious Requirements Engineering (RE) tasks. Research on improving their\naccuracy mainly focuses on prompt engineering, model fine-tuning, and retrieval\naugmented generation. However, these methods often treat models as isolated\nblack boxes - relying on single-pass outputs without iterative refinement or\ncollaboration, limiting robustness and adaptability. Objective: We propose\nthat, just as human debates enhance accuracy and reduce bias in RE tasks by\nincorporating diverse perspectives, different LLM agents debating and\ncollaborating may achieve similar improvements. Our goal is to investigate\nwhether Multi-Agent Debate (MAD) strategies can enhance RE performance. Method:\nWe conducted a systematic study of existing MAD strategies across various\ndomains to identify their key characteristics. To assess their applicability in\nRE, we implemented and tested a preliminary MAD-based framework for RE\nclassification. Results: Our study identified and categorized several MAD\nstrategies, leading to a taxonomy outlining their core attributes. Our\npreliminary evaluation demonstrated the feasibility of applying MAD to RE\nclassification. Conclusions: MAD presents a promising approach for improving\nLLM accuracy in RE tasks. This study provides a foundational understanding of\nMAD strategies, offering insights for future research and refinements in RE\napplications.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\uff08MAD\uff09\u7b56\u7565\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u9700\u6c42\u5de5\u7a0b\uff08RE\uff09\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u521d\u6b65\u6846\u67b6\u5e76\u9a8c\u8bc1\u4e86\u5176\u53ef\u884c\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06LLM\u89c6\u4e3a\u5b64\u7acb\u9ed1\u7bb1\uff0c\u7f3a\u4e4f\u8fed\u4ee3\u4f18\u5316\u548c\u534f\u4f5c\uff0c\u9650\u5236\u4e86\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002\u53d7\u4eba\u7c7b\u8fa9\u8bba\u542f\u53d1\uff0c\u7814\u7a76MAD\u7b56\u7565\u662f\u5426\u80fd\u63d0\u5347RE\u4efb\u52a1\u7684\u6027\u80fd\u3002", "method": "\u7cfb\u7edf\u7814\u7a76\u73b0\u6709MAD\u7b56\u7565\uff0c\u5206\u7c7b\u5176\u6838\u5fc3\u7279\u5f81\uff0c\u5e76\u521d\u6b65\u5b9e\u73b0\u4e86\u4e00\u4e2a\u57fa\u4e8eMAD\u7684RE\u5206\u7c7b\u6846\u67b6\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u7814\u7a76\u603b\u7ed3\u4e86MAD\u7b56\u7565\u7684\u5206\u7c7b\u4f53\u7cfb\uff0c\u521d\u6b65\u8bc4\u4f30\u663e\u793aMAD\u5728RE\u5206\u7c7b\u4e2d\u5177\u6709\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "MAD\u662f\u63d0\u5347LLM\u5728RE\u4efb\u52a1\u4e2d\u51c6\u786e\u6027\u7684\u6709\u524d\u666f\u65b9\u6cd5\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u548c\u5e94\u7528\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.05995", "categories": ["cs.SE", "68Nxx", "D.2.0; D.2.8"], "pdf": "https://arxiv.org/pdf/2507.05995", "abs": "https://arxiv.org/abs/2507.05995", "authors": ["Pengzhou Chen", "Tao Chen"], "title": "PromiseTune: Unveiling Causally Promising and Explainable Configuration Tuning", "comment": "This paper has been accepted by ICSE26", "summary": "The high configurability of modern software systems has made configuration\ntuning a crucial step for assuring system performance, e.g., latency or\nthroughput. However, given the expensive measurements, large configuration\nspace, and rugged configuration landscape, existing tuners suffer\nineffectiveness due to the difficult balance of budget utilization between\nexploring uncertain regions (for escaping from local optima) and exploiting\nguidance of known good configurations (for fast convergence). The root cause is\nthat we lack knowledge of where the promising regions lay, which also causes\nchallenges in the explainability of the results.\n  In this paper, we propose PromiseTune that tunes configuration guided by\ncausally purified rules. PromiseTune is unique in the sense that we learn\nrules, which reflect certain regions in the configuration landscape, and purify\nthem with causal inference. The remaining rules serve as approximated\nreflections of the promising regions, bounding the tuning to emphasize these\nplaces in the landscape. This, as we demonstrate, can effectively mitigate the\nimpact of the exploration and exploitation trade-off. Those purified regions\ncan then be paired with the measured configurations to provide spatial\nexplainability at the landscape level. Comparing with 11 state-of-the-art\ntuners on 12 systems and varying budgets, we show that PromiseTune performs\nsignificantly better than the others with $42\\%$ superior rank to the overall\nsecond best while providing richer information to explain the hidden system\ncharacteristics.", "AI": {"tldr": "PromiseTune\u901a\u8fc7\u56e0\u679c\u63a8\u65ad\u7eaf\u5316\u7684\u89c4\u5219\u6307\u5bfc\u914d\u7f6e\u8c03\u4f18\uff0c\u6709\u6548\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u4ee3\u8f6f\u4ef6\u7cfb\u7edf\u7684\u9ad8\u53ef\u914d\u7f6e\u6027\u4f7f\u914d\u7f6e\u8c03\u4f18\u6210\u4e3a\u786e\u4fdd\u6027\u80fd\u7684\u5173\u952e\u6b65\u9aa4\uff0c\u4f46\u73b0\u6709\u8c03\u4f18\u5de5\u5177\u56e0\u63a2\u7d22\u4e0e\u5229\u7528\u7684\u5e73\u8861\u95ee\u9898\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51faPromiseTune\uff0c\u901a\u8fc7\u5b66\u4e60\u5e76\u56e0\u679c\u7eaf\u5316\u89c4\u5219\uff0c\u754c\u5b9a\u6709\u6f5c\u529b\u7684\u914d\u7f6e\u533a\u57df\uff0c\u6307\u5bfc\u8c03\u4f18\u8fc7\u7a0b\u3002", "result": "\u572812\u4e2a\u7cfb\u7edf\u548c\u4e0d\u540c\u9884\u7b97\u4e0b\uff0cPromiseTune\u8868\u73b0\u663e\u8457\u4f18\u4e8e11\u79cd\u73b0\u6709\u65b9\u6cd5\uff0c\u6392\u540d\u4f18\u52bf\u8fbe42%\u3002", "conclusion": "PromiseTune\u4e0d\u4ec5\u63d0\u5347\u4e86\u8c03\u4f18\u6548\u679c\uff0c\u8fd8\u901a\u8fc7\u7eaf\u5316\u533a\u57df\u63d0\u4f9b\u4e86\u914d\u7f6e\u7a7a\u95f4\u7684\u89e3\u91ca\u6027\u3002"}}
{"id": "2507.06014", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.06014", "abs": "https://arxiv.org/abs/2507.06014", "authors": ["Tim Puhlf\u00fcr\u00df", "Julia Butzke", "Walid Maalej"], "title": "Model Cards Revisited: Bridging the Gap Between Theory and Practice for Ethical AI Requirements", "comment": "Accepted for publication at the 33rd IEEE International Requirements\n  Engineering 2025 conference", "summary": "Model cards are the primary documentation framework for developers of\nartificial intelligence (AI) models to communicate critical information to\ntheir users. Those users are often developers themselves looking for relevant\ndocumentation to ensure that their AI systems comply with the ethical\nrequirements of existing laws, guidelines, and standards. Recent studies\nindicate inadequate model documentation practices, suggesting a gap between AI\nrequirements and current practices in model documentation. To understand this\ngap and provide actionable guidance to bridge it, we conducted a thematic\nanalysis of 26 guidelines on ethics and AI, three AI documentation frameworks,\nthree quantitative studies of model cards, and ten actual model cards. We\nidentified a total of 43 ethical requirements relevant to model documentation\nand organized them into a taxonomy featuring four themes and twelve sub-themes\nrepresenting ethical principles. Our findings indicate that model developers\npredominantly emphasize model capabilities and reliability in the documentation\nwhile overlooking other ethical aspects, such as explainability, user autonomy,\nand fairness. This underscores the need for enhanced support in documenting\nethical AI considerations. Our taxonomy serves as a foundation for a revised\nmodel card framework that holistically addresses ethical AI requirements.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86AI\u6a21\u578b\u6587\u6863\uff08\u6a21\u578b\u5361\uff09\u7684\u73b0\u72b6\uff0c\u53d1\u73b0\u5f00\u53d1\u8005\u4e3b\u8981\u5173\u6ce8\u6a21\u578b\u80fd\u529b\u548c\u53ef\u9760\u6027\uff0c\u800c\u5ffd\u7565\u4e86\u5176\u4ed6\u4f26\u7406\u8981\u6c42\u3002\u901a\u8fc7\u5206\u679026\u4efd\u4f26\u7406\u6307\u5357\u548c\u5b9e\u9645\u6a21\u578b\u5361\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b43\u9879\u4f26\u7406\u8981\u6c42\u7684\u5206\u7c7b\u6cd5\uff0c\u4e3a\u6539\u8fdb\u6a21\u578b\u5361\u6846\u67b6\u63d0\u4f9b\u57fa\u7840\u3002", "motivation": "\u5f53\u524dAI\u6a21\u578b\u6587\u6863\u5b9e\u8df5\u4e0d\u8db3\uff0c\u5b58\u5728\u4f26\u7406\u8981\u6c42\u4e0e\u5b9e\u9645\u6587\u6863\u5185\u5bb9\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u9700\u8981\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u6307\u5bfc\u6765\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u5bf926\u4efd\u4f26\u7406\u548cAI\u6307\u5357\u30013\u4e2aAI\u6587\u6863\u6846\u67b6\u30013\u9879\u6a21\u578b\u5361\u5b9a\u91cf\u7814\u7a76\u548c10\u4efd\u5b9e\u9645\u6a21\u578b\u5361\u8fdb\u884c\u4e86\u4e3b\u9898\u5206\u6790\u3002", "result": "\u8bc6\u522b\u51fa43\u9879\u4e0e\u6a21\u578b\u6587\u6863\u76f8\u5173\u7684\u4f26\u7406\u8981\u6c42\uff0c\u5e76\u5c06\u5176\u5206\u7c7b\u4e3a4\u4e2a\u4e3b\u9898\u548c12\u4e2a\u5b50\u4e3b\u9898\u3002\u53d1\u73b0\u5f00\u53d1\u8005\u4e3b\u8981\u5173\u6ce8\u6a21\u578b\u80fd\u529b\u548c\u53ef\u9760\u6027\uff0c\u5ffd\u7565\u4e86\u5176\u4ed6\u4f26\u7406\u65b9\u9762\u3002", "conclusion": "\u9700\u8981\u6539\u8fdb\u6a21\u578b\u5361\u6846\u67b6\u4ee5\u5168\u9762\u6ee1\u8db3\u4f26\u7406\u8981\u6c42\uff0c\u63d0\u51fa\u7684\u5206\u7c7b\u6cd5\u4e3a\u6b64\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
