<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 9]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Quality attributes of test cases and test suites -- importance & challenges from practitioners' perspectives](https://arxiv.org/abs/2507.06343)
*Huynh Khanh Vi Tran,Nauman bin Ali,Michael Unterkalmsteiner,Jürgen Börstler,Panagiota Chatzipetrou*

Main category: cs.SE

TL;DR: 论文研究了测试用例和测试套件的质量属性在实践中的相对重要性，通过工业调查发现Fault Detection等属性最重要，同时揭示了实践中的挑战。


<details>
  <summary>Details</summary>
Motivation: 理解测试用例和测试套件质量属性的相对重要性及实践中的挑战，为研究方向和公司支持提供依据。

Method: 基于文献综述设计问卷，通过LinkedIn采样，收集354份从业者反馈。

Result: Fault Detection、Usability等属性最重要，资源效率等属性意见分歧；挑战包括定义不足、缺乏指标等。

Conclusion: 研究结果可为学术研究方向和企业支持实践者提供指导，以提升测试质量。

Abstract: Context: The quality of the test suites and the constituent test cases
significantly impacts confidence in software testing. While research has
identified several quality attributes of test cases and test suites, there is a
need for a better understanding of their relative importance in practice.
Objective: We investigate practitioners' perceptions regarding the relative
importance of quality attributes of test cases and test suites and the
challenges they face in ensuring the perceived important quality attributes.
Method: We conducted an industrial survey using a questionnaire based on the
quality attributes identified in an extensive literature review. We used a
sampling strategy that leverages LinkedIn to draw a large and heterogeneous
sample of professionals with experience in software testing. Results: We
collected 354 responses from practitioners with a wide range of experience. We
found that the majority of practitioners rated Fault Detection, Usability,
Maintainability, Reliability, and Coverage to be the most important quality
attributes. Resource Efficiency, Reusability, and Simplicity received the most
divergent opinions, which, according to our analysis, depend on the
software-testing contexts. We identified common challenges that apply to the
important attributes, namely inadequate definition, lack of useful metrics,
lack of an established review process, and lack of external support.
Conclusion: The findings point out where practitioners actually need further
support with respect to achieving high-quality test cases and test suites under
different software testing contexts. The findings can serve as a guideline for
academic researchers when looking for research directions on the topic. The
findings can also be used to encourage companies to provide more support to
practitioners to achieve high-quality test cases and test suites.

</details>


### [2] [A proposal and assessment of an improved heuristic for the Eager Test smell detection](https://arxiv.org/abs/2507.06354)
*Huynh Khanh Vi Tran,Nauman bin Ali,Michael Unterkalmsteiner,Jürgen Börstler*

Main category: cs.SE

TL;DR: 论文改进了检测Eager Test smell的规则，提出了一种新的定义和启发式方法，并通过实验验证其优于现有规则。


<details>
  <summary>Details</summary>
Motivation: 现有检测Eager Test smell的规则存在不足，导致检测结果不一致，无法满足实践需求。

Method: 通过文献综述分析现有定义和规则，提出新的定义和启发式方法，并在300个Java单元测试案例中手动验证。

Result: 发现现有规则因定义不明确导致检测结果不一致，新启发式方法能更准确地识别Eager Test smell。

Conclusion: 新启发式方法更精确地捕捉了Eager Test smell的本质，有望解决实践中的问题。

Abstract: Context: The evidence for the prevalence of test smells at the unit testing
level has relied on the accuracy of detection tools, which have seen intense
research in the last two decades. The Eager Test smell, one of the most
prevalent, is often identified using simplified detection rules that
practitioners find inadequate. Objective: We aim to improve the rules for
detecting the Eager Test smell. Method: We reviewed the literature on test
smells to analyze the definitions and detection rules of the Eager Test smell.
We proposed a novel, unambiguous definition of the test smell and a heuristic
to address the limitations of the existing rules. We evaluated our heuristic
against existing detection rules by manually applying it to 300 unit test cases
in Java. Results: Our review identified 56 relevant studies. We found that
inadequate interpretations of original definitions of the Eager Test smell led
to imprecise detection rules, resulting in a high level of disagreement in
detection outcomes. Also, our heuristic detected patterns of eager and
non-eager tests that existing rules missed. Conclusion: Our heuristic captures
the essence of the Eager Test smell more precisely; hence, it may address
practitioners' concerns regarding the adequacy of existing detection rules.

</details>


### [3] [Evaluating Efficiency and Novelty of LLM-Generated Code for Graph Analysis](https://arxiv.org/abs/2507.06463)
*Atieh Barati Nia,Mohammad Dindoost,David A. Bader*

Main category: cs.SE

TL;DR: 研究评估了8种先进LLM在生成高效C语言图分析代码方面的能力，发现Claude Sonnet 4 Extended表现最佳，但LLM在创新算法方面仍有局限。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注功能正确性或高级语言（如Python），缺乏对LLM生成高效C语言代码的系统评估。

Method: 采用两种方法：一是测试LLM生成优于现有算法的代码能力，二是评估其生成可集成到基准测试中的图算法能力。

Result: Claude Sonnet 4 Extended在生成即用代码和效率方面表现最佳，甚至超越人工编写的基线。

Conclusion: 当代LLM擅长优化和集成现有算法，但在发明新技术方面仍有不足。

Abstract: Large Language Models (LLMs) are increasingly used to automate software
development, yet most prior evaluations focus on functional correctness or
high-level languages such as Python. We present the first systematic study of
LLMs' ability to generate efficient C implementations of graph-analysis
routines--code that must satisfy the stringent runtime and memory constraints.
Eight state-of-the-art models (OpenAI ChatGPT o3 and o4-mini-high, Anthropic
Claude 4 Sonnet and Sonnet Extended, Google Gemini 2.5 Flash and Pro, xAI Grok
3-Think, and DeepSeek DeepThink R1) are benchmarked by two distinct approaches.
The first approach checks the ability of LLMs in generating an algorithm
outperforming other present algorithms in the benchmark. The second approach
evaluates the ability of LLMs to generate graph algorithms for integration into
the benchmark. Results show that Claude Sonnet 4 Extended achieves the best
result in the case of ready-to-use code generation and efficiency,
outperforming human-written baselines in triangle counting. The study confirms
that contemporary LLMs excel at optimizing and integrating established
algorithms but not inventing novel techniques. We provide prompts, the first
approach's generated code, and measurement scripts to foster reproducible
research.

</details>


### [4] [Issue Tracking Ecosystems: Context and Best Practices](https://arxiv.org/abs/2507.06704)
*Lloyd Montgomery*

Main category: cs.SE

TL;DR: 该论文探讨了问题跟踪生态系统（ITE）的复杂性，提出了最佳实践本体以解决研究和实践中的对齐问题。


<details>
  <summary>Details</summary>
Motivation: 理解问题跟踪生态系统（ITE）的复杂性和多样性，填补现有研究的空白。

Method: 通过访谈从业者和对多种问题跟踪系统（ITS）进行档案分析。

Result: 揭示了ITE问题的上下文依赖性，提出了最佳实践本体。

Conclusion: 需要更多上下文特定的ITE研究，最佳实践本体有助于对齐研究和实践。

Abstract: Issue Tracking Systems (ITSs), such as GitHub and Jira, are popular tools
that support Software Engineering (SE) organisations through the management of
``issues'', which represent different SE artefacts such as requirements,
development tasks, and maintenance items. ITSs also support internal linking
between issues, and external linking to other tools and information sources.
This provides SE organisations key forms of documentation, including forwards
and backwards traceability (e.g., Feature Requests linked to sprint releases
and code commits linked to Bug Reports). An Issue Tracking Ecosystem (ITE) is
the aggregate of the central ITS and the related SE artefacts, stakeholders,
and processes -- with an emphasis on how these contextual factors interact with
the ITS. The quality of ITEs is central to the success of these organisations
and their software products. There are challenges, however, within ITEs,
including complex networks of interlinked artefacts and diverse workflows.
While ITSs have been the subject of study in SE research for decades, ITEs as a
whole need further exploration.
  In this thesis, I undertake the challenge of understanding ITEs at a broader
level, addressing these questions regarding complexity and diversity. I
interviewed practitioners and performed archival analysis on a diverse set of
ITSs. These analyses revealed the context-dependent nature of ITE problems,
highlighting the need for context-specific ITE research. While previous work
has produced many solutions to specific ITS problems, these solutions are not
consistently framed in a context-rich and comparable way, leading to a desire
for more aligned solutions across research and practice. To address this
emergent information and lack of alignment, I created the Best Practice
Ontology for ITEs. <... truncated due to arXiv abstract character limit ...>

</details>


### [5] [Leveraging LLMs for Semantic Conflict Detection via Unit Test Generation](https://arxiv.org/abs/2507.06762)
*Nathalia Barbosa,Paulo Borba,Léuson Da Silva*

Main category: cs.SE

TL;DR: 论文提出了一种基于Code Llama 70B的新测试生成工具，用于改进SMAT在检测语义冲突时的性能，尽管在复杂场景中仍存在挑战。


<details>
  <summary>Details</summary>
Motivation: 传统合并工具无法检测语义冲突，SMAT虽有效但存在高假阴性率，研究探索LLM是否能克服现有测试生成工具的局限性。

Method: 集成基于Code Llama 70B的新测试生成工具到SMAT中，评估不同交互策略、提示内容和参数配置的效果。

Result: LLM生成的测试在复杂场景中仍具挑战性和计算成本高，但显示出改进语义冲突检测的潜力。

Conclusion: LLM在测试生成方面有潜力，但需进一步优化以应对复杂系统的需求。

Abstract: Semantic conflicts arise when a developer introduces changes to a codebase
that unintentionally affect the behavior of changes integrated in parallel by
other developers. Traditional merge tools are unable to detect such conflicts,
so complementary tools like SMAT have been proposed. SMAT relies on generating
and executing unit tests: if a test fails on the base version, passes on a
developer's modified version, but fails again after merging with another
developer's changes, a semantic conflict is indicated. While SMAT is effective
at detecting conflicts, it suffers from a high rate of false negatives, partly
due to the limitations of unit test generation tools such as Randoop and
Evosuite. To investigate whether large language models (LLMs) can overcome
these limitations, we propose and integrate a new test generation tool based on
Code Llama 70B into SMAT. We explore the model's ability to generate tests
using different interaction strategies, prompt contents, and parameter
configurations. Our evaluation uses two samples: a benchmark with simpler
systems from related work, and a more significant sample based on complex,
real-world systems. We assess the effectiveness of the new SMAT extension in
detecting conflicts. Results indicate that, although LLM-based test generation
remains challenging and computationally expensive in complex scenarios, there
is promising potential for improving semantic conflict detection.
  --
  Conflitos sem^anticos surgem quando um desenvolvedor introduz mudan\c{c}as em
uma base de c\'odigo que afetam, de forma n~ao intencional, o comportamento de
altera\c{c}~oes integradas em paralelo por outros desenvolvedores. Ferramentas
tradicionais de merge n~ao conseguem detectar esse tipo de conflito, por isso
ferramentas complementares como o SMAT foram propostas. O SMAT depende da
gera\c{c}~ao e execu\c{c}~ao de testes de unidade: se um teste falha na vers~ao
base, passa na vers~ao modificada por um desenvolvedor, mas volta a falhar
ap\'os o merge com as mudan\c{c}as de outro desenvolvedor, um conflito
sem^antico \'e identificado. Embora o SMAT seja eficaz na detec\c{c}~ao de
conflitos, apresenta alta taxa de falsos negativos, em parte devido \`as
limita\c{c}~oes das ferramentas de gera\c{c}~ao de testes como Randoop e
Evosuite. Para investigar se modelos de linguagem de grande porte (LLMs) podem
superar essas limita\c{c}~oes, propomos e integramos ao SMAT uma nova
ferramenta de gera\c{c}~ao de testes baseada no Code Llama 70B. Exploramos a
capacidade do modelo de gerar testes utilizando diferentes estrat\'egias de
intera\c{c}~ao, conte\'udos de prompts e configura\c{c}~oes de par^ametros.
Nossa avalia\c{c}~ao utiliza duas amostras: um benchmark com sistemas mais
simples, usados em trabalhos relacionados, e uma amostra mais significativa
baseada em sistemas complexos e reais. Avaliamos a efic\'acia da nova extens~ao
do SMAT na detec\c{c}~ao de conflitos. Os resultados indicam que, embora a
gera\c{c}~ao de testes por LLM em cen\'arios complexos ainda seja desafiadora e
custosa computacionalmente, h\'a potencial promissor para aprimorar a
detec\c{c}~ao de conflitos sem^anticos.

</details>


### [6] [Formalization of the AADL Run-Time Services with Time](https://arxiv.org/abs/2507.06881)
*Brian R Larson,Ehsan Ahmad*

Main category: cs.SE

TL;DR: 本文扩展并简化了AADL的形式化语义，引入时间建模，并扩展了AADL标准的运行时服务以支持行为规范语言。


<details>
  <summary>Details</summary>
Motivation: AADL标准缺乏对时间的明确建模，且运行时服务未涵盖行为规范语言的需求。

Method: 使用基于Kripke结构的模态逻辑进行形式化建模，并扩展AADL运行时服务以支持BLESS和BA语言。

Result: 提出了一个包含时间建模的AADL运行时服务实现示例，展示了HAMR工具对BLESS行为的支持。

Conclusion: 该研究填补了AADL在时间建模和行为规范语言支持上的空白，为嵌入式系统设计提供了更完整的工具链。

Abstract: The Architecture Analysis & Design Language (AADL) is an architecture
description language for design of cyber-physical systems--machines controlled
by software. The AADL standard, SAE International AS5506D, describes Run-Time
Services (RTS) to be provided to execute AADL models in accordance with
semantics defined by the standard. The RTS of primary concern are transport
services and timing services. Although, the study presented in [1] sets a
foundation for the formal semantics of AADL, but without modeling time. This
paper extends and simplifies this formalization using a modal logic defined by
a Kripke structure, to explicitly include time. The RTS defined in the AADL
standard are also expanded to support reactive state-transition machines of the
Behavior Specification annex standard language (BA) and its closely-related,
formally-defined counterpart, the Behavior Language for Embedded Systems with
Software (BLESS). An example of AADL RTS with time, implemented by the High
Assurance Modeling and Rapid Engineering for Embedded Systems (HAMR) for
state-transition machine behavior written in BLESS, is also presented.

</details>


### [7] [Are They All Good? Evaluating the Quality of CoTs in LLM-based Code Generation](https://arxiv.org/abs/2507.06980)
*Binquan Zhang,Li Zhang,Zhiwen Luo,Yuxin Du,Fang Liu,Song Wang,Lin Shi*

Main category: cs.SE

TL;DR: 论文探讨了大语言模型（LLMs）在代码生成中链式思维（CoT）提示技术的质量及其影响因素，发现外部和内部因素均影响CoT质量，并提出了改进方法。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs生成的CoT质量及其对代码生成的影响，以提升代码的正确性和可靠性。

Method: 通过分析1,023个失败代码样本和210个CoT-代码对，评估CoT质量，并通过提示LLMs改进低质量CoT。

Result: 发现外部因素（53.60%）和内部因素（40.10%）影响CoT质量；即使CoT正确，18.5%的代码仍有错误；改进低质量CoT可行。

Conclusion: 研究揭示了CoT在代码生成中的关键挑战，为提升LLM推理和可靠性提供了方向。

Abstract: Large language models (LLMs) have demonstrated impressive performance in code
generation, particularly when augmented with chain-of-thought (CoT) prompting
techniques. They break down requirements into intermediate reasoning steps,
which act as design rationales to guide LLMs in writing code like human
programmers. Thus, the quality of these steps is crucial for ensuring the
correctness and reliability of the generated code. However, little is known
about the quality of CoT generated by LLMs. To what extent can we trust the
thoughts generated by LLMs? How good are they? This paper empirically explores
the external and internal factors of why LLMs generate unsatisfactory CoTs by
analyzing 1,023 failed code samples on two widely used code generation
benchmarks. We also evaluate their impact on code generation performance by
analyzing 210 CoT-code pairs and refining the unsatisfied CoTs by prompting
LLMs. Our study reveals three key findings: (1) External factors (53.60%), such
as unclear requirements and lack of context, mainly affect CoT quality, while
internal factors (40.10%) stem from LLMs' misunderstanding prompts. (2) Even
when CoTs are correct, 18.5% of the generated code contains errors due to
instruction-following issues; conversely, 11.90% of correct code is paired with
flawed CoTs. (3) Refining low-quality CoTs is feasible, i.e., LLMs improve when
given detailed problem descriptions. These findings highlight key challenges in
CoT-based code generation and suggest directions for improving LLM reasoning
and reliability.

</details>


### [8] [Exploring Fairness Interventions in Open Source Projects](https://arxiv.org/abs/2507.07026)
*Sadia Afrin Mim,Fatema Tuz Zohra,Justin Smith,Brittany Johnson*

Main category: cs.SE

TL;DR: 论文分析了62种开源公平性干预措施，发现32%在过去一年内积极维护，50%提供偏差检测和缓解功能。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型的偏见在关键领域（如刑事司法和医疗）产生了负面影响，但公平性干预措施的采用率低。

Method: 系统识别并分析62种开源公平性干预措施，评估其维护状态和功能。

Result: 32%的干预措施在过去一年内积极维护，50%提供偏差检测和缓解功能。

Conclusion: 开源公平性干预措施的维护和功能分布不均，需提高实践者的认知和采用。

Abstract: The deployment of biased machine learning (ML) models has resulted in adverse
effects in crucial sectors such as criminal justice and healthcare. To address
these challenges, a diverse range of machine learning fairness interventions
have been developed, aiming to mitigate bias and promote the creation of more
equitable models. Despite the growing availability of these interventions,
their adoption in real-world applications remains limited, with many
practitioners unaware of their existence. To address this gap, we
systematically identified and compiled a dataset of 62 open source fairness
interventions and identified active ones. We conducted an in-depth analysis of
their specifications and features to uncover considerations that may drive
practitioner preference and to identify the software interventions actively
maintained in the open source ecosystem. Our findings indicate that 32% of
these interventions have been actively maintained within the past year, and 50%
of them offer both bias detection and mitigation capabilities, mostly during
inprocessing.

</details>


### [9] [5C Prompt Contracts: A Minimalist, Creative-Friendly, Token-Efficient Design Framework for Individual and SME LLM Usage](https://arxiv.org/abs/2507.07045)
*Ugur Ari*

Main category: cs.SE

TL;DR: 论文提出了一种名为5C Prompt Contract的框架，将提示设计简化为五个直观组件，以提高LLM交互的效率和灵活性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）在关键任务中的应用增加，需要一种既系统又简洁的提示设计框架，以降低认知和令牌开销。

Method: 提出了5C Prompt Contract框架，包括Character、Cause、Constraint、Contingency和Calibration五个组件，旨在优化提示设计。

Result: 实验表明，5C框架在多种LLM架构中均能高效利用输入令牌，同时保持输出的一致性和丰富性。

Conclusion: 5C框架特别适合资源有限的中小企业和个人，提供了一种可靠且灵活的LLM交互方法。

Abstract: The progression from traditional prompt engineering to a more rigorous
discipline of prompt design marks a pivotal shift in human-LLM interaction. As
Large Language Models (LLMs) become increasingly embedded in mission-critical
applications, there emerges a pressing need for frameworks that are not only
explicit and systematic but also minimal enough to remain practical and broadly
accessible. While many existing approaches address prompt structuring through
elaborate Domain-Specific Languages (DSLs) or multi-layered templates, such
methods can impose significant token and cognitive overhead, potentially
constraining the model's creative capacity. In this context, we propose the 5C
Prompt Contract, a framework that distills prompt design into five intuitive
components: Character, Cause, Constraint, Contingency, and Calibration. This
minimal cognitive schema explicitly integrates fallback and output optimization
directives, fostering reliable, interpretable, and creatively flexible AI
interactions. Experimental results demonstrate that the 5C framework
consistently achieves superior input token efficiency while maintaining rich
and consistent outputs across diverse LLM architectures (OpenAI, Anthropic,
DeepSeek, and Gemini), making it particularly suited for individuals and
Small-to-Medium Enterprises (SMEs) with limited AI engineering resources.

</details>
