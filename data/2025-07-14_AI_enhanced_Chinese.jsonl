{"id": "2507.08061", "categories": ["cs.SE", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2507.08061", "abs": "https://arxiv.org/abs/2507.08061", "authors": ["Andrea Morales Coto", "Aditi Verma"], "title": "The State of Computational Science in Fission and Fusion Energy", "comment": null, "summary": "The tools used to engineer something are just as important as the thing that\nis actually being engineered. In fact, in many cases, the tools can indeed\ndetermine what is engineerable. In fusion and fission1 energy engineering,\nsoftware has become the dominant tool for design. For that reason, in 2024, for\nthe first time ever, we asked 103 computational scientists developing the codes\nused in fusion and fission energy about the problems they are attempting to\nsolve with their codes, the tools available to them to solve them, and their\nend to end developer experience with said tools.\n  The results revealed a changing tide in software tools in fusion and fission,\nwith more and more computational scientists preferring modern programming\nlanguages, open-source codes, and modular software. These trends represent a\npeek into what will happen 5 to 10 years in the future of nuclear engineering.\nSince the majority of our respondents belonged to US national labs and\nuniversities, these results hint at the most cutting-edge trends in the\nindustry. The insights included in the State of Computational Science in\nFission and Fusion Energy indicate a dramatic shift toward multiphysics codes,\na drop-off in the use of FORTRAN in favor of more modern languages like Python\nand C++, and ever-rising budgets for code development, at times reaching $50M\nin a single organization.\n  Our survey paints a future of nuclear engineering codes that is modular in\nnature, small in terms of compute, and increasingly prioritized by\norganizations. Access to our results in web form are available online.", "AI": {"tldr": "\u8c03\u67e5\u663e\u793a\uff0c\u6838\u5de5\u7a0b\u9886\u57df\u7684\u8ba1\u7b97\u79d1\u5b66\u5bb6\u66f4\u503e\u5411\u4e8e\u4f7f\u7528\u73b0\u4ee3\u7f16\u7a0b\u8bed\u8a00\u3001\u5f00\u6e90\u4ee3\u7801\u548c\u6a21\u5757\u5316\u8f6f\u4ef6\uff0c\u9884\u793a\u7740\u672a\u67655\u523010\u5e74\u7684\u884c\u4e1a\u8d8b\u52bf\u3002", "motivation": "\u7814\u7a76\u6838\u5de5\u7a0b\u4e2d\u8f6f\u4ef6\u5de5\u5177\u7684\u91cd\u8981\u6027\u53ca\u5176\u5bf9\u5de5\u7a0b\u8bbe\u8ba1\u7684\u5f71\u54cd\u3002", "method": "\u5bf9103\u4f4d\u4ece\u4e8b\u6838\u805a\u53d8\u548c\u6838\u88c2\u53d8\u80fd\u6e90\u4ee3\u7801\u5f00\u53d1\u7684\u79d1\u5b66\u5bb6\u8fdb\u884c\u8c03\u67e5\uff0c\u4e86\u89e3\u5176\u5de5\u5177\u4f7f\u7528\u548c\u5f00\u53d1\u4f53\u9a8c\u3002", "result": "\u8d8b\u52bf\u5305\u62ec\u8f6c\u5411\u73b0\u4ee3\u8bed\u8a00\uff08\u5982Python\u3001C++\uff09\u3001\u6a21\u5757\u5316\u8f6f\u4ef6\u548c\u591a\u7269\u7406\u573a\u4ee3\u7801\uff0c\u5f00\u53d1\u9884\u7b97\u663e\u8457\u589e\u52a0\u3002", "conclusion": "\u6838\u5de5\u7a0b\u4ee3\u7801\u672a\u6765\u5c06\u66f4\u6a21\u5757\u5316\u3001\u8ba1\u7b97\u91cf\u66f4\u5c0f\uff0c\u5e76\u53d7\u5230\u66f4\u591a\u7ec4\u7ec7\u91cd\u89c6\u3002"}}
{"id": "2507.08149", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.08149", "abs": "https://arxiv.org/abs/2507.08149", "authors": ["Valerie Chen", "Ameet Talwalkar", "Robert Brennan", "Graham Neubig"], "title": "Code with Me or for Me? How Increasing AI Automation Transforms Developer Workflows", "comment": null, "summary": "Developers now have access to a growing array of increasingly autonomous AI\ntools to support software development. While numerous studies have examined\ndeveloper use of copilots, which can provide chat assistance or code\ncompletions, evaluations of coding agents, which can automatically write files\nand run code, still largely rely on static benchmarks without\nhumans-in-the-loop. In this work, we conduct the first academic study to\nexplore developer interactions with coding agents and characterize how more\nautonomous AI tools affect user productivity and experience, compared to\nexisting copilots. We evaluate two leading copilot and agentic coding\nassistants, GitHub Copilot and OpenHands, recruiting participants who regularly\nuse the former. Our results show agents have the potential to assist developers\nin ways that surpass copilots (e.g., completing tasks that humans might not\nhave accomplished before) and reduce the user effort required to complete\ntasks. However, there are challenges involved in enabling their broader\nadoption, including how to ensure users have an adequate understanding of agent\nbehaviors. Our results not only provide insights into how developer workflows\nchange as a result of coding agents but also highlight how user interactions\nwith agents differ from those with existing copilots, motivating a set of\nrecommendations for researchers building new agents. Given the broad set of\ndevelopers who still largely rely on copilot-like systems, our work highlights\nkey challenges of adopting more agentic systems into developer workflows.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5f00\u53d1\u8005\u4e0e\u81ea\u52a8\u5316\u7f16\u7801\u4ee3\u7406\u7684\u4e92\u52a8\uff0c\u53d1\u73b0\u5176\u6f5c\u529b\u8d85\u8d8a\u4f20\u7edfcopilot\uff0c\u4f46\u4e5f\u9762\u4e34\u7528\u6237\u7406\u89e3\u4e0d\u8db3\u7b49\u6311\u6218\u3002", "motivation": "\u8bc4\u4f30\u66f4\u81ea\u4e3b\u7684AI\u5de5\u5177\u5bf9\u5f00\u53d1\u8005\u751f\u4ea7\u529b\u548c\u4f53\u9a8c\u7684\u5f71\u54cd\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83GitHub Copilot\u548cOpenHands\uff0c\u62db\u52df\u5e38\u7528\u524d\u8005\u7684\u5f00\u53d1\u8005\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u4ee3\u7406\u80fd\u5b8c\u6210\u4f20\u7edfcopilot\u65e0\u6cd5\u5b8c\u6210\u7684\u4efb\u52a1\uff0c\u51cf\u5c11\u7528\u6237\u5de5\u4f5c\u91cf\uff0c\u4f46\u9700\u89e3\u51b3\u7528\u6237\u7406\u89e3\u95ee\u9898\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5f00\u53d1\u8005\u5de5\u4f5c\u6d41\u53d8\u5316\u63d0\u4f9b\u89c1\u89e3\uff0c\u5e76\u4e3a\u672a\u6765\u4ee3\u7406\u8bbe\u8ba1\u63d0\u51fa\u5efa\u8bae\u3002"}}
{"id": "2507.08160", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.08160", "abs": "https://arxiv.org/abs/2507.08160", "authors": ["Ot\u00e1vio Cury", "Guilherme Avelino"], "title": "The Impact of Generative AI on Code Expertise Models: An Exploratory Study", "comment": null, "summary": "Generative Artificial Intelligence (GenAI) tools for source code generation\nhave significantly boosted productivity in software development. However, they\nalso raise concerns, particularly the risk that developers may rely heavily on\nthese tools, reducing their understanding of the generated code. We hypothesize\nthat this loss of understanding may be reflected in source code knowledge\nmodels, which are used to identify developer expertise. In this work, we\npresent an exploratory analysis of how a knowledge model and a Truck Factor\nalgorithm built upon it can be affected by GenAI usage. To investigate this, we\ncollected statistical data on the integration of ChatGPT-generated code into\nGitHub projects and simulated various scenarios by adjusting the degree of\nGenAI contribution. Our findings reveal that most scenarios led to measurable\nimpacts, indicating the sensitivity of current expertise metrics. This suggests\nthat as GenAI becomes more integrated into development workflows, the\nreliability of such metrics may decrease.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u751f\u6210\u5f0fAI\u5de5\u5177\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u5bf9\u5f00\u53d1\u8005\u7406\u89e3\u529b\u7684\u6f5c\u5728\u5f71\u54cd\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u5b9e\u9a8c\u5206\u6790\u4e86\u5176\u5bf9\u4ee3\u7801\u77e5\u8bc6\u6a21\u578b\u548cTruck Factor\u7b97\u6cd5\u7684\u654f\u611f\u6027\u3002", "motivation": "\u751f\u6210\u5f0fAI\u5de5\u5177\u867d\u63d0\u5347\u5f00\u53d1\u6548\u7387\uff0c\u4f46\u53ef\u80fd\u5bfc\u81f4\u5f00\u53d1\u8005\u5bf9\u751f\u6210\u4ee3\u7801\u7684\u7406\u89e3\u4e0d\u8db3\uff0c\u8fdb\u800c\u5f71\u54cd\u4ee3\u7801\u77e5\u8bc6\u6a21\u578b\u7684\u51c6\u786e\u6027\u3002", "method": "\u901a\u8fc7\u6536\u96c6ChatGPT\u751f\u6210\u4ee3\u7801\u5728GitHub\u9879\u76ee\u4e2d\u7684\u7edf\u8ba1\u6570\u636e\uff0c\u5e76\u6a21\u62df\u4e0d\u540cGenAI\u8d21\u732e\u7a0b\u5ea6\u7684\u60c5\u666f\u3002", "result": "\u591a\u6570\u60c5\u666f\u4e0b\uff0cGenAI\u7684\u4f7f\u7528\u5bf9\u5f53\u524d\u4e13\u4e1a\u77e5\u8bc6\u6307\u6807\u4ea7\u751f\u4e86\u53ef\u6d4b\u91cf\u7684\u5f71\u54cd\u3002", "conclusion": "\u968f\u7740GenAI\u5728\u5f00\u53d1\u6d41\u7a0b\u4e2d\u7684\u6df1\u5165\u5e94\u7528\uff0c\u73b0\u6709\u4e13\u4e1a\u77e5\u8bc6\u6307\u6807\u7684\u53ef\u9760\u6027\u53ef\u80fd\u4e0b\u964d\u3002"}}
{"id": "2507.08250", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.08250", "abs": "https://arxiv.org/abs/2507.08250", "authors": ["Yasaman Abedini", "Abbas Heydarnoori"], "title": "Leveraging Large Language Models for Classifying App Users' Feedback", "comment": null, "summary": "In recent years, significant research has been conducted into classifying\napplication (app) user feedback, primarily relying on supervised machine\nlearning algorithms. However, fine-tuning more generalizable classifiers based\non existing labeled datasets remains an important challenge, as creating large\nand accurately labeled datasets often requires considerable time and resources.\nIn this paper, we evaluate the capabilities of four advanced LLMs, including\nGPT-3.5-Turbo, GPT-4, Flan-T5, and Llama3-70b, to enhance user feedback\nclassification and address the challenge of the limited labeled dataset. To\nachieve this, we conduct several experiments on eight datasets that have been\nmeticulously labeled in prior research. These datasets include user reviews\nfrom app stores, posts from the X platform, and discussions from the public\nforums, widely recognized as representative sources of app user feedback. We\nanalyze the performance of various LLMs in identifying both fine-grained and\ncoarse-grained user feedback categories. Given the substantial volume of daily\nuser feedback and the computational limitations of LLMs, we leverage these\nmodels as an annotation tool to augment labeled datasets with general and\napp-specific data. This augmentation aims to enhance the performance of\nstate-of-the-art BERT-based classification models. Our findings indicate that\nLLMs when guided by well-crafted prompts, can effectively classify user\nfeedback into coarse-grained categories. Moreover, augmenting the training\ndataset with datasets labeled using the consensus of LLMs can significantly\nenhance classifier performance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5229\u7528\u56db\u79cd\u5148\u8fdbLLM\uff08\u5982GPT-3.5-Turbo\u3001GPT-4\u7b49\uff09\u63d0\u5347\u7528\u6237\u53cd\u9988\u5206\u7c7b\u80fd\u529b\uff0c\u89e3\u51b3\u6807\u6ce8\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u4f46\u6807\u6ce8\u6210\u672c\u9ad8\uff0c\u56e0\u6b64\u63a2\u7d22LLM\u5728\u7528\u6237\u53cd\u9988\u5206\u7c7b\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u5728\u516b\u4e2a\u6807\u6ce8\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u56db\u79cdLLM\u7684\u5206\u7c7b\u6027\u80fd\uff0c\u5e76\u5229\u7528LLM\u751f\u6210\u6807\u6ce8\u6570\u636e\u4ee5\u589e\u5f3aBERT\u6a21\u578b\u7684\u8bad\u7ec3\u3002", "result": "LLM\u5728\u7c97\u7c92\u5ea6\u5206\u7c7b\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4e14\u901a\u8fc7LLM\u6807\u6ce8\u6570\u636e\u589e\u5f3a\u8bad\u7ec3\u96c6\u80fd\u663e\u8457\u63d0\u5347\u5206\u7c7b\u5668\u6027\u80fd\u3002", "conclusion": "LLM\u53ef\u4f5c\u4e3a\u6709\u6548\u7684\u6807\u6ce8\u5de5\u5177\uff0c\u63d0\u5347\u7528\u6237\u53cd\u9988\u5206\u7c7b\u6027\u80fd\uff0c\u5c24\u5176\u5728\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2507.08467", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.08467", "abs": "https://arxiv.org/abs/2507.08467", "authors": ["Youshuai Tan", "Zhanwei Zhang", "Jinfu Chen", "Zishuo Ding", "Jifeng Xuan", "Weiyi Shang"], "title": "Computing Floating-Point Errors by Injecting Perturbations", "comment": "arXiv admin note: text overlap with arXiv:2412.20804", "summary": "Floating-point programs form the foundation of modern science and\nengineering, providing the essential computational framework for a wide range\nof applications, such as safety-critical systems, aerospace engineering, and\nfinancial analysis. Floating-point errors can lead to severe consequences.\nAlthough floating-point errors widely exist, only a subset of inputs may\ntrigger significant errors in floating-point programs. Therefore, it is crucial\nto determine whether a given input could produce such errors. Researchers tend\nto take the results of high-precision floating-point programs as oracles for\ndetecting floating-point errors, which introduces two main limitations: (1)\ndifficulty of implementation and (2) prolonged execution time. The two recent\ntools, ATOMU and FPCC, can partially address these issues. However, ATOMU\nsuffers from false positives; while FPCC, though eliminating false positives,\noperates at a considerably slower speed.\n  To address these two challenges, we propose a novel approach named\nPI-detector to computing floating-point errors effectively and efficiently. Our\napproach is based on the observation that floating-point errors stem from large\ncondition numbers in atomic operations (such as addition and subtraction),\nwhich then propagate and accumulate. PI-detector injects small perturbations\ninto the operands of individual atomic operations within the program and\ncompares the outcomes of the original program with the perturbed version to\ncompute floating-point errors. We evaluate PI-detector with datasets from ATOMU\nand HSED, as well as a complex linear system-solving program. Experimental\nresults demonstrate that PI-detector can perform efficient and accurate\nfloating-point error computation.", "AI": {"tldr": "PI-detector\u662f\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u51c6\u786e\u5730\u8ba1\u7b97\u6d6e\u70b9\u7a0b\u5e8f\u4e2d\u7684\u8bef\u5dee\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5de5\u5177ATOMU\u548cFPCC\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u6d6e\u70b9\u7a0b\u5e8f\u5e7f\u6cdb\u5e94\u7528\u4e8e\u5173\u952e\u9886\u57df\uff0c\u4f46\u6d6e\u70b9\u8bef\u5dee\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u540e\u679c\u3002\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5b9e\u73b0\u56f0\u96be\u548c\u6267\u884c\u65f6\u95f4\u957f\u7684\u95ee\u9898\uff0cATOMU\u6709\u8bef\u62a5\uff0cFPCC\u901f\u5ea6\u6162\u3002", "method": "PI-detector\u901a\u8fc7\u89c2\u5bdf\u6d6e\u70b9\u8bef\u5dee\u6e90\u4e8e\u539f\u5b50\u64cd\u4f5c\u7684\u5927\u6761\u4ef6\u6570\uff0c\u901a\u8fc7\u5411\u64cd\u4f5c\u6570\u6ce8\u5165\u5c0f\u6270\u52a8\u5e76\u6bd4\u8f83\u7ed3\u679c\u6765\u8ba1\u7b97\u8bef\u5dee\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPI-detector\u80fd\u9ad8\u6548\u51c6\u786e\u5730\u8ba1\u7b97\u6d6e\u70b9\u8bef\u5dee\u3002", "conclusion": "PI-detector\u89e3\u51b3\u4e86\u73b0\u6709\u5de5\u5177\u7684\u4e0d\u8db3\uff0c\u4e3a\u6d6e\u70b9\u8bef\u5dee\u68c0\u6d4b\u63d0\u4f9b\u4e86\u66f4\u4f18\u65b9\u6848\u3002"}}
{"id": "2507.08523", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.08523", "abs": "https://arxiv.org/abs/2507.08523", "authors": ["Yilun Wang", "Pengfei Chen", "Haiyu Huang", "Zilong He", "Gou Tan", "Chuanfu Zhang", "Jingkai He", "Zibin Zheng"], "title": "InferLog: Accelerating LLM Inference for Online Log Parsing via ICL-oriented Prefix Caching", "comment": null, "summary": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy.", "AI": {"tldr": "InferLog\u662f\u4e00\u79cd\u9488\u5bf9\u5728\u7ebf\u65e5\u5fd7\u89e3\u6790\u7684LLM\u63a8\u7406\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u6539\u8fdb\u524d\u7f00\u7f13\u5b58\u6548\u7387\u548c\u52a8\u6001\u914d\u7f6e\u8c03\u4f18\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u800c\u4e0d\u5f71\u54cd\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u4ee3\u8f6f\u4ef6\u7cfb\u7edf\u751f\u6210\u5927\u91cf\u8fd0\u884c\u65f6\u65e5\u5fd7\uff0c\u9700\u8981\u9ad8\u6548\u51c6\u786e\u7684\u65e5\u5fd7\u89e3\u6790\u3002\u73b0\u6709LLM\u89e3\u6790\u5668\u56e0\u9690\u79c1\u548c\u6027\u80fd\u95ee\u9898\u96be\u4ee5\u90e8\u7f72\uff0cInferLog\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u74f6\u9888\u3002", "method": "\u8bbe\u8ba1\u524d\u7f00\u611f\u77e5ICL\u4f18\u5316\u7b56\u7565\u548c\u57fa\u4e8e\u5143\u5b66\u4e60\u7684\u52a8\u6001\u914d\u7f6e\u8c03\u4f18\u7ba1\u9053\uff0c\u63d0\u5347LLM\u63a8\u7406\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u663e\u793aInferLog\u5728Loghub\u6570\u636e\u96c6\u548cvLLM\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u52a0\u901f\u4e86\u65e5\u5fd7\u89e3\u6790\u3002", "conclusion": "InferLog\u901a\u8fc7\u4f18\u5316\u63a8\u7406\u6548\u7387\uff0c\u89e3\u51b3\u4e86LLM\u5728\u65e5\u5fd7\u89e3\u6790\u4e2d\u7684\u6027\u80fd\u74f6\u9888\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2507.08594", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.08594", "abs": "https://arxiv.org/abs/2507.08594", "authors": ["Fernando Ayach", "Vitor Lameir\u00e3o", "Raul Le\u00e3o", "Jerfferson Felizardo", "Rafael Sobrinho", "Vanessa Borges", "Patr\u00edcia Matsubara", "Awdren Font\u00e3o"], "title": "Generating Proto-Personas through Prompt Engineering: A Case Study on Efficiency, Effectiveness and Empathy", "comment": "12 pages; 2 figures; Preprint with the original submission accepted\n  for publication at 39th Brazilian Symposium on Software Engineering (SBES)", "summary": "Proto-personas are commonly used during early-stage Product Discovery, such\nas Lean Inception, to guide product definition and stakeholder alignment.\nHowever, the manual creation of proto-personas is often time-consuming,\ncognitively demanding, and prone to bias. In this paper, we propose and\nempirically investigate a prompt engineering-based approach to generate\nproto-personas with the support of Generative AI (GenAI). Our goal is to\nevaluate the approach in terms of efficiency, effectiveness, user acceptance,\nand the empathy elicited by the generated personas. We conducted a case study\nwith 19 participants embedded in a real Lean Inception, employing a qualitative\nand quantitative methods design. The results reveal the approach's efficiency\nby reducing time and effort and improving the quality and reusability of\npersonas in later discovery phases, such as Minimum Viable Product (MVP)\nscoping and feature refinement. While acceptance was generally high, especially\nregarding perceived usefulness and ease of use, participants noted limitations\nrelated to generalization and domain specificity. Furthermore, although\ncognitive empathy was strongly supported, affective and behavioral empathy\nvaried significantly across participants. These results contribute novel\nempirical evidence on how GenAI can be effectively integrated into software\nProduct Discovery practices, while also identifying key challenges to be\naddressed in future iterations of such hybrid design processes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63d0\u793a\u5de5\u7a0b\u7684\u751f\u6210\u5f0fAI\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u539f\u578b\u4eba\u7269\u89d2\u8272\uff0c\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u8d28\u91cf\uff0c\u4f46\u5b58\u5728\u6cdb\u5316\u548c\u9886\u57df\u7279\u5f02\u6027\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u624b\u52a8\u521b\u5efa\u539f\u578b\u4eba\u7269\u89d2\u8272\u7684\u8017\u65f6\u3001\u8ba4\u77e5\u8d1f\u62c5\u548c\u504f\u89c1\u95ee\u9898\u3002", "method": "\u91c7\u7528\u63d0\u793a\u5de5\u7a0b\u548c\u751f\u6210\u5f0fAI\u751f\u6210\u539f\u578b\u4eba\u7269\u89d2\u8272\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u8bc4\u4f30\u5176\u6548\u7387\u3001\u6548\u679c\u548c\u7528\u6237\u63a5\u53d7\u5ea6\u3002", "result": "\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u65f6\u95f4\u548c\u7cbe\u529b\uff0c\u63d0\u9ad8\u4e86\u4eba\u7269\u89d2\u8272\u7684\u8d28\u91cf\u548c\u53ef\u91cd\u7528\u6027\uff0c\u4f46\u6cdb\u5316\u548c\u9886\u57df\u7279\u5f02\u6027\u4ecd\u6709\u5c40\u9650\u3002", "conclusion": "\u751f\u6210\u5f0fAI\u53ef\u6709\u6548\u652f\u6301\u4ea7\u54c1\u53d1\u73b0\u5b9e\u8df5\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u89e3\u51b3\u6cdb\u5316\u548c\u60c5\u611f\u5171\u9e23\u95ee\u9898\u3002"}}
{"id": "2507.08627", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.08627", "abs": "https://arxiv.org/abs/2507.08627", "authors": ["Chi-en Amy Tai", "Pengyu Nie", "Lukasz Golab", "Alexander Wong"], "title": "NL in the Middle: Code Translation with LLMs and Intermediate Representations", "comment": null, "summary": "Studies show that large language models (LLMs) produce buggy code\ntranslations. One avenue to improve translation accuracy is through\nintermediate representations, which could provide structured insights to guide\nthe model's understanding. We explore whether code translation using LLMs can\nbenefit from intermediate representations via natural language (NL) and\nabstract syntax trees (ASTs). Since prompt engineering greatly affects LLM\nperformance, we consider several ways to integrate these representations, from\none-shot to chain-of-thought (CoT) prompting. Using Open Gpt4 8X7B and\nspecialized StarCoder and CodeGen models on popular code translation benchmarks\n(CodeNet and AVATAR), we find that CoT with an intermediate NL summary performs\nbest, with an increase of 13.8% and 6.7%, respectively, in successful\ntranslations for the best-performing model (Open Gpt4 8X7B) compared to the\nzero-shot prompt.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4ee3\u7801\u7ffb\u8bd1\u4e2d\u5b58\u5728\u9519\u8bef\u3002\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\uff08NL\uff09\u548c\u62bd\u8c61\u8bed\u6cd5\u6811\uff08ASTs\uff09\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\uff0c\u7ed3\u5408\u94fe\u5f0f\u601d\u8003\uff08CoT\uff09\u63d0\u793a\uff0c\u80fd\u663e\u8457\u63d0\u5347\u7ffb\u8bd1\u51c6\u786e\u6027\u3002", "motivation": "\u6539\u8fdbLLMs\u5728\u4ee3\u7801\u7ffb\u8bd1\u4e2d\u7684\u51c6\u786e\u6027\uff0c\u63a2\u7d22\u4e2d\u95f4\u8868\u793a\uff08\u5982NL\u548cASTs\uff09\u7684\u4f5c\u7528\u3002", "method": "\u4f7f\u7528Open Gpt4 8X7B\u3001StarCoder\u548cCodeGen\u6a21\u578b\uff0c\u5728CodeNet\u548cAVATAR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6bd4\u8f83\u4e0d\u540c\u63d0\u793a\u65b9\u6cd5\uff08\u4ece\u96f6\u6837\u672c\u5230CoT\uff09\u7684\u6548\u679c\u3002", "result": "CoT\u7ed3\u5408NL\u4e2d\u95f4\u8868\u793a\u6548\u679c\u6700\u4f73\uff0cOpen Gpt4 8X7B\u7684\u6210\u529f\u7ffb\u8bd1\u7387\u5206\u522b\u63d0\u9ad8\u4e8613.8%\u548c6.7%\u3002", "conclusion": "\u4e2d\u95f4\u8868\u793a\uff08\u5c24\u5176\u662fNL\uff09\u7ed3\u5408CoT\u63d0\u793a\u80fd\u6709\u6548\u63d0\u5347LLMs\u7684\u4ee3\u7801\u7ffb\u8bd1\u6027\u80fd\u3002"}}
{"id": "2507.08671", "categories": ["cs.SE", "D.2.3; D.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2507.08671", "abs": "https://arxiv.org/abs/2507.08671", "authors": ["Hua Ge", "Juan Zhai", "Minxue Pan", "Fusen He", "Ziyue Tan"], "title": "LLMCup: Ranking-Enhanced Comment Updating with LLMs", "comment": "13 pages, 10 figures", "summary": "While comments are essential for enhancing code readability and\nmaintainability in modern software projects, developers are often motivated to\nupdate code but not comments, leading to outdated or inconsistent documentation\nthat hinders future understanding and maintenance. Recent approaches such as\nCUP and HebCup have attempted automatic comment updating using neural\nsequence-to-sequence models and heuristic rules, respectively. However, these\nmethods can miss or misinterpret crucial information during comment updating,\nresulting in inaccurate comments, and they often struggle with complex update\nscenarios. Given these challenges, a promising direction lies in leveraging\nlarge language models (LLMs), which have shown impressive performance in\nsoftware engineering tasks such as comment generation, code synthesis, and\nprogram repair. This suggests their strong potential to capture the logic\nbehind code modifications - an ability that is crucial for the task of comment\nupdating. Nevertheless, selecting an appropriate prompt strategy for an LLM on\neach update case remains challenging. To address this, we propose a novel\ncomment updating framework, LLMCup, which first uses multiple prompt strategies\nto provide diverse candidate updated comments via an LLM, and then employs a\nranking model, CupRank, to select the best candidate as final updated comment.\nExperimental results demonstrate the effectiveness of LLMCup, with improvements\nover state-of-the-art baselines (CUP and HebCup) by 49.0%-116.9% in Accuracy,\n10.8%-20% in BLEU-4, 4.6% in METEOR, 0.9%-1.9% in F1, and 2.1%-3.4% in\nSentenceBert similarity. Furthermore, a user study shows that comments updated\nby LLMCup sometimes surpass human-written updates, highlighting the importance\nof incorporating human evaluation in comment quality assessment.", "AI": {"tldr": "LLMCup\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6ce8\u91ca\u66f4\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u63d0\u793a\u7b56\u7565\u751f\u6210\u5019\u9009\u6ce8\u91ca\uff0c\u5e76\u4f7f\u7528\u6392\u540d\u6a21\u578b\u9009\u62e9\u6700\u4f73\u6ce8\u91ca\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f00\u53d1\u8005\u503e\u5411\u4e8e\u66f4\u65b0\u4ee3\u7801\u800c\u975e\u6ce8\u91ca\uff0c\u5bfc\u81f4\u6ce8\u91ca\u8fc7\u65f6\u6216\u4e0d\u4e00\u81f4\uff0c\u5f71\u54cd\u4ee3\u7801\u7406\u89e3\u548c\u7ef4\u62a4\u3002\u73b0\u6709\u65b9\u6cd5\uff08\u5982CUP\u548cHebCup\uff09\u5728\u590d\u6742\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faLLMCup\u6846\u67b6\uff0c\u5229\u7528LLM\u751f\u6210\u591a\u6837\u5019\u9009\u6ce8\u91ca\uff0c\u5e76\u901a\u8fc7\u6392\u540d\u6a21\u578bCupRank\u9009\u62e9\u6700\u4f73\u6ce8\u91ca\u3002", "result": "\u5b9e\u9a8c\u663e\u793aLLMCup\u5728\u51c6\u786e\u6027\u3001BLEU-4\u3001METEOR\u3001F1\u548cSentenceBert\u76f8\u4f3c\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u90e8\u5206\u60c5\u51b5\u4e0b\u751a\u81f3\u4f18\u4e8e\u4eba\u5de5\u66f4\u65b0\u3002", "conclusion": "LLMCup\u5c55\u793a\u4e86LLM\u5728\u6ce8\u91ca\u66f4\u65b0\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u5f3a\u8c03\u4e86\u4eba\u5de5\u8bc4\u4f30\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.08730", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08730", "abs": "https://arxiv.org/abs/2507.08730", "authors": ["Zezhen Xiang", "Jingzhi Gong", "Tao Chen"], "title": "Dually Hierarchical Drift Adaptation for Online Configuration Performance Learning", "comment": "Accepted by ICSE 2026", "summary": "Modern configurable software systems need to learn models that correlate\nconfiguration and performance. However, when the system operates in dynamic\nenvironments, the workload variations, hardware changes, and system updates\nwill inevitably introduce concept drifts at different levels - global drifts,\nwhich reshape the performance landscape of the entire configuration space; and\nlocal drifts, which only affect certain sub-regions of that space. As such,\nexisting offline and transfer learning approaches can struggle to adapt to\nthese implicit and unpredictable changes in real-time, rendering configuration\nperformance learning challenging. To address this, we propose DHDA, an online\nconfiguration performance learning framework designed to capture and adapt to\nthese drifts at different levels. The key idea is that DHDA adapts to both the\nlocal and global drifts using dually hierarchical adaptation: at the upper\nlevel, we redivide the data into different divisions, within each of which the\nlocal model is retrained, to handle global drifts only when necessary. At the\nlower level, the local models of the divisions can detect local drifts and\nadapt themselves asynchronously. To balance responsiveness and efficiency, DHDA\ncombines incremental updates with periodic full retraining to minimize\nredundant computation when no drifts are detected. Through evaluating eight\nsoftware systems and against state-of-the-art approaches, we show that DHDA\nachieves considerably better accuracy and can effectively adapt to drifts with\nup to 2x improvements, while incurring reasonable overhead and is able to\nimprove different local models in handling concept drift.", "AI": {"tldr": "DHDA\u662f\u4e00\u4e2a\u5728\u7ebf\u914d\u7f6e\u6027\u80fd\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u91cd\u5c42\u6b21\u9002\u5e94\u673a\u5236\u5904\u7406\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5168\u5c40\u548c\u5c40\u90e8\u6982\u5ff5\u6f02\u79fb\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u52a8\u6001\u73af\u5883\u4e2d\u7684\u914d\u7f6e\u6027\u80fd\u5b66\u4e60\u56e0\u5168\u5c40\u548c\u5c40\u90e8\u6982\u5ff5\u6f02\u79fb\u800c\u53d8\u5f97\u590d\u6742\uff0c\u73b0\u6709\u79bb\u7ebf\u5b66\u4e60\u548c\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u5b9e\u65f6\u9002\u5e94\u8fd9\u4e9b\u53d8\u5316\u3002", "method": "DHDA\u91c7\u7528\u53cc\u91cd\u5c42\u6b21\u9002\u5e94\u673a\u5236\uff1a\u4e0a\u5c42\u91cd\u65b0\u5212\u5206\u6570\u636e\u4ee5\u5904\u7406\u5168\u5c40\u6f02\u79fb\uff0c\u4e0b\u5c42\u5c40\u90e8\u6a21\u578b\u5f02\u6b65\u68c0\u6d4b\u548c\u9002\u5e94\u5c40\u90e8\u6f02\u79fb\uff0c\u7ed3\u5408\u589e\u91cf\u66f4\u65b0\u548c\u5b9a\u671f\u5168\u91cd\u8bad\u7ec3\u4ee5\u5e73\u8861\u6548\u7387\u3002", "result": "\u5728\u516b\u4e2a\u8f6f\u4ef6\u7cfb\u7edf\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cDHDA\u51c6\u786e\u6027\u663e\u8457\u63d0\u5347\uff0c\u9002\u5e94\u6f02\u79fb\u80fd\u529b\u63d0\u9ad82\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u5408\u7406\u5f00\u9500\u3002", "conclusion": "DHDA\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u6001\u73af\u5883\u4e2d\u914d\u7f6e\u6027\u80fd\u5b66\u4e60\u7684\u6311\u6218\uff0c\u4e3a\u5b9e\u65f6\u9002\u5e94\u6982\u5ff5\u6f02\u79fb\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
