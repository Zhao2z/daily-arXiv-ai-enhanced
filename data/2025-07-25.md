<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 18]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [How Software Engineers Engage with AI: A Pragmatic Process Model and Decision Framework Grounded in Industry Observations](https://arxiv.org/abs/2507.17930)
*Vahid Garousi,Zafar Jafarov*

Main category: cs.SE

TL;DR: 论文探讨了AI如何辅助软件工程，提出了一个过程模型和决策框架，帮助开发者权衡AI生成输出的质量与节省的努力。


<details>
  <summary>Details</summary>
Motivation: 研究AI工具（如GitHub Copilot和ChatGPT）在软件工程中的实际应用，尤其是开发者如何决定信任、优化或拒绝AI生成的内容。

Method: 基于土耳其和阿塞拜疆三个行业环境的实践者报告和直接观察，提出了一个过程模型和2D决策框架。

Result: 模型和框架为开发者提供了结构化、轻量级的指导，支持更有效和深思熟虑的AI工具使用。

Conclusion: 研究为实际人机协作提供了实用指导，促进了AI在软件工程中的更有效应用。

Abstract: Artificial Intelligence (AI) has the potential to transform Software
Engineering (SE) by enhancing productivity, efficiency, and decision support.
Tools like GitHub Copilot and ChatGPT have given rise to "vibe coding"-an
exploratory, prompt-driven development style. Yet, how software engineers
engage with these tools in daily tasks, especially in deciding whether to
trust, refine, or reject AI-generated outputs, remains underexplored. This
paper presents two complementary contributions. First, a pragmatic process
model capturing real-world AI-assisted SE activities, including prompt design,
inspection, fallback, and refinement. Second, a 2D decision framework that
could help developers reason about trade-offs between effort saved and output
quality. Grounded in practitioner reports and direct observations in three
industry settings across Turkiye and Azerbaijan, our work illustrates how
engineers navigate AI use with human oversight. These models offer structured,
lightweight guidance to support more deliberate and effective use of AI tools
in SE, contributing to ongoing discussions on practical human-AI collaboration.

</details>


### [2] [Use as Directed? A Comparison of Software Tools Intended to Check Rigor and Transparency of Published Work](https://arxiv.org/abs/2507.17991)
*Peter Eckmann,Adrian Barnett,Alexandra Bannach-Brown,Elisa Pilar Bascunan Atria,Guillaume Cabanac,Louise Delwen Owen Franzen,Małgorzata Anna Gazda,Kaitlyn Hair,James Howison,Halil Kilicoglu,Cyril Labbe,Sarah McCann,Vladislav Nachev,Martijn Roelandse,Maia Salholz-Hillel,Robert Schulz,Gerben ter Riet,Colby Vorland,Anita Bandrowski,Tracey Weissgerber*

Main category: cs.SE

TL;DR: 论文比较了11种自动化工具在9种严谨性标准上的表现，发现某些工具在特定标准（如开放数据检测）上表现突出，而工具组合在其他标准（如纳入排除标准检测）上更优。


<details>
  <summary>Details</summary>
Motivation: 解决科学报告中标准化和透明度不足导致的再现性危机。

Method: 对11种自动化工具在9种严谨性标准上进行广泛比较。

Result: 某些工具在特定标准上表现优异，工具组合在某些标准上优于单一工具。

Conclusion: 提出了对严谨性和透明度检测工具开发的见解和建议，并公开了代码和数据。

Abstract: The causes of the reproducibility crisis include lack of standardization and
transparency in scientific reporting. Checklists such as ARRIVE and CONSORT
seek to improve transparency, but they are not always followed by authors and
peer review often fails to identify missing items. To address these issues,
there are several automated tools that have been designed to check different
rigor criteria. We have conducted a broad comparison of 11 automated tools
across 9 different rigor criteria from the ScreenIT group. We found some
criteria, including detecting open data, where the combination of tools showed
a clear winner, a tool which performed much better than other tools. In other
cases, including detection of inclusion and exclusion criteria, the combination
of tools exceeded the performance of any one tool. We also identified key areas
where tool developers should focus their effort to make their tool maximally
useful. We conclude with a set of insights and recommendations for stakeholders
in the development of rigor and transparency detection tools. The code and data
for the study is available at https://github.com/PeterEckmann1/tool-comparison.

</details>


### [3] [An Empirical Study of GenAI Adoption in Open-Source Game Development: Tools, Tasks, and Developer Challenges](https://arxiv.org/abs/2507.18029)
*Xiang Echo Chen,Wenhan Zhu,Guoshuai Albert Shi,Michael W. Godfrey*

Main category: cs.SE

TL;DR: 研究探讨了生成式AI（GenAI）在开源游戏开发中的应用，通过分析GitHub上的问题讨论，比较了GenAI与传统AI（TradAI）及非AI话题的差异。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在游戏设计中的潜力日益显现，但缺乏实际开发中的实证研究，尤其是在开源社区中的应用情况。

Method: 构建开源游戏仓库的数据集，通过开放式卡片分类和主题分析对GitHub问题进行分类和标注，比较GenAI、TradAI和非AI话题。

Result: 揭示了GenAI在开源游戏开发中的使用模式、开发者关注点及集成实践上的独特之处。

Conclusion: GenAI正在改变开源游戏开发的工作流程和痛点，与传统AI和非AI方法有明显差异。

Abstract: The growing capabilities of generative AI (GenAI) have begun to reshape how
games are designed and developed, offering new tools for content creation,
gameplay simulation, and design ideation. While prior research has explored
traditional uses of AI in games, such as controlling agents or generating
procedural content. There is limited empirical understanding of how GenAI is
adopted by developers in real-world contexts, especially within the open-source
community. This study aims to explore how GenAI technologies are discussed,
adopted, and integrated into open-source game development by analyzing issue
discussions on GitHub. We investigate the tools, tasks, and challenges
associated with GenAI by comparing GenAI-related issues to those involving
traditional AI (TradAI) and NonAI topics. Our goal is to uncover how GenAI
differs from other approaches in terms of usage patterns, developer concerns,
and integration practices. To address this objective, we construct a dataset of
open-source game repositories that discuss AI-related topics. We apply open
card sorting and thematic analysis to a stratified sample of GitHub issues,
labelling each by type and content. These annotations enable comparative
analysis across GenAI, TradAI, and NonAI groups, and provide insight into how
GenAI is shaping the workflows and pain points of open-source game developers.

</details>


### [4] [Your ATs to Ts: MITRE ATT&CK Attack Technique to P-SSCRM Task Mapping](https://arxiv.org/abs/2507.18037)
*Sivana Hamer,Jacob Bowen,Md Nazmul Haque,Chris Madden,Laurie Williams*

Main category: cs.SE

TL;DR: 该论文提出了一个将MITRE ATT&CK攻击技术与P-SSCRM任务映射的框架，帮助组织识别软件供应链攻击的风险缓解措施。


<details>
  <summary>Details</summary>
Motivation: 软件供应链攻击日益复杂，需要系统化的方法来识别和缓解风险。

Method: 通过四种独立策略创建MITRE ATT&CK与P-SSCRM任务的映射，并将其与其他10个框架关联。

Result: 提供了MITRE ATT&CK与其他行业和政府框架之间的映射关系。

Conclusion: 该框架为组织提供了一种系统化的方法，用于管理软件供应链攻击风险。

Abstract: The MITRE Adversarial Tactics, Techniques and Common Knowledge (MITRE ATT&CK)
Attack Technique to Proactive Software Supply Chain Risk Management Framework
(P-SSCRM) Task mapping described in this document helps software organizations
to determine how different tasks mitigate the attack techniques of software
supply chain attacks. The mapping was created through four independent
strategies to find agreed-upon mappings. Because each P-SSCRM task is mapped to
one or more tasks from the 10 frameworks, the mapping we provide is also a
mapping between MITRE ATT&CK and other prominent government and industry
frameworks.

</details>


### [5] [Factors Impacting Faculty Adoption of Project-Based Learning in Computing Education: a Survey](https://arxiv.org/abs/2507.18039)
*Ahmad D. Suleiman,Yiming Tang,Daqing Hou*

Main category: cs.SE

TL;DR: 研究探讨了影响计算教育工作者在软件工程和计算课程中采用项目式学习（PjBL）的因素，揭示了挑战和促进因素。


<details>
  <summary>Details</summary>
Motivation: 尽管PjBL能提升学生能力，但其采用因机构支持不足、时间限制等问题而不一致，研究旨在探索这些障碍及解决方案。

Method: 采用混合方法，通过在线调查收集80名计算教师的定量和定性数据，进行统计和主题分析。

Result: 结果显示PjBL虽受重视，但采用受限于规划、项目设计和机构支持不足；同行协作、专业发展和机构激励是促进因素。

Conclusion: 研究强调需要系统性支持结构，以帮助教师实验和扩展PjBL实践。

Abstract: This research full paper investigates the factors influencing computing
educators' adoption of project-based learning (PjBL) in software engineering
and computing curricula. Recognized as a student-centered pedagogical approach,
PjBL has the potential to enhance student motivation, engagement, critical
thinking, collaboration, and problem-solving skills. Despite these benefits,
faculty adoption remains inconsistent due to challenges such as insufficient
institutional support, time constraints, limited training opportunities,
designing or sourcing projects, and aligning them with course objectives. This
research explores these barriers and investigates the strategies and resources
that facilitate a successful adoption. Using a mixed-methods approach, data
from 80 computing faculty were collected through an online survey comprising
closed-ended questions to quantify barriers, enablers, and resource needs,
along with an open-ended question to gather qualitative insights. Quantitative
data were analyzed using statistical methods, while qualitative responses
underwent thematic analysis. Results reveal that while PjBL is widely valued,
its adoption is often selective and impacted by challenges in planning and
managing the learning process, designing suitable projects, and a lack of
institutional support, such as time, funding, and teaching assistants. Faculty
are more likely to adopt or sustain PjBL when they have access to peer
collaboration, professional development, and institutional incentives. In
addition, sourcing projects from research, industry partnerships, and borrowing
from peers emerged as key facilitators for new projects. These findings
underscore the need for systemic support structures to empower faculty to
experiment with and scale PjBL practices.

</details>


### [6] [An Empirical Study of Complexity, Heterogeneity, and Compliance of GitHub Actions Workflows](https://arxiv.org/abs/2507.18062)
*Edward Abrokwah,Taher A. Ghaleb*

Main category: cs.SE

TL;DR: 该研究分析了GitHub Actions（GHA）在开源项目中的使用情况，探讨了其复杂性、结构模式、合规性以及跨编程语言的差异。


<details>
  <summary>Details</summary>
Motivation: 尽管GHA提供了官方文档和社区最佳实践，但缺乏对开源项目中实际CI工作流与这些实践对齐情况的实证研究。

Method: 通过分析Java、Python和C++仓库中的大量GHA工作流数据，研究其结构、复杂性和合规性。

Result: 预计将揭示最佳实践的遵循情况以及需要改进的领域。

Conclusion: 研究结果可为CI服务提供更清晰的指南和示例，以优化文档和实践。

Abstract: Continuous Integration (CI) has evolved from a tooling strategy to a
fundamental mindset in modern CI engineering. It enables teams to develop,
test, and deliver software rapidly and collaboratively. Among CI services,
GitHub Actions (GHA) has emerged as a dominant service due to its deep
integration with GitHub and a vast ecosystem of reusable workflow actions.
Although GHA provides official documentation and community-supported best
practices, there appears to be limited empirical understanding of how
open-source real-world CI workflows align with such practices. Many workflows
might be unnecessarily complex and not aligned with the simplicity goals of CI
practices. This study will investigate the structure, complexity,
heterogeneity, and compliance of GHA workflows in open-source software
repositories. Using a large dataset of GHA workflows from Java, Python, and C++
repositories, our goal is to (a) identify workflow complexities, (b) analyze
recurring and heterogeneous structuring patterns, (c) assess compliance with
GHA best practices, and (d) uncover differences in CI pipeline design across
programming languages. Our findings are expected to reveal both areas of strong
adherence to best practices and areas for improvement where needed. These
insights will also have implications for CI services, as they will highlight
the need for clearer guidelines and comprehensive examples in CI documentation.

</details>


### [7] [Identifier Name Similarities: An Exploratory Study](https://arxiv.org/abs/2507.18081)
*Carol Wong,Mai Abe,Silvia De Benedictis,Marissa Halim,Anthony Peruma*

Main category: cs.SE

TL;DR: 研究探讨了标识符名称相似性对代码理解和协作的影响，并提出了一个分类法来分类不同形式的名称相似性。


<details>
  <summary>Details</summary>
Motivation: 标识符名称在代码理解中至关重要，但名称相似性可能导致误解和认知负担，影响协作。

Method: 通过开发一个分类法，研究了软件项目中标识符名称相似性的出现情况。

Result: 初步提出了一个分类法，为研究者提供了分析名称相似性对代码理解和协作影响的平台。

Conclusion: 分类法为进一步研究和改进提供了基础，未来可扩展和细化。

Abstract: Identifier names, which comprise a significant portion of the codebase, are
the cornerstone of effective program comprehension. However, research has shown
that poorly chosen names can significantly increase cognitive load and hinder
collaboration. Even names that appear readable in isolation may lead to
misunderstandings in contexts when they closely resemble other names in either
structure or functionality. In this exploratory study, we present our
preliminary findings on the occurrence of identifier name similarity in
software projects through the development of a taxonomy that categorizes
different forms of identifier name similarity. We envision our initial taxonomy
providing researchers with a platform to analyze and evaluate the impact of
identifier name similarity on code comprehension, maintainability, and
collaboration among developers, while also allowing for further refinement and
expansion of the taxonomy.

</details>


### [8] [Understanding the Supply Chain and Risks of Large Language Model Applications](https://arxiv.org/abs/2507.18105)
*Yujie Ma,Lili Quan,Xiaofei Xie,Qiang Hu,Jiongchi Yu,Yao Zhang,Sen Chen*

Main category: cs.SE

TL;DR: 论文提出了首个全面的LLM供应链安全数据集，揭示了深度嵌套的依赖关系和显著漏洞，并提出了实用建议。


<details>
  <summary>Details</summary>
Motivation: 随着LLM系统的广泛应用，其复杂供应链的风险评估缺乏系统性研究，现有方法多局限于模型或数据层面。

Method: 收集3,859个真实LLM应用，分析其依赖关系（模型、数据集、库），并提取1,555个风险问题进行评估。

Result: 发现LLM应用存在深度嵌套依赖和供应链广泛漏洞，凸显全面安全分析的必要性。

Conclusion: 建议研究人员和开发者采取综合安全措施，以构建更安全的LLM系统。

Abstract: The rise of Large Language Models (LLMs) has led to the widespread deployment
of LLM-based systems across diverse domains. As these systems proliferate,
understanding the risks associated with their complex supply chains is
increasingly important. LLM-based systems are not standalone as they rely on
interconnected supply chains involving pretrained models, third-party
libraries, datasets, and infrastructure. Yet, most risk assessments narrowly
focus on model or data level, overlooking broader supply chain vulnerabilities.
While recent studies have begun to address LLM supply chain risks, there
remains a lack of benchmarks for systematic research.
  To address this gap, we introduce the first comprehensive dataset for
analyzing and benchmarking LLM supply chain security. We collect 3,859
real-world LLM applications and perform interdependency analysis, identifying
109,211 models, 2,474 datasets, and 9,862 libraries. We extract model
fine-tuning paths, dataset reuse, and library reliance, mapping the ecosystem's
structure. To evaluate security, we gather 1,555 risk-related issues-50 for
applications, 325 for models, 18 for datasets, and 1,229 for libraries from
public vulnerability databases.
  Using this dataset, we empirically analyze component dependencies and risks.
Our findings reveal deeply nested dependencies in LLM applications and
significant vulnerabilities across the supply chain, underscoring the need for
comprehensive security analysis. We conclude with practical recommendations to
guide researchers and developers toward safer, more trustworthy LLM-enabled
systems.

</details>


### [9] [NoCode-bench: A Benchmark for Evaluating Natural Language-Driven Feature Addition](https://arxiv.org/abs/2507.18130)
*Le Deng,Zhonghao Jiang,Jialun Cao,Michael Pradel,Zhongxin Liu*

Main category: cs.SE

TL;DR: NoCode-bench是一个评估LLMs在自然语言驱动无代码开发中性能的基准，包含634个任务。实验显示，最佳LLMs的任务成功率仅为15.79%，表明LLMs在跨文件编辑和代码库理解方面仍有挑战。


<details>
  <summary>Details</summary>
Motivation: 通过自然语言驱动无代码开发提高生产力和普及开发，利用LLMs潜力。

Method: 引入NoCode-bench基准，包含634个任务和114k代码变更，通过文档更新与代码实现配对，验证开发者编写的测试用例。

Result: 最佳LLMs的任务成功率为15.79%，显示其在跨文件编辑和代码库理解方面的不足。

Conclusion: LLMs尚未准备好完全支持自然语言驱动的无代码开发，NoCode-bench为未来研究奠定了基础。

Abstract: Natural language-driven no-code development allows users to specify software
functionality using natural language (NL) instead of editing source code,
promising increased productivity and democratized development. Large language
models (LLMs) show potential in enabling this paradigm. In this context,
software documentation acts as an NL specification for functionality. This work
introduces NoCode-bench, a benchmark designed to evaluate LLMs on real-world
NL-driven feature addition tasks, consisting of 634 tasks across 10 projects
and 114k code changes. Each task pairs documentation updates with corresponding
code implementations, validated by developer-written test cases. A subset of
114 high-quality, human-verified instances, NoCode-bench Verified, ensures
reliable evaluation. Our experiments reveal that, despite high token usage, the
best LLMs achieve a task success rate of only 15.79%, highlighting challenges
in cross-file editing, codebase understanding, and tool calling. These findings
indicate that LLMs are not yet ready for fully NL-driven no-code development.
NoCode-bench lays the foundation for future advances in this area.

</details>


### [10] [SMECS: A Software Metadata Extraction and Curation Software](https://arxiv.org/abs/2507.18159)
*Stephan Ferenz,Aida Jafarbigloo,Oliver Werth,Astrid Nieße*

Main category: cs.SE

TL;DR: SMECS是一个帮助研究人员和研究软件工程师高效提取和编辑元数据的工具，支持FAIR原则。


<details>
  <summary>Details</summary>
Motivation: 高质量的元数据对实现FAIR原则至关重要，但手动创建费时费力。

Method: 开发了SMECS工具，从GitHub等在线仓库提取元数据，并提供交互界面进行编辑和导出。

Result: 通过可用性实验验证，SMECS提供了良好的用户体验。

Conclusion: SMECS简化了元数据创建，支持研究软件的FAIR化。

Abstract: Metadata play a crucial role in adopting the FAIR principles for research
software and enables findability and reusability. However, creating
high-quality metadata can be resource-intensive for researchers and research
software engineers. To address this challenge, we developed the Software
Metadata Extraction and Curation Software (SMECS) which integrates the
extraction of metadata from existing sources together with a user-friendly
interface for metadata curation. SMECS extracts metadata from online
repositories such as GitHub and presents it to researchers through an
interactive interface for further curation and export as a CodeMeta file. The
usability of SMECS was evaluated through usability experiments which confirmed
that SMECS provides a satisfactory user experience. SMECS supports the
FAIRification of research software by simplifying metadata creation.

</details>


### [11] [GenAI for Automotive Software Development: From Requirements to Wheels](https://arxiv.org/abs/2507.18223)
*Nenad Petrovic,Fengjunjie Pan,Vahid Zolfaghari,Krzysztof Lebioda,Andre Schamschurko,Alois Knoll*

Main category: cs.SE

TL;DR: 提出一种基于GenAI的汽车软件开发方法，专注于自动驾驶和ADAS功能，利用LLM和RAG技术优化需求一致性检查、测试场景生成和代码实现。


<details>
  <summary>Details</summary>
Motivation: 缩短ADAS相关功能的合规性和再工程周期，减少开发和测试时间。

Method: 采用LLM进行需求建模（Ecore、XMI、OCL）、测试场景生成、仿真代码（Python）和目标平台代码（C++），并利用RAG从法规文档中增强测试场景生成。

Result: 实现了自动化需求检查、测试场景和代码生成，优化了开发流程。

Conclusion: 该方法显著提升了ADAS开发的效率和合规性。

Abstract: This paper introduces a GenAI-empowered approach to automated development of
automotive software, with emphasis on autonomous and Advanced Driver Assistance
Systems (ADAS) capabilities. The process starts with requirements as input,
while the main generated outputs are test scenario code for simulation
environment, together with implementation of desired ADAS capabilities
targeting hardware platform of the vehicle connected to testbench. Moreover, we
introduce additional steps for requirements consistency checking leveraging
Model-Driven Engineering (MDE). In the proposed workflow, Large Language Models
(LLMs) are used for model-based summarization of requirements (Ecore metamodel,
XMI model instance and OCL constraint creation), test scenario generation,
simulation code (Python) and target platform code generation (C++).
Additionally, Retrieval Augmented Generation (RAG) is adopted to enhance test
scenario generation from autonomous driving regulations-related documents. Our
approach aims shorter compliance and re-engineering cycles, as well as reduced
development and testing time when it comes to ADAS-related capabilities.

</details>


### [12] [An Empirical Study on Embodied Artificial Intelligence Robot (EAIR) Software Bugs](https://arxiv.org/abs/2507.18267)
*Zeqin Liao,Zibin Zheng,Peifan Reng,Henglong Liang,Zixu Gao,Zhixiang Chen,Wei Li,Yuhong Nan*

Main category: cs.SE

TL;DR: 该研究首次系统分析了885个EAIR系统错误，揭示了18种根本原因、15种症状和13个受影响模块，为未来研究提供了新见解。


<details>
  <summary>Details</summary>
Motivation: EAIR系统错误的普遍性和复杂性缺乏深入理解，阻碍了相关技术和实践的发展。

Method: 通过分析80个EAIR项目中的885个错误，分类症状、原因和模块分布。

Result: 发现8种EAIR特有症状和8种特有原因，并构建了错误原因与模块的映射关系。

Conclusion: 研究为EAIR系统错误的预测、检测和修复提供了重要参考。

Abstract: Embodied Artificial Intelligence Robots (EAIR) is an emerging and rapidly
evolving technological domain. Ensuring their program correctness is
fundamental to their successful deployment. However, a general and in-depth
understanding of EAIR system bugs remains lacking, which hinders the
development of practices and techniques to tackle EAIR system bugs.
  To bridge this gap, we conducted the first systematic study of 885 EAIR
system bugs collected from 80 EAIR system projects to investigate their
symptoms, underlying causes, and module distribution. Our analysis takes
considerable effort, which classifies these bugs into 18 underlying causes, 15
distinct symptoms, and identifies 13 affected modules. It reveals several new
interesting findings and implications which help shed light on future research
on tackling or repairing EAIR system bugs. First, among the 15 identified
symptoms, our findings highlight 8 symptoms specific to EAIR systems, which is
characterized by severe functional failures and potential physical hazards.
Second, within the 18 underlying causes, we define 8 EAIR-specific causes, the
majority of which stem from the intricate issues of AI- agent reasoning and
decision making. Finally, to facilitate precise and efficient bug prediction,
detection, and repair, we constructed a mapping between underlying causes and
the modules in which they most frequently occur, which enables researchers to
focus diagnostic efforts on the modules most susceptible to specific bug types.

</details>


### [13] [Scheduzz: Constraint-based Fuzz Driver Generation with Dual Scheduling](https://arxiv.org/abs/2507.18289)
*Yan Li,Wenzhang Yang,Yuekun Wang,Jian Gao,Shaohua Wang,Yinxing Xue,Lijun Zhang*

Main category: cs.SE

TL;DR: Scheduzz是一种基于LLM的自动库模糊测试技术，通过理解库的合理使用和提取API组合约束，优化计算资源利用，显著提高覆盖率和发现未知漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有库模糊测试技术因缺乏对库使用规范的遵循，生成不合理驱动程序，浪费资源且效果不佳。

Method: 利用LLM理解库的合理使用并提取API组合约束，采用双调度框架管理API组合和模糊驱动程序。

Result: 在33个真实库中，Scheduzz显著减少计算开销，覆盖率和漏洞发现优于现有技术。

Conclusion: Scheduzz通过智能调度和LLM支持，有效提升库模糊测试的效率和效果。

Abstract: Fuzzing a library requires experts to understand the library usage well and
craft high-quality fuzz drivers, which is tricky and tedious. Therefore, many
techniques have been proposed to automatically generate fuzz drivers. However,
they fail to generate rational fuzz drivers due to the lack of adherence to
proper library usage conventions, such as ensuring a resource is closed after
being opened. To make things worse, existing library fuzzing techniques
unconditionally execute each driver, resulting in numerous irrational drivers
that waste computational resources while contributing little coverage and
generating false positive bug reports.
  To tackle these challenges, we propose a novel automatic library fuzzing
technique, Scheduzz, an LLM-based library fuzzing technique. It leverages LLMs
to understand rational usage of libraries and extract API combination
constraints. To optimize computational resource utilization, a dual scheduling
framework is implemented to efficiently manage API combinations and fuzz
drivers. The framework models driver generation and the corresponding fuzzing
campaign as an online optimization problem. Within the scheduling loop,
multiple API combinations are selected to generate fuzz drivers, while
simultaneously, various optimized fuzz drivers are scheduled for execution or
suspension.
  We implemented Scheduzz and evaluated it in 33 real-world libraries. Compared
to baseline approaches, Scheduzz significantly reduces computational overhead
and outperforms UTopia on 16 out of 21 libraries. It achieves 1.62x, 1.50x, and
1.89x higher overall coverage than the state-of-the-art techniques CKGFuzzer,
Promptfuzz, and the handcrafted project OSS-Fuzz, respectively. In addition,
Scheduzz discovered 33 previously unknown bugs in these well-tested libraries,
3 of which have been assigned CVEs.

</details>


### [14] [YATE: The Role of Test Repair in LLM-Based Unit Test Generation](https://arxiv.org/abs/2507.18316)
*Michael Konstantinou,Renzo Degiovanni,Jie M. Zhang,Mark Harman,Mike Papadakis*

Main category: cs.SE

TL;DR: YATE是一种通过静态分析和重新提示修复语言模型生成的错误测试的技术，显著提升了测试覆盖率和突变杀死率。


<details>
  <summary>Details</summary>
Motivation: 语言模型生成的测试常存在语法和语义错误，修复这些错误可以提升测试价值并为生成更多测试提供良好种子。

Method: 结合基于规则的静态分析和重新提示技术修复错误测试。

Result: YATE在6个开源项目中平均提升32.06%的行覆盖率和21.77%的突变杀死率，优于其他LLM方法。

Conclusion: YATE是一种高效且低成本的测试修复方法，显著优于现有技术。

Abstract: Recent advances in automated test generation utilises language models to
produce unit tests. While effective, language models tend to generate many
incorrect tests with respect to both syntax and semantics. Although such
incorrect tests can be easily detected and discarded, they constitute a "missed
opportunity" -- if fixed, they are often valuable as they directly add testing
value (they effectively target the underlying program logic to be tested) and
indirectly form good seeds for generating additional tests. To this end, we
propose a simple technique for repairing some of these incorrect tests through
a combination of rule-based static analysis and re-prompting. We evaluate this
simple approach, named YATE, on a set of 6 open-source projects and show that
it can effectively produce tests that cover on average 32.06% more lines and
kill 21.77% more mutants than a plain LLM-based method. We also compare YATE
with four other LLM-based methods, namely HITS, SYMPROMPT, TESTSPARK and
COVERUP and show that it produces tests that cover substantially more code.
YATE achieves 22% higher line coverage, 20% higher branch coverage and kill 20%
more mutants at a comparable cost (number of calls to LLMs).

</details>


### [15] [Gotta catch 'em all! Towards File Localisation from Issues at Large](https://arxiv.org/abs/2507.18319)
*Jesse Maarleveld,Jiapan Guo,Daniel Feitosa*

Main category: cs.SE

TL;DR: 该论文提出了一个通用的文件定位数据管道，用于处理所有类型的问题，而非仅限于缺陷。研究发现，传统基于缺陷的启发式方法在通用问题上表现不佳，且不同问题类型之间存在微小但显著的性能差异。


<details>
  <summary>Details</summary>
Motivation: 研究动机是开发一种能够处理所有类型问题的文件定位方法，而不仅限于缺陷，以节省开发者的时间。

Method: 方法包括构建一个数据管道来处理任意分支和合并实践，使用传统信息检索方法评估文件定位性能，并通过统计分析研究已知偏差的影响。

Result: 结果显示，基于缺陷的启发式方法在通用问题上表现不佳，不同问题类型之间存在显著性能差异，且标识符对性能有轻微影响。

Conclusion: 结论是需要开发通用模型，并针对项目特定特征调整方法。

Abstract: Bug localisation, the study of developing methods to localise the files
requiring changes to resolve bugs, has been researched for a long time to
develop methods capable of saving developers' time. Recently, researchers are
starting to consider issues outside of bugs. Nevertheless, most existing
research into file localisation from issues focusses on bugs or uses other
selection methods to ensure only certain types of issues are considered as part
of the focus of the work. Our goal is to work on all issues at large, without
any specific selection.
  In this work, we provide a data pipeline for the creation of issue file
localisation datasets, capable of dealing with arbitrary branching and merging
practices. We provide a baseline performance evaluation for the file
localisation problem using traditional information retrieval approaches.
Finally, we use statistical analysis to investigate the influence of biases
known in the bug localisation community on our dataset.
  Our results show that methods designed using bug-specific heuristics perform
poorly on general issue types, indicating a need for research into general
purpose models. Furthermore, we find that there are small, but statistically
significant differences in performance between different issue types. Finally,
we find that the presence of identifiers have a small effect on performance for
most issue types. Many results are project-dependent, encouraging the
development of methods which can be tuned to project-specific characteristics.

</details>


### [16] [FMI Meets SystemC: A Framework for Cross-Tool Virtual Prototyping](https://arxiv.org/abs/2507.18339)
*Nils Bosbach,Meik Schmidt,Lukas Jünger,Matthias Berthold,Rainer Leupers*

Main category: cs.SE

TL;DR: 提出了一种新框架，通过FMI标准将SystemC虚拟平台与外部工具集成，实现更广泛的协同仿真，支持未修改的目标软件运行并接收真实环境数据。


<details>
  <summary>Details</summary>
Motivation: 随着系统复杂性增加，需要更全面的测试和虚拟原型设计。SystemC缺乏原生FMI支持，限制了其在协同仿真中的集成。

Method: 开发了一个新框架，利用FMI连接SystemC虚拟平台与外部工具，并通过案例研究展示如何通过FMI从外部工具获取温度数据。

Result: 该框架使目标软件能在虚拟平台上运行并接收真实环境数据，支持更早的软件测试和认证（如ISO 26262）。

Conclusion: 该框架扩展了SystemC虚拟平台的功能，使其能更好地融入协同仿真环境，加速软件开发与认证流程。

Abstract: As systems become more complex, the demand for thorough testing and virtual
prototyping grows. To simulate whole systems, multiple tools are usually needed
to cover different parts. These parts include the hardware of a system and the
environment with which the system interacts. The Functional Mock-up Interface
(FMI) standard for co-simulation can be used to connect these tools.
  The control part of modern systems is usually a computing unit, such as a
System-on-a-Chip (SoC) or Microcontroller Unit (MCU), which executes software
from a connected memory and interacts with peripherals. To develop software
without requiring access to physical hardware, full-system simulators, the
so-called Virtual Platforms (VPs), are commonly used. The IEEE-standardized
framework for VP development is SystemC TLM. SystemC provides interfaces and
concepts that enable modular design and model exchange. However, SystemC lacks
native FMI support, which limits the integration into broader co-simulation
environments.
  This paper presents a novel framework to control and interact with
SystemC-based VPs using the FMI. We present a case study showing how a
simulated temperature sensor in a SystemC simulation can obtain temperature
values from an external tool via FMI. This approach allows the unmodified
target software to run on the VP and receive realistic environmental input data
such as temperature, velocity, or acceleration values from other tools. Thus,
extensive software testing and verification is enabled. By having tests ready
and the software pre-tested using a VP once the physical hardware is available,
certifications like ISO 26262 can be done earlier.

</details>


### [17] [Automated Code Review Using Large Language Models with Symbolic Reasoning](https://arxiv.org/abs/2507.18476)
*Busra Icoz,Goksel Biricik*

Main category: cs.SE

TL;DR: 论文提出了一种结合符号推理技术与大语言模型（LLMs）的混合方法，用于自动化代码审查，提高了准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 手动代码审查主观性强且耗时，而现有的大语言模型缺乏逻辑推理能力，无法完全理解和评估代码。

Method: 采用混合方法，结合符号推理技术和LLMs，并在CodexGlue数据集上测试了多种模型（如CodeT5、CodeBERT、GraphCodeBERT）。

Result: 实验结果表明，该方法显著提升了自动化代码审查的准确性和效率。

Conclusion: 结合符号推理与LLMs的混合方法是自动化代码审查的有效解决方案。

Abstract: Code review is one of the key processes in the software development lifecycle
and is essential to maintain code quality. However, manual code review is
subjective and time consuming. Given its rule-based nature, code review is well
suited for automation. In recent years, significant efforts have been made to
automate this process with the help of artificial intelligence. Recent
developments in Large Language Models (LLMs) have also emerged as a promising
tool in this area, but these models often lack the logical reasoning
capabilities needed to fully understand and evaluate code. To overcome this
limitation, this study proposes a hybrid approach that integrates symbolic
reasoning techniques with LLMs to automate the code review process. We tested
our approach using the CodexGlue dataset, comparing several models, including
CodeT5, CodeBERT, and GraphCodeBERT, to assess the effectiveness of combining
symbolic reasoning and prompting techniques with LLMs. Our results show that
this approach improves the accuracy and efficiency of automated code review.

</details>


### [18] [A Deep Dive into Retrieval-Augmented Generation for Code Completion: Experience on WeChat](https://arxiv.org/abs/2507.18515)
*Zezhou Yang,Ting Peng,Cuiyun Gao,Chaozheng Wang,Hailiang Huang,Yuetang Deng*

Main category: cs.SE

TL;DR: 论文研究了检索增强生成（RAG）在工业级闭源代码库（如微信）中的代码补全效果，发现相似性RAG优于标识符RAG，且结合词法和语义检索技术效果最佳。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注开源代码库，而闭源代码库的分布差异带来独特挑战，需探索RAG在工业场景的实际表现。

Method: 在微信代码库中，对比标识符RAG和相似性RAG，测试26个开源LLM（0.5B-671B参数），并采用词法（BM25）和语义（GTE-Qwen）检索技术。

Result: 相似性RAG表现更优，且高级检索技术提升效果；词法和语义检索结合效果最佳。

Conclusion: RAG在闭源代码库中有效，结合多种检索技术可最大化实用价值，开发者调查验证了其实际应用潜力。

Abstract: Code completion, a crucial task in software engineering that enhances
developer productivity, has seen substantial improvements with the rapid
advancement of large language models (LLMs). In recent years,
retrieval-augmented generation (RAG) has emerged as a promising method to
enhance the code completion capabilities of LLMs, which leverages relevant
context from codebases without requiring model retraining. While existing
studies have demonstrated the effectiveness of RAG on public repositories and
benchmarks, the potential distribution shift between open-source and
closed-source codebases presents unique challenges that remain unexplored. To
mitigate the gap, we conduct an empirical study to investigate the performance
of widely-used RAG methods for code completion in the industrial-scale codebase
of WeChat, one of the largest proprietary software systems. Specifically, we
extensively explore two main types of RAG methods, namely identifier-based RAG
and similarity-based RAG, across 26 open-source LLMs ranging from 0.5B to 671B
parameters. For a more comprehensive analysis, we employ different retrieval
techniques for similarity-based RAG, including lexical and semantic retrieval.
Based on 1,669 internal repositories, we achieve several key findings: (1) both
RAG methods demonstrate effectiveness in closed-source repositories, with
similarity-based RAG showing superior performance, (2) the effectiveness of
similarity-based RAG improves with more advanced retrieval techniques, where
BM25 (lexical retrieval) and GTE-Qwen (semantic retrieval) achieve superior
performance, and (3) the combination of lexical and semantic retrieval
techniques yields optimal results, demonstrating complementary strengths.
Furthermore, we conduct a developer survey to validate the practical utility of
RAG methods in real-world development environments.

</details>
