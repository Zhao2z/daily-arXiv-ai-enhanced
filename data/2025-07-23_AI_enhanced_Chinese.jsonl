{"id": "2507.15887", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15887", "abs": "https://arxiv.org/abs/2507.15887", "authors": ["Ori Press", "Brandon Amos", "Haoyu Zhao", "Yikai Wu", "Samuel K. Ainsworth", "Dominik Krupke", "Patrick Kidger", "Touqir Sajed", "Bartolomeo Stellato", "Jisun Park", "Nathanael Bosch", "Eli Meril", "Albert Steppi", "Arman Zharmagambetov", "Fangzhao Zhang", "David Perez-Pineiro", "Alberto Mercurio", "Ni Zhan", "Talor Abramovich", "Kilian Lieret", "Hanlin Zhang", "Shirley Huang", "Matthias Bethge", "Ofir Press"], "title": "AlgoTune: Can Language Models Speed Up General-Purpose Numerical Programs?", "comment": null, "summary": "Despite progress in language model (LM) capabilities, evaluations have thus\nfar focused on models' performance on tasks that humans have previously solved,\nincluding in programming (Jimenez et al., 2024) and mathematics (Glazer et al.,\n2024). We therefore propose testing models' ability to design and implement\nalgorithms in an open-ended benchmark: We task LMs with writing code that\nefficiently solves computationally challenging problems in computer science,\nphysics, and mathematics. Our AlgoTune benchmark consists of 155 coding tasks\ncollected from domain experts and a framework for validating and timing\nLM-synthesized solution code, which is compared to reference implementations\nfrom popular open-source packages. In addition, we develop a baseline LM agent,\nAlgoTuner, and evaluate its performance across a suite of frontier models.\nAlgoTuner achieves an average 1.72x speedup against our reference solvers,\nwhich use libraries such as SciPy, sk-learn and CVXPY. However, we find that\ncurrent models fail to discover algorithmic innovations, instead preferring\nsurface-level optimizations. We hope that AlgoTune catalyzes the development of\nLM agents exhibiting creative problem solving beyond state-of-the-art human\nperformance.", "AI": {"tldr": "\u7814\u7a76\u8005\u63d0\u51fa\u4e86AlgoTune\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u8bbe\u8ba1\u548c\u5b9e\u73b0\u7b97\u6cd5\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u867d\u80fd\u5b9e\u73b01.72\u500d\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4f46\u7f3a\u4e4f\u7b97\u6cd5\u521b\u65b0\u80fd\u529b\uff0c\u4e3b\u8981\u4f9d\u8d56\u8868\u9762\u4f18\u5316\u3002", "motivation": "\u73b0\u6709\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u4e3b\u8981\u96c6\u4e2d\u5728\u4eba\u7c7b\u5df2\u89e3\u51b3\u7684\u4efb\u52a1\u4e0a\uff0c\u7f3a\u4e4f\u5bf9\u6a21\u578b\u5728\u5f00\u653e\u6027\u7b97\u6cd5\u8bbe\u8ba1\u548c\u5b9e\u73b0\u65b9\u9762\u521b\u65b0\u80fd\u529b\u7684\u6d4b\u8bd5\uff0c\u9700\u8981\u8bc4\u4f30\u6a21\u578b\u662f\u5426\u80fd\u8d85\u8d8a\u73b0\u6709\u4eba\u7c7b\u8868\u73b0\u5e76\u5c55\u73b0\u521b\u9020\u6027\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b155\u4e2a\u7f16\u7801\u4efb\u52a1\u7684AlgoTune\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4efb\u52a1\u6765\u81ea\u9886\u57df\u4e13\u5bb6\uff0c\u6db5\u76d6\u8ba1\u7b97\u673a\u79d1\u5b66\u3001\u7269\u7406\u5b66\u548c\u6570\u5b66\u7b49\u9886\u57df\uff1b\u5f00\u53d1\u4e86\u9a8c\u8bc1\u548c\u8ba1\u65f6\u6846\u67b6\u6765\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u89e3\u51b3\u65b9\u6848\u4ee3\u7801\uff1b\u521b\u5efa\u4e86\u57fa\u7ebf\u8bed\u8a00\u6a21\u578b\u4ee3\u7406AlgoTuner\u5e76\u5728\u591a\u4e2a\u524d\u6cbf\u6a21\u578b\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "AlgoTuner\u76f8\u6bd4\u4f7f\u7528SciPy\u3001sk-learn\u548cCVXPY\u7b49\u5e93\u7684\u53c2\u8003\u6c42\u89e3\u5668\u5e73\u5747\u5b9e\u73b0\u4e861.72\u500d\u7684\u52a0\u901f\uff1b\u4f46\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u65e0\u6cd5\u53d1\u73b0\u7b97\u6cd5\u521b\u65b0\uff0c\u66f4\u503e\u5411\u4e8e\u8fdb\u884c\u8868\u9762\u5c42\u6b21\u7684\u4f18\u5316\u800c\u975e\u6df1\u5c42\u7b97\u6cd5\u6539\u8fdb\u3002", "conclusion": "\u867d\u7136\u8bed\u8a00\u6a21\u578b\u5728\u7b97\u6cd5\u5b9e\u73b0\u6548\u7387\u4e0a\u6709\u6240\u63d0\u5347\uff0c\u4f46\u5728\u7b97\u6cd5\u521b\u65b0\u65b9\u9762\u4ecd\u5b58\u5728\u663e\u8457\u4e0d\u8db3\uff0c\u5e0c\u671bAlgoTune\u57fa\u51c6\u80fd\u591f\u63a8\u52a8\u5f00\u53d1\u51fa\u5177\u6709\u8d85\u8d8a\u4eba\u7c7b\u6700\u5148\u8fdb\u8868\u73b0\u7684\u521b\u9020\u6027\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u7684\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u3002"}}
{"id": "2507.15889", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15889", "abs": "https://arxiv.org/abs/2507.15889", "authors": ["Noah van der Vleuten"], "title": "Dr. Boot: Bootstrapping Program Synthesis Language Models to Perform Repairing", "comment": "Master's thesis, University of Amsterdam, 2023\n  (https://scripties.uba.uva.nl/search?id=record_54126). Code and experiments\n  available at: https://github.com/NoahVl/Dr-Boot", "summary": "Language models for program synthesis are usually trained and evaluated on\nprogramming competition datasets (MBPP, APPS). However, these datasets are\nlimited in size and quality, while these language models are extremely data\nhungry. Additionally, the language models have a misaligned program synthesis\nprocess compared to humans. While humans iteratively develop code with the help\nof a compiler, most program synthesis models currently produce code in one go.\nTo solve these issues, we introduce a bootstrapping algorithm for program\nsynthesis, that supports teaching models how to repair. We show that\nbootstrapping consistently outperforms regular fine-tuning. Compared to other\nwork, our bootstrapped model performs on par with fine-tuned models that are\n68\\% larger. Notably, bootstrapping with repairing also improves non-repairing\nperformance compared to regular bootstrapping during inference. However, on our\nmodels, repairing during inference is likely inferior to simply sampling the\nsame number of solutions. Furthermore, we find that there are issues with the\nexample test cases in the training portion of the APPS dataset that are\nvaluable to the community, as many repairing and reinforcement learning methods\nrely on them.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u7a0b\u5e8f\u5408\u6210\u7684\u81ea\u4e3e\u7b97\u6cd5\uff0c\u901a\u8fc7\u6559\u6388\u6a21\u578b\u5982\u4f55\u4fee\u590d\u4ee3\u7801\u6765\u89e3\u51b3\u73b0\u6709\u7f16\u7a0b\u6570\u636e\u96c6\u89c4\u6a21\u6709\u9650\u548c\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u7f16\u7a0b\u8fc7\u7a0b\u4e0d\u5339\u914d\u7684\u95ee\u9898\u3002\u5b9e\u9a8c\u8868\u660e\u81ea\u4e3e\u65b9\u6cd5\u6301\u7eed\u4f18\u4e8e\u5e38\u89c4\u5fae\u8c03\uff0c\u4e14\u4fee\u590d\u8bad\u7ec3\u8fd8\u80fd\u63d0\u5347\u975e\u4fee\u590d\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7a0b\u5e8f\u5408\u6210\u8bed\u8a00\u6a21\u578b\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u8bad\u7ec3\u548c\u8bc4\u4f30\u6570\u636e\u96c6\uff08MBPP\u3001APPS\uff09\u89c4\u6a21\u6709\u9650\u4e14\u8d28\u91cf\u4e0d\u9ad8\uff0c\u800c\u8bed\u8a00\u6a21\u578b\u9700\u8981\u5927\u91cf\u6570\u636e\uff1b2\uff09\u6a21\u578b\u7684\u7a0b\u5e8f\u5408\u6210\u8fc7\u7a0b\u4e0e\u4eba\u7c7b\u4e0d\u5339\u914d\uff0c\u4eba\u7c7b\u901a\u8fc7\u7f16\u8bd1\u5668\u8fed\u4ee3\u5f00\u53d1\u4ee3\u7801\uff0c\u800c\u5927\u591a\u6570\u6a21\u578b\u4e00\u6b21\u6027\u751f\u6210\u4ee3\u7801\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u652f\u6301\u6559\u6388\u6a21\u578b\u4fee\u590d\u80fd\u529b\u7684\u7a0b\u5e8f\u5408\u6210\u81ea\u4e3e\u7b97\u6cd5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u81ea\u4e3e\u8fc7\u7a0b\u8bad\u7ec3\u6a21\u578b\u5b66\u4f1a\u4fee\u590d\u4ee3\u7801\uff0c\u4f7f\u5176\u66f4\u63a5\u8fd1\u4eba\u7c7b\u7684\u8fed\u4ee3\u7f16\u7a0b\u8fc7\u7a0b\u3002", "result": "\u81ea\u4e3e\u65b9\u6cd5\u6301\u7eed\u4f18\u4e8e\u5e38\u89c4\u5fae\u8c03\uff1b\u4e0e\u5176\u4ed6\u5de5\u4f5c\u76f8\u6bd4\uff0c\u81ea\u4e3e\u6a21\u578b\u7684\u6027\u80fd\u4e0e\u592768%\u7684\u5fae\u8c03\u6a21\u578b\u76f8\u5f53\uff1b\u5e26\u4fee\u590d\u7684\u81ea\u4e3e\u8bad\u7ec3\u8fd8\u80fd\u63d0\u5347\u63a8\u7406\u65f6\u7684\u975e\u4fee\u590d\u6027\u80fd\uff1b\u4f46\u5728\u63a8\u7406\u65f6\u8fdb\u884c\u4fee\u590d\u53ef\u80fd\u4e0d\u5982\u7b80\u5355\u5730\u91c7\u6837\u76f8\u540c\u6570\u91cf\u7684\u89e3\u51b3\u65b9\u6848\uff1b\u53d1\u73b0APPS\u6570\u636e\u96c6\u8bad\u7ec3\u90e8\u5206\u7684\u793a\u4f8b\u6d4b\u8bd5\u7528\u4f8b\u5b58\u5728\u95ee\u9898\u3002", "conclusion": "\u81ea\u4e3e\u7b97\u6cd5\u662f\u6539\u8fdb\u7a0b\u5e8f\u5408\u6210\u6a21\u578b\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u7ed3\u5408\u4fee\u590d\u80fd\u529b\u7684\u8bad\u7ec3\u3002\u867d\u7136\u4fee\u590d\u8bad\u7ec3\u80fd\u63d0\u5347\u6574\u4f53\u6027\u80fd\uff0c\u4f46\u63a8\u7406\u65f6\u7684\u4fee\u590d\u7b56\u7565\u4ecd\u9700\u4f18\u5316\u3002\u6b64\u5916\uff0c\u73b0\u6709\u6570\u636e\u96c6\u7684\u8d28\u91cf\u95ee\u9898\u9700\u8981\u793e\u533a\u5173\u6ce8\uff0c\u7279\u522b\u662f\u5bf9\u4f9d\u8d56\u6d4b\u8bd5\u7528\u4f8b\u7684\u4fee\u590d\u548c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002"}}
{"id": "2507.15892", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.15892", "abs": "https://arxiv.org/abs/2507.15892", "authors": ["Elijah Nnorom", "Md Basim Uddin Ahmed", "Jiho Shin", "Hung Viet Pham", "Song Wang"], "title": "StaAgent: An Agentic Framework for Testing Static Analyzers", "comment": null, "summary": "Static analyzers play a critical role in identifying bugs early in the\nsoftware development lifecycle, but their rule implementations are often\nunder-tested and prone to inconsistencies. To address this, we propose\nStaAgent, an agentic framework that harnesses the generative capabilities of\nLarge Language Models (LLMs) to systematically evaluate static analyzer rules.\nStaAgent comprises four specialized agents: a Seed Generation Agent that\ntranslates bug detection rules into concrete, bug-inducing seed programs; a\nCode Validation Agent that ensures the correctness of these seeds; a Mutation\nGeneration Agent that produces semantically equivalent mutants; and an Analyzer\nEvaluation Agent that performs metamorphic testing by comparing the static\nanalyzer's behavior on seeds and their corresponding mutants. By revealing\ninconsistent behaviors, StaAgent helps uncover flaws in rule implementations.\nThis LLM-driven, multi-agent framework offers a scalable and adaptable solution\nto improve the reliability of static analyzers. We evaluated StaAgent with five\nstate-of-the-art LLMs (CodeL-lama, DeepSeek, Codestral, Qwen, and GPT-4o)\nacross five widely used static analyzers (SpotBugs, SonarQube, ErrorProne,\nInfer, and PMD). The experimental results show that our approach can help\nreveal 64 problematic rules in the latest versions of these five static\nanalyzers (i.e., 28 in SpotBugs, 18 in SonarQube, 6 in ErrorProne, 4 in Infer,\nand 8 in PMD). In addition, 53 out of the 64 bugs cannot be detected by the\nSOTA baseline. We have reported all the bugs to developers, with two of them\nalready fixed. Three more have been confirmed by developers, while the rest are\nawaiting response. These results demonstrate the effectiveness of our approach\nand underscore the promise of agentic, LLM-driven data synthesis to advance\nsoftware engineering.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86StaAgent\uff0c\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u7cfb\u7edf\u6027\u8bc4\u4f30\u9759\u6001\u5206\u6790\u5668\u89c4\u5219\u7684\u6b63\u786e\u6027\u3002\u8be5\u6846\u67b6\u5305\u542b\u56db\u4e2a\u4e13\u95e8\u7684\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u53d8\u5f02\u6d4b\u8bd5\u53d1\u73b0\u9759\u6001\u5206\u6790\u5668\u5b9e\u73b0\u4e2d\u7684\u7f3a\u9677\uff0c\u5728\u4e94\u4e2a\u4e3b\u6d41\u9759\u6001\u5206\u6790\u5668\u4e2d\u53d1\u73b0\u4e8664\u4e2a\u95ee\u9898\u89c4\u5219\u3002", "motivation": "\u9759\u6001\u5206\u6790\u5668\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\uff0c\u4f46\u5176\u89c4\u5219\u5b9e\u73b0\u5f80\u5f80\u6d4b\u8bd5\u4e0d\u8db3\u4e14\u5bb9\u6613\u51fa\u73b0\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u7cfb\u7edf\u6027\u5730\u8bc4\u4f30\u9759\u6001\u5206\u6790\u5668\u89c4\u5219\u7684\u6b63\u786e\u6027\uff0c\u9700\u8981\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\u6765\u63d0\u9ad8\u9759\u6001\u5206\u6790\u5668\u7684\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51faStaAgent\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5305\u542b\u56db\u4e2a\u4e13\u95e8\u667a\u80fd\u4f53\uff1a1\uff09\u79cd\u5b50\u751f\u6210\u667a\u80fd\u4f53\uff1a\u5c06\u6f0f\u6d1e\u68c0\u6d4b\u89c4\u5219\u8f6c\u6362\u4e3a\u5177\u4f53\u7684\u5305\u542b\u6f0f\u6d1e\u7684\u79cd\u5b50\u7a0b\u5e8f\uff1b2\uff09\u4ee3\u7801\u9a8c\u8bc1\u667a\u80fd\u4f53\uff1a\u786e\u4fdd\u79cd\u5b50\u7a0b\u5e8f\u7684\u6b63\u786e\u6027\uff1b3\uff09\u53d8\u5f02\u751f\u6210\u667a\u80fd\u4f53\uff1a\u4ea7\u751f\u8bed\u4e49\u7b49\u4ef7\u7684\u53d8\u5f02\u4f53\uff1b4\uff09\u5206\u6790\u5668\u8bc4\u4f30\u667a\u80fd\u4f53\uff1a\u901a\u8fc7\u6bd4\u8f83\u9759\u6001\u5206\u6790\u5668\u5728\u79cd\u5b50\u7a0b\u5e8f\u53ca\u5176\u53d8\u5f02\u4f53\u4e0a\u7684\u884c\u4e3a\u8fdb\u884c\u53d8\u5f02\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e0d\u4e00\u81f4\u7684\u884c\u4e3a\u6a21\u5f0f\u3002", "result": "\u4f7f\u7528\u4e94\u4e2a\u6700\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e94\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u9759\u6001\u5206\u6790\u5668\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u53d1\u73b0\u4e8664\u4e2a\u95ee\u9898\u89c4\u5219\uff08SpotBugs 28\u4e2a\uff0cSonarQube 18\u4e2a\uff0cErrorProne 6\u4e2a\uff0cInfer 4\u4e2a\uff0cPMD 8\u4e2a\uff09\u3002\u5176\u4e2d53\u4e2a\u7f3a\u9677\u65e0\u6cd5\u88ab\u73b0\u6709\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\u68c0\u6d4b\u5230\u3002\u5df2\u5411\u5f00\u53d1\u8005\u62a5\u544a\u6240\u6709\u7f3a\u9677\uff0c2\u4e2a\u5df2\u4fee\u590d\uff0c3\u4e2a\u5df2\u786e\u8ba4\uff0c\u5176\u4f59\u6b63\u5728\u7b49\u5f85\u56de\u590d\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u57fa\u4e8e\u667a\u80fd\u4f53\u548c\u5927\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u6570\u636e\u5408\u6210\u5728\u63a8\u8fdb\u8f6f\u4ef6\u5de5\u7a0b\u65b9\u9762\u7684\u6f5c\u529b\u3002StaAgent\u4e3a\u63d0\u9ad8\u9759\u6001\u5206\u6790\u5668\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u8f6f\u4ef6\u8d28\u91cf\u4fdd\u8bc1\u9886\u57df\u5e26\u6765\u4e86\u65b0\u7684\u6280\u672f\u7a81\u7834\u3002"}}
{"id": "2507.16037", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.16037", "abs": "https://arxiv.org/abs/2507.16037", "authors": ["Zhili Zeng", "Kimya Khakzad Shahandashti", "Alvine Boaye Belle", "Song Wang", "Zhen Ming", "Jiang"], "title": "A Pilot Study on LLM-Based Agentic Translation from Android to iOS: Pitfalls and Insights", "comment": null, "summary": "The rapid advancement of mobile applications has led to a significant demand\nfor cross-platform compatibility, particularly between the Android and iOS\nplatforms. Traditional approaches to mobile application translation often rely\non manual intervention or rule-based systems, which are labor-intensive and\ntime-consuming. While recent advancements in machine learning have introduced\nautomated methods, they often lack contextual understanding and adaptability,\nresulting in suboptimal translations. Large Language Models (LLMs) were\nrecently leveraged to enhance code translation at different granularities,\nincluding the method, class, and repository levels. Researchers have\ninvestigated common errors, limitations, and potential strategies to improve\nthese tasks. However, LLM-based application translation across different\nplatforms, such as migrating mobile applications between Android and iOS or\nadapting software across diverse frameworks, remains underexplored.\nUnderstanding the performance, strengths, and limitations of LLMs in\ncross-platform application translation is critical for advancing software\nengineering automation. This study aims to fill this gap by evaluating\nLLM-based agentic approaches for mobile application translation, identifying\nkey failure points, and proposing guidelines to improve translation\nperformance. We developed a chain of agents that account for dependencies,\nspecifications, program structure, and program control flow when translating\napplications from Android to iOS. To evaluate the performance, we manually\nexamined the translated code for syntactic correctness, semantic accuracy, and\nfunctional completeness. For translation failures, we further conducted a\ndetailed root cause analysis to understand the underlying limitations of the\nagentic translation process and identify opportunities for improvement.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\u65b9\u6cd5\u5728\u79fb\u52a8\u5e94\u7528\u8de8\u5e73\u53f0\u7ffb\u8bd1\u4e2d\u7684\u6027\u80fd\uff0c\u901a\u8fc7\u5f00\u53d1\u4e00\u4e2a\u8003\u8651\u4f9d\u8d56\u5173\u7cfb\u3001\u89c4\u8303\u3001\u7a0b\u5e8f\u7ed3\u6784\u548c\u63a7\u5236\u6d41\u7684\u667a\u80fd\u4f53\u94fe\uff0c\u5b9e\u73b0Android\u5230iOS\u5e94\u7528\u7684\u81ea\u52a8\u5316\u7ffb\u8bd1\uff0c\u5e76\u5206\u6790\u4e86\u7ffb\u8bd1\u5931\u8d25\u7684\u6839\u672c\u539f\u56e0\u4ee5\u63d0\u51fa\u6539\u8fdb\u6307\u5bfc\u65b9\u9488\u3002", "motivation": "\u4f20\u7edf\u7684\u79fb\u52a8\u5e94\u7528\u7ffb\u8bd1\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u5e72\u9884\u6216\u57fa\u4e8e\u89c4\u5219\u7684\u7cfb\u7edf\uff0c\u8017\u65f6\u4e14\u52b3\u52a8\u5bc6\u96c6\u3002\u867d\u7136\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u63d0\u4f9b\u4e86\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u7406\u89e3\u548c\u9002\u5e94\u6027\u3002\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u7ffb\u8bd1\u65b9\u9762\u5df2\u6709\u5e94\u7528\uff0c\u4f46\u8de8\u5e73\u53f0\u5e94\u7528\u7ffb\u8bd1\uff08\u5982Android\u4e0eiOS\u4e4b\u95f4\u7684\u8fc1\u79fb\uff09\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\uff0c\u9700\u8981\u6df1\u5165\u4e86\u89e3LLM\u5728\u6b64\u9886\u57df\u7684\u6027\u80fd\u3001\u4f18\u52bf\u548c\u5c40\u9650\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u667a\u80fd\u4f53\u94fe(chain of agents)\uff0c\u5728\u5c06\u5e94\u7528\u4eceAndroid\u7ffb\u8bd1\u5230iOS\u65f6\u8003\u8651\u4f9d\u8d56\u5173\u7cfb\u3001\u89c4\u8303\u3001\u7a0b\u5e8f\u7ed3\u6784\u548c\u7a0b\u5e8f\u63a7\u5236\u6d41\u3002\u901a\u8fc7\u624b\u52a8\u68c0\u67e5\u7ffb\u8bd1\u4ee3\u7801\u7684\u8bed\u6cd5\u6b63\u786e\u6027\u3001\u8bed\u4e49\u51c6\u786e\u6027\u548c\u529f\u80fd\u5b8c\u6574\u6027\u6765\u8bc4\u4f30\u6027\u80fd\u3002\u5bf9\u7ffb\u8bd1\u5931\u8d25\u6848\u4f8b\u8fdb\u884c\u8be6\u7ec6\u7684\u6839\u672c\u539f\u56e0\u5206\u6790\uff0c\u4ee5\u7406\u89e3\u667a\u80fd\u4f53\u7ffb\u8bd1\u8fc7\u7a0b\u7684\u6f5c\u5728\u5c40\u9650\u6027\u3002", "result": "\u901a\u8fc7\u8bc4\u4f30\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u65b9\u6cd5\u5728\u79fb\u52a8\u5e94\u7528\u7ffb\u8bd1\u4e2d\u7684\u8868\u73b0\uff0c\u8bc6\u522b\u4e86\u5173\u952e\u7684\u5931\u8d25\u70b9\uff0c\u5e76\u5bf9\u7ffb\u8bd1\u5931\u8d25\u8fdb\u884c\u4e86\u6839\u672c\u539f\u56e0\u5206\u6790\u3002\u7814\u7a76\u63ed\u793a\u4e86\u667a\u80fd\u4f53\u7ffb\u8bd1\u8fc7\u7a0b\u7684\u6f5c\u5728\u5c40\u9650\u6027\uff0c\u4e3a\u6539\u8fdb\u7ffb\u8bd1\u6027\u80fd\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002", "conclusion": "\u672c\u7814\u7a76\u586b\u8865\u4e86LLM\u5728\u8de8\u5e73\u53f0\u5e94\u7528\u7ffb\u8bd1\u9886\u57df\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u7ffb\u8bd1\u65b9\u6cd5\uff0c\u8bc6\u522b\u4e86\u5173\u952e\u6311\u6218\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u6307\u5bfc\u65b9\u9488\u3002\u8fd9\u4e3a\u63a8\u8fdb\u8f6f\u4ef6\u5de5\u7a0b\u81ea\u52a8\u5316\uff0c\u7279\u522b\u662f\u79fb\u52a8\u5e94\u7528\u8de8\u5e73\u53f0\u8fc1\u79fb\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u6709\u6548\u7684\u81ea\u52a8\u5316\u7ffb\u8bd1\u5de5\u5177\u548c\u65b9\u6cd5\u3002"}}
{"id": "2507.16044", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.16044", "abs": "https://arxiv.org/abs/2507.16044", "authors": ["Meriem Mastouri", "Emna Ksontini", "Wael Kessentini"], "title": "Making REST APIs Agent-Ready: From OpenAPI to Model Context Protocol Servers for Tool-Augmented LLMs", "comment": null, "summary": "Large Language Models (LLMs) are evolving from passive text generators into\nactive agents that invoke external tools. To support this shift, scalable\nprotocols for tool integration are essential. The Model Context Protocol (MCP),\nintroduced by Anthropic in 2024, offers a schema-driven standard for dynamic\ntool discovery and invocation. Yet, building MCP servers remains manual and\nrepetitive, requiring developers to write glue code, handle authentication, and\nconfigure schemas by hand-replicating much of the integration effort MCP aims\nto eliminate.\n  This paper investigates whether MCP server construction can be meaningfully\nautomated. We begin by analyzing adoption trends: among 22,000+ MCP-tagged\nGitHub repositories created within six months of release, fewer than 5% include\nservers, typically small, single-maintainer projects dominated by repetitive\nscaffolding. To address this gap, we present AutoMCP, a compiler that generates\nMCP servers from OpenAPI 2.0/3.0 specifications. AutoMCP parses REST API\ndefinitions and produces complete server implementations, including schema\nregistration and authentication handling.\n  We evaluate AutoMCP on 50 real-world APIs spanning 5,066 endpoints across\nover 10 domains. From a stratified sample of 1,023 tool calls, 76.5% succeeded\nout of the box. Manual failure analysis revealed five recurring issues, all\nattributable to inconsistencies or omissions in the OpenAPI contracts. After\nminor fixes, averaging 19 lines of spec changes per API, AutoMCP achieved 99.9%\nsuccess.\n  Our findings (i) analyze MCP adoption and quantify the cost of manual server\ndevelopment, (ii) demonstrate that OpenAPI specifications, despite quality\nissues, enable near-complete MCP server automation, and (iii) contribute a\ncorpus of 5,066 callable tools along with insights on repairing common\nspecification flaws.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAutoMCP\u7f16\u8bd1\u5668\uff0c\u80fd\u591f\u4eceOpenAPI\u89c4\u8303\u81ea\u52a8\u751f\u6210MCP\u670d\u52a1\u5668\uff0c\u89e3\u51b3\u4e86\u624b\u52a8\u6784\u5efaMCP\u670d\u52a1\u5668\u7e41\u7410\u91cd\u590d\u7684\u95ee\u9898\uff0c\u572850\u4e2a\u771f\u5b9eAPI\u76845066\u4e2a\u7aef\u70b9\u6d4b\u8bd5\u4e2d\u8fbe\u523099.9%\u7684\u6210\u529f\u7387\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u6b63\u4ece\u88ab\u52a8\u6587\u672c\u751f\u6210\u5668\u8f6c\u5411\u4e3b\u52a8\u8c03\u7528\u5916\u90e8\u5de5\u5177\u7684\u667a\u80fd\u4f53\uff0c\u9700\u8981\u53ef\u6269\u5c55\u7684\u5de5\u5177\u96c6\u6210\u534f\u8bae\u3002\u867d\u7136Anthropic\u7684\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae(MCP)\u63d0\u4f9b\u4e86\u52a8\u6001\u5de5\u5177\u53d1\u73b0\u548c\u8c03\u7528\u7684\u6807\u51c6\uff0c\u4f46\u6784\u5efaMCP\u670d\u52a1\u5668\u4ecd\u9700\u5927\u91cf\u624b\u52a8\u5de5\u4f5c\uff0c\u5305\u62ec\u7f16\u5199\u80f6\u6c34\u4ee3\u7801\u3001\u5904\u7406\u8ba4\u8bc1\u548c\u624b\u52a8\u914d\u7f6e\u6a21\u5f0f\uff0c\u8fd9\u4e0eMCP\u65e8\u5728\u6d88\u9664\u96c6\u6210\u5de5\u4f5c\u7684\u76ee\u6807\u76f8\u77db\u76fe\u3002", "method": "\u7814\u7a76\u56e2\u961f\u9996\u5148\u5206\u6790\u4e86MCP\u7684\u91c7\u7528\u8d8b\u52bf\uff0c\u53d1\u73b022000+\u4e2aMCP\u6807\u8bb0\u7684GitHub\u4ed3\u5e93\u4e2d\u53ea\u6709\u4e0d\u52305%\u5305\u542b\u670d\u52a1\u5668\u3002\u968f\u540e\u63d0\u51faAutoMCP\u7f16\u8bd1\u5668\uff0c\u8be5\u7f16\u8bd1\u5668\u80fd\u591f\u89e3\u6790OpenAPI 2.0/3.0\u89c4\u8303\u4e2d\u7684REST API\u5b9a\u4e49\uff0c\u81ea\u52a8\u751f\u6210\u5b8c\u6574\u7684MCP\u670d\u52a1\u5668\u5b9e\u73b0\uff0c\u5305\u62ec\u6a21\u5f0f\u6ce8\u518c\u548c\u8ba4\u8bc1\u5904\u7406\u3002", "result": "\u572850\u4e2a\u771f\u5b9e\u4e16\u754cAPI\u76845066\u4e2a\u7aef\u70b9\u6d4b\u8bd5\u4e2d\uff0c\u4ece1023\u4e2a\u5206\u5c42\u62bd\u6837\u7684\u5de5\u5177\u8c03\u7528\u4e2d\uff0c76.5%\u5f00\u7bb1\u5373\u7528\u6210\u529f\u3002\u901a\u8fc7\u624b\u52a8\u6545\u969c\u5206\u6790\u53d1\u73b05\u4e2a\u91cd\u590d\u51fa\u73b0\u7684\u95ee\u9898\uff0c\u90fd\u5f52\u56e0\u4e8eOpenAPI\u5408\u7ea6\u7684\u4e0d\u4e00\u81f4\u6216\u9057\u6f0f\u3002\u7ecf\u8fc7\u5e73\u5747\u6bcf\u4e2aAPI 19\u884c\u89c4\u8303\u4fee\u6539\u7684\u8f7b\u5fae\u4fee\u590d\u540e\uff0cAutoMCP\u8fbe\u5230\u4e8699.9%\u7684\u6210\u529f\u7387\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86\u5c3d\u7ba1OpenAPI\u89c4\u8303\u5b58\u5728\u8d28\u91cf\u95ee\u9898\uff0c\u4f46\u4ecd\u80fd\u5b9e\u73b0\u8fd1\u4e4e\u5b8c\u5168\u7684MCP\u670d\u52a1\u5668\u81ea\u52a8\u5316\u3002\u8be5\u5de5\u4f5c\u4e0d\u4ec5\u5206\u6790\u4e86MCP\u91c7\u7528\u60c5\u51b5\u5e76\u91cf\u5316\u4e86\u624b\u52a8\u670d\u52a1\u5668\u5f00\u53d1\u6210\u672c\uff0c\u8fd8\u8d21\u732e\u4e86\u5305\u542b5066\u4e2a\u53ef\u8c03\u7528\u5de5\u5177\u7684\u8bed\u6599\u5e93\u4ee5\u53ca\u4fee\u590d\u5e38\u89c1\u89c4\u8303\u7f3a\u9677\u7684\u89c1\u89e3\uff0c\u4e3aLLM\u5de5\u5177\u96c6\u6210\u7684\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.16063", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16063", "abs": "https://arxiv.org/abs/2507.16063", "authors": ["Yousab Grees", "Polina Iaremchuk", "Ramtin Ehsani", "Esteban Parra", "Preetha Chatterjee", "Sonia Haiduc"], "title": "AI-Powered Commit Explorer (APCE)", "comment": null, "summary": "Commit messages in a version control system provide valuable information for\ndevelopers regarding code changes in software systems. Commit messages can be\nthe only source of information left for future developers describing what was\nchanged and why. However, writing high-quality commit messages is often\nneglected in practice. Large Language Model (LLM) generated commit messages\nhave emerged as a way to mitigate this issue. We introduce the AI-Powered\nCommit Explorer (APCE), a tool to support developers and researchers in the use\nand study of LLM-generated commit messages. APCE gives researchers the option\nto store different prompts for LLMs and provides an additional evaluation\nprompt that can further enhance the commit message provided by LLMs. APCE also\nprovides researchers with a straightforward mechanism for automated and human\nevaluation of LLM-generated messages. Demo link https://youtu.be/zYrJ9s6sZvo", "AI": {"tldr": "\u7814\u7a76\u8005\u5f00\u53d1\u4e86AI-Powered Commit Explorer (APCE)\u5de5\u5177\uff0c\u7528\u4e8e\u652f\u6301\u5f00\u53d1\u8005\u548c\u7814\u7a76\u4eba\u5458\u4f7f\u7528\u548c\u7814\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u4ee3\u7801\u63d0\u4ea4\u4fe1\u606f\uff0c\u8be5\u5de5\u5177\u63d0\u4f9b\u4e86\u63d0\u793a\u5b58\u50a8\u3001\u8bc4\u4f30\u589e\u5f3a\u548c\u81ea\u52a8\u5316\u8bc4\u4f30\u529f\u80fd\u3002", "motivation": "\u5728\u7248\u672c\u63a7\u5236\u7cfb\u7edf\u4e2d\uff0c\u63d0\u4ea4\u4fe1\u606f\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u5173\u4e8e\u4ee3\u7801\u53d8\u66f4\u7684\u5b9d\u8d35\u4fe1\u606f\uff0c\u4f46\u5728\u5b9e\u8df5\u4e2d\u9ad8\u8d28\u91cf\u63d0\u4ea4\u4fe1\u606f\u7684\u7f16\u5199\u7ecf\u5e38\u88ab\u5ffd\u89c6\u3002\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u63d0\u4ea4\u4fe1\u606f\u53ef\u4ee5\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f46\u9700\u8981\u4e13\u95e8\u7684\u5de5\u5177\u6765\u652f\u6301\u5176\u4f7f\u7528\u548c\u7814\u7a76\u3002", "method": "\u5f00\u53d1\u4e86AI-Powered Commit Explorer (APCE)\u5de5\u5177\uff0c\u8be5\u5de5\u5177\u5177\u6709\u4ee5\u4e0b\u529f\u80fd\uff1a1) \u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u50a8\u4e0d\u540c\u7684\u63d0\u793a\uff1b2) \u63d0\u4f9b\u989d\u5916\u7684\u8bc4\u4f30\u63d0\u793a\u6765\u8fdb\u4e00\u6b65\u589e\u5f3aLLM\u751f\u6210\u7684\u63d0\u4ea4\u4fe1\u606f\uff1b3) \u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u81ea\u52a8\u5316\u548c\u4eba\u5de5\u8bc4\u4f30LLM\u751f\u6210\u4fe1\u606f\u7684\u76f4\u63a5\u673a\u5236\u3002", "result": "\u6210\u529f\u5f00\u53d1\u4e86APCE\u5de5\u5177\uff0c\u4e3a\u5f00\u53d1\u8005\u548c\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u5e73\u53f0\u6765\u4f7f\u7528\u548c\u7814\u7a76LLM\u751f\u6210\u7684\u63d0\u4ea4\u4fe1\u606f\uff0c\u5e76\u63d0\u4f9b\u4e86\u6f14\u793a\u94fe\u63a5\u5c55\u793a\u5de5\u5177\u529f\u80fd\u3002", "conclusion": "APCE\u5de5\u5177\u4e3aLLM\u751f\u6210\u63d0\u4ea4\u4fe1\u606f\u7684\u4f7f\u7528\u548c\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u6548\u652f\u6301\uff0c\u901a\u8fc7\u63d0\u793a\u7ba1\u7406\u3001\u8bc4\u4f30\u589e\u5f3a\u548c\u81ea\u52a8\u5316\u8bc4\u4f30\u529f\u80fd\uff0c\u5e2e\u52a9\u6539\u5584\u4ee3\u7801\u63d0\u4ea4\u4fe1\u606f\u7684\u8d28\u91cf\u548c\u7814\u7a76\u6548\u7387\u3002"}}
{"id": "2507.16166", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.16166", "abs": "https://arxiv.org/abs/2507.16166", "authors": ["Nasir U. Eisty", "David E. Bernholdt", "Alex Koufos", "David J. Luet", "Miranda Mundt"], "title": "Ten Essential Guidelines for Building High-Quality Research Software", "comment": null, "summary": "High-quality research software is a cornerstone of modern scientific\nprogress, enabling researchers to analyze complex data, simulate phenomena, and\nshare reproducible results. However, creating such software requires adherence\nto best practices that ensure robustness, usability, and sustainability. This\npaper presents ten guidelines for producing high-quality research software,\ncovering every stage of the development lifecycle. These guidelines emphasize\nthe importance of planning, writing clean and readable code, using version\ncontrol, and implementing thorough testing strategies. Additionally, they\naddress key principles such as modular design, reproducibility, performance\noptimization, and long-term maintenance. The paper also highlights the role of\ndocumentation and community engagement in enhancing software usability and\nimpact. By following these guidelines, researchers can create software that\nadvances their scientific objectives and contributes to a broader ecosystem of\nreliable and reusable research tools. This work serves as a practical resource\nfor researchers and developers aiming to elevate the quality and impact of\ntheir research software.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u5f00\u53d1\u9ad8\u8d28\u91cf\u79d1\u7814\u8f6f\u4ef6\u7684\u5341\u4e2a\u6307\u5bfc\u539f\u5219\uff0c\u6db5\u76d6\u8f6f\u4ef6\u5f00\u53d1\u5168\u751f\u547d\u5468\u671f\uff0c\u65e8\u5728\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u521b\u5efa\u7a33\u5065\u3001\u53ef\u7528\u4e14\u53ef\u6301\u7eed\u7684\u79d1\u7814\u5de5\u5177\u3002", "motivation": "\u73b0\u4ee3\u79d1\u5b66\u7814\u7a76\u9ad8\u5ea6\u4f9d\u8d56\u9ad8\u8d28\u91cf\u7684\u7814\u7a76\u8f6f\u4ef6\u6765\u5206\u6790\u590d\u6742\u6570\u636e\u3001\u6a21\u62df\u73b0\u8c61\u548c\u5206\u4eab\u53ef\u91cd\u73b0\u7684\u7ed3\u679c\uff0c\u4f46\u521b\u5efa\u6b64\u7c7b\u8f6f\u4ef6\u9700\u8981\u9075\u5faa\u786e\u4fdd\u7a33\u5065\u6027\u3001\u53ef\u7528\u6027\u548c\u53ef\u6301\u7eed\u6027\u7684\u6700\u4f73\u5b9e\u8df5\uff0c\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u6027\u7684\u6307\u5bfc\u539f\u5219\u3002", "method": "\u63d0\u51fa\u6db5\u76d6\u8f6f\u4ef6\u5f00\u53d1\u5168\u751f\u547d\u5468\u671f\u7684\u5341\u4e2a\u6307\u5bfc\u539f\u5219\uff0c\u5305\u62ec\u89c4\u5212\u3001\u7f16\u5199\u6e05\u6d01\u53ef\u8bfb\u7684\u4ee3\u7801\u3001\u4f7f\u7528\u7248\u672c\u63a7\u5236\u3001\u5b9e\u65bd\u5168\u9762\u7684\u6d4b\u8bd5\u7b56\u7565\u3001\u6a21\u5757\u5316\u8bbe\u8ba1\u3001\u53ef\u91cd\u73b0\u6027\u3001\u6027\u80fd\u4f18\u5316\u3001\u957f\u671f\u7ef4\u62a4\u3001\u6587\u6863\u7f16\u5199\u548c\u793e\u533a\u53c2\u4e0e\u7b49\u65b9\u9762\u3002", "result": "\u5f62\u6210\u4e86\u4e00\u5957\u5b8c\u6574\u7684\u79d1\u7814\u8f6f\u4ef6\u5f00\u53d1\u6307\u5bfc\u6846\u67b6\uff0c\u6db5\u76d6\u4e86\u4ece\u9879\u76ee\u89c4\u5212\u5230\u957f\u671f\u7ef4\u62a4\u7684\u5404\u4e2a\u5173\u952e\u73af\u8282\uff0c\u7279\u522b\u5f3a\u8c03\u4e86\u6587\u6863\u7f16\u5199\u548c\u793e\u533a\u53c2\u4e0e\u5728\u63d0\u5347\u8f6f\u4ef6\u53ef\u7528\u6027\u548c\u5f71\u54cd\u529b\u65b9\u9762\u7684\u91cd\u8981\u4f5c\u7528\u3002", "conclusion": "\u901a\u8fc7\u9075\u5faa\u8fd9\u4e9b\u6307\u5bfc\u539f\u5219\uff0c\u7814\u7a76\u4eba\u5458\u80fd\u591f\u521b\u5efa\u65e2\u63a8\u8fdb\u5176\u79d1\u5b66\u76ee\u6807\u53c8\u4e3a\u53ef\u9760\u3001\u53ef\u91cd\u7528\u7814\u7a76\u5de5\u5177\u7684\u66f4\u5e7f\u6cdb\u751f\u6001\u7cfb\u7edf\u505a\u51fa\u8d21\u732e\u7684\u8f6f\u4ef6\uff0c\u4e3a\u63d0\u5347\u79d1\u7814\u8f6f\u4ef6\u8d28\u91cf\u548c\u5f71\u54cd\u529b\u63d0\u4f9b\u4e86\u5b9e\u7528\u8d44\u6e90\u3002"}}
{"id": "2507.16208", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16208", "abs": "https://arxiv.org/abs/2507.16208", "authors": ["Sohaib Muhammad", "Ashwati Vipin", "Karan Shetti", "Honey Mittal"], "title": "LOCOFY Large Design Models -- Design to code conversion solution", "comment": null, "summary": "Despite rapid advances in Large Language Models and Multimodal Large Language\nModels (LLMs), numerous challenges related to interpretability, scalability,\nresource requirements and repeatability remain, related to their application in\nthe design-to-code space. To address this, we introduce the Large Design Models\n(LDMs) paradigm specifically trained on designs and webpages to enable seamless\nconversion from design-to-code. We have developed a training and inference\npipeline by incorporating data engineering and appropriate model architecture\nmodification. The training pipeline consists of the following: 1)Design\nOptimiser: developed using a proprietary ground truth dataset and addresses\nsub-optimal designs; 2)Tagging and feature detection: using pre-trained and\nfine-tuned models, this enables the accurate detection and classification of UI\nelements; and 3)Auto Components: extracts repeated UI structures into reusable\ncomponents to enable creation of modular code, thus reducing redundancy while\nenhancing code reusability. In this manner, each model addresses distinct but\nkey issues for design-to-code conversion. Separately, our inference pipeline\nprocesses real-world designs to produce precise and interpretable instructions\nfor code generation and ensures reliability. Additionally, our models\nillustrated exceptional end-to-end design-to-code conversion accuracy using a\nnovel preview match score metric. Comparative experiments indicated superior\nperformance of LDMs against LLMs on accuracy of node positioning,\nresponsiveness and reproducibility. Moreover, our custom-trained tagging and\nfeature detection model demonstrated high precision and consistency in\nidentifying UI elements across a wide sample of test designs. Thus, our\nproposed LDMs are a reliable and superior solution to understanding designs\nthat subsequently enable the generation of efficient and reliable\nproduction-ready code.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u5927\u578b\u8bbe\u8ba1\u6a21\u578b(LDMs)\u8303\u5f0f\uff0c\u4e13\u95e8\u9488\u5bf9\u8bbe\u8ba1\u5230\u4ee3\u7801\u8f6c\u6362\u8fdb\u884c\u8bad\u7ec3\uff0c\u901a\u8fc7\u8bbe\u8ba1\u4f18\u5316\u5668\u3001\u6807\u8bb0\u68c0\u6d4b\u548c\u81ea\u52a8\u7ec4\u4ef6\u63d0\u53d6\u4e09\u4e2a\u6a21\u5757\uff0c\u5b9e\u73b0\u4e86\u6bd4\u4f20\u7edf\u5927\u8bed\u8a00\u6a21\u578b\u66f4\u51c6\u786e\u3001\u53ef\u89e3\u91ca\u548c\u53ef\u91cd\u590d\u7684\u8bbe\u8ba1\u5230\u4ee3\u7801\u8f6c\u6362\u3002", "motivation": "\u4f20\u7edf\u7684\u5927\u8bed\u8a00\u6a21\u578b\u548c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bbe\u8ba1\u5230\u4ee3\u7801\u8f6c\u6362\u5e94\u7528\u4e2d\u5b58\u5728\u53ef\u89e3\u91ca\u6027\u5dee\u3001\u6269\u5c55\u6027\u4e0d\u8db3\u3001\u8d44\u6e90\u9700\u6c42\u9ad8\u548c\u53ef\u91cd\u590d\u6027\u4f4e\u7b49\u6311\u6218\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u8bbe\u8ba1\u9886\u57df\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u4e86\u4e13\u95e8\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u7ba1\u9053\uff0c\u5305\u62ec\u4e09\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a1)\u8bbe\u8ba1\u4f18\u5316\u5668\uff1a\u4f7f\u7528\u4e13\u6709\u771f\u5b9e\u6570\u636e\u96c6\u5904\u7406\u6b21\u4f18\u8bbe\u8ba1\uff1b2)\u6807\u8bb0\u548c\u7279\u5f81\u68c0\u6d4b\uff1a\u4f7f\u7528\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u6a21\u578b\u51c6\u786e\u68c0\u6d4b\u548c\u5206\u7c7bUI\u5143\u7d20\uff1b3)\u81ea\u52a8\u7ec4\u4ef6\uff1a\u63d0\u53d6\u91cd\u590d\u7684UI\u7ed3\u6784\u4e3a\u53ef\u91cd\u7528\u7ec4\u4ef6\uff0c\u751f\u6210\u6a21\u5757\u5316\u4ee3\u7801\u3002", "result": "LDMs\u5728\u7aef\u5230\u7aef\u8bbe\u8ba1\u5230\u4ee3\u7801\u8f6c\u6362\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f7f\u7528\u65b0\u9896\u7684\u9884\u89c8\u5339\u914d\u5206\u6570\u6307\u6807\u9a8c\u8bc1\u3002\u4e0e\u4f20\u7edfLLMs\u76f8\u6bd4\uff0c\u5728\u8282\u70b9\u5b9a\u4f4d\u51c6\u786e\u6027\u3001\u54cd\u5e94\u6027\u548c\u53ef\u91cd\u590d\u6027\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002\u5b9a\u5236\u8bad\u7ec3\u7684\u6807\u8bb0\u548c\u7279\u5f81\u68c0\u6d4b\u6a21\u578b\u5728\u8bc6\u522bUI\u5143\u7d20\u65b9\u9762\u5c55\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u4e00\u81f4\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u5927\u578b\u8bbe\u8ba1\u6a21\u578b(LDMs)\u662f\u7406\u89e3\u8bbe\u8ba1\u5e76\u751f\u6210\u9ad8\u6548\u3001\u53ef\u9760\u7684\u751f\u4ea7\u5c31\u7eea\u4ee3\u7801\u7684\u4f18\u8d8a\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u8bbe\u8ba1\u5230\u4ee3\u7801\u8f6c\u6362\u4e2d\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2507.16327", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.16327", "abs": "https://arxiv.org/abs/2507.16327", "authors": ["Karoline Nyl\u00e6nder", "Aitor Arrieta", "Shaukat Ali", "Paolo Arcaini"], "title": "Search-based Generation of Waypoints for Triggering Self-Adaptations in Maritime Autonomous Vessels", "comment": "9 pages, 3 figures. Accepted at GECCO 2025 (Genetic and Evolutionary\n  Computation Conference), July 14-18, 2025, Malaga, Spain", "summary": "Self-adaptation in maritime autonomous vessels (AVs) enables them to adapt\ntheir behaviors to address unexpected situations while maintaining\ndependability requirements. During the design of such AVs, it is crucial to\nunderstand and identify the settings that should trigger adaptations, enabling\nvalidation of their implementation. To this end, we focus on the navigation\nsoftware of AVs, which must adapt their behavior during operation through\nadaptations. AVs often rely on predefined waypoints to guide them along\ndesignated routes, ensuring safe navigation. We propose a multiobjective\nsearch-based approach, called WPgen, to generate minor modifications to the\npredefined set of waypoints, keeping them as close as possible to the original\nwaypoints, while causing the AV to navigate inappropriately when navigating\nwith the generated waypoints. WPgen uses NSGA-II as the multi-objective search\nalgorithm with three seeding strategies for its initial population, resulting\nin three variations of WPgen. We evaluated these variations on three AVs (one\noverwater tanker and two underwater). We compared the three variations of WPgen\nwith Random Search as the baseline and with each other. Experimental results\nshowed that the effectiveness of these variations varied depending on the AV.\nBased on the results, we present the research and practical implications of\nWPgen.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86WPgen\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u76ee\u6807\u641c\u7d22\u7b97\u6cd5\u751f\u6210\u5fae\u5c0f\u4fee\u6539\u7684\u822a\u8def\u70b9\u6765\u6d4b\u8bd5\u6d77\u4e8b\u81ea\u4e3b\u8239\u8236\u7684\u81ea\u9002\u5e94\u5bfc\u822a\u7cfb\u7edf\uff0c\u4ee5\u9a8c\u8bc1\u5176\u5728\u5f02\u5e38\u60c5\u51b5\u4e0b\u7684\u9002\u5e94\u80fd\u529b", "motivation": "\u6d77\u4e8b\u81ea\u4e3b\u8239\u8236\u9700\u8981\u5728\u610f\u5916\u60c5\u51b5\u4e0b\u81ea\u9002\u5e94\u8c03\u6574\u884c\u4e3a\u4ee5\u7ef4\u6301\u53ef\u9760\u6027\u8981\u6c42\uff0c\u5728\u8bbe\u8ba1\u9636\u6bb5\u9700\u8981\u7406\u89e3\u548c\u8bc6\u522b\u89e6\u53d1\u81ea\u9002\u5e94\u7684\u8bbe\u7f6e\u6761\u4ef6\uff0c\u4ee5\u4fbf\u9a8c\u8bc1\u5176\u5b9e\u73b0\u7684\u6709\u6548\u6027", "method": "\u63d0\u51faWPgen\u591a\u76ee\u6807\u641c\u7d22\u65b9\u6cd5\uff0c\u4f7f\u7528NSGA-II\u7b97\u6cd5\u5bf9\u9884\u5b9a\u4e49\u822a\u8def\u70b9\u8fdb\u884c\u5fae\u5c0f\u4fee\u6539\uff0c\u4fdd\u6301\u5c3d\u53ef\u80fd\u63a5\u8fd1\u539f\u59cb\u822a\u8def\u70b9\u7684\u540c\u65f6\u5bfc\u81f4\u8239\u8236\u5bfc\u822a\u4e0d\u5f53\u3002\u91c7\u7528\u4e09\u79cd\u79cd\u5b50\u7b56\u7565\u751f\u6210\u521d\u59cb\u79cd\u7fa4\uff0c\u5f62\u6210\u4e09\u4e2aWPgen\u53d8\u4f53", "result": "\u5728\u4e09\u79cd\u81ea\u4e3b\u8239\u8236\uff08\u4e00\u8258\u6c34\u9762\u6cb9\u8f6e\u548c\u4e24\u8258\u6c34\u4e0b\u8239\u8236\uff09\u4e0a\u8bc4\u4f30\u4e86WPgen\u7684\u4e09\u4e2a\u53d8\u4f53\uff0c\u4e0e\u968f\u673a\u641c\u7d22\u57fa\u7ebf\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u4e0d\u540c\u53d8\u4f53\u7684\u6709\u6548\u6027\u56e0\u8239\u8236\u7c7b\u578b\u800c\u5f02", "conclusion": "\u57fa\u4e8e\u5b9e\u9a8c\u7ed3\u679c\uff0c\u8bba\u6587\u63d0\u51fa\u4e86WPgen\u65b9\u6cd5\u7684\u7814\u7a76\u548c\u5b9e\u8df5\u610f\u4e49\uff0c\u4e3a\u6d77\u4e8b\u81ea\u4e3b\u8239\u8236\u81ea\u9002\u5e94\u5bfc\u822a\u7cfb\u7edf\u7684\u8bbe\u8ba1\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6d4b\u8bd5\u65b9\u6cd5"}}
{"id": "2507.16407", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.16407", "abs": "https://arxiv.org/abs/2507.16407", "authors": ["Shuhan Liu", "Xing Hu", "Kerui Huang", "Xiaohu Yang", "David Lo", "Xin Xia"], "title": "Improving Code LLM Robustness to Prompt Perturbations via Layer-Aware Model Editing", "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\ncode generation, where the natural language prompt plays a crucial role in\nconveying user intent to the model. However, prior studies have shown that LLMs\nare highly sensitive to prompt perturbations. Minor modifications in wording,\nsyntax, or formatting can significantly reduce the functional correctness of\ngenerated code. As perturbations frequently occur in real-world scenarios,\nimproving the robustness of LLMs to prompt perturbations is essential for\nensuring reliable performance in practical code generation. In this paper, we\nintroduce CREME (Code Robustness Enhancement via Model Editing), a novel\napproach that enhances LLM robustness through targeted parameter updates. CREME\nfirst identifies robustness-sensitive layers by comparing hidden states between\nan original prompt and its perturbed variant. Then, it performs lightweight\nparameter editing at the identified layer to reduce performance degradation. We\nevaluate CREME on two widely used code generation benchmarks (HumanEval and\nMBPP) along with their perturbed counterparts. Experimental results show that\nCREME improves Pass@1 accuracy by 63% on perturbed prompts while maintaining\nstable performance on clean inputs, with accuracy deviations within 1%. Further\nanalysis reveals that robustness-sensitive layers are primarily concentrated in\nthe middle and deeper layers of the network, and their locations vary across\ndifferent model architectures. These insights provide a valuable foundation for\ndeveloping future robustness-oriented editing strategies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCREME\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u578b\u7f16\u8f91\u6280\u672f\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u5bf9\u63d0\u793a\u6270\u52a8\u7684\u9c81\u68d2\u6027\uff0c\u5728\u6270\u52a8\u63d0\u793a\u4e0a\u5c06Pass@1\u51c6\u786e\u7387\u63d0\u534763%\uff0c\u540c\u65f6\u4fdd\u6301\u539f\u59cb\u8f93\u5165\u7684\u7a33\u5b9a\u6027\u80fd\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u5bf9\u63d0\u793a\u6270\u52a8\u9ad8\u5ea6\u654f\u611f\uff0c\u5fae\u5c0f\u7684\u63aa\u8f9e\u3001\u8bed\u6cd5\u6216\u683c\u5f0f\u4fee\u6539\u90fd\u4f1a\u663e\u8457\u964d\u4f4e\u751f\u6210\u4ee3\u7801\u7684\u529f\u80fd\u6b63\u786e\u6027\u3002\u7531\u4e8e\u6270\u52a8\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u9891\u7e41\u51fa\u73b0\uff0c\u63d0\u5347LLMs\u5bf9\u63d0\u793a\u6270\u52a8\u7684\u9c81\u68d2\u6027\u5bf9\u786e\u4fdd\u5b9e\u9645\u4ee3\u7801\u751f\u6210\u7684\u53ef\u9760\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002", "method": "CREME\uff08\u901a\u8fc7\u6a21\u578b\u7f16\u8f91\u589e\u5f3a\u4ee3\u7801\u9c81\u68d2\u6027\uff09\u9996\u5148\u901a\u8fc7\u6bd4\u8f83\u539f\u59cb\u63d0\u793a\u548c\u6270\u52a8\u53d8\u4f53\u4e4b\u95f4\u7684\u9690\u85cf\u72b6\u6001\u6765\u8bc6\u522b\u9c81\u68d2\u6027\u654f\u611f\u5c42\uff0c\u7136\u540e\u5728\u8bc6\u522b\u51fa\u7684\u5c42\u4e0a\u8fdb\u884c\u8f7b\u91cf\u7ea7\u53c2\u6570\u7f16\u8f91\u4ee5\u51cf\u5c11\u6027\u80fd\u4e0b\u964d\u3002", "result": "\u5728HumanEval\u548cMBPP\u4e24\u4e2a\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u53ca\u5176\u6270\u52a8\u7248\u672c\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCREME\u5728\u6270\u52a8\u63d0\u793a\u4e0a\u5c06Pass@1\u51c6\u786e\u7387\u63d0\u5347\u4e8663%\uff0c\u540c\u65f6\u5728\u5e72\u51c0\u8f93\u5165\u4e0a\u4fdd\u6301\u7a33\u5b9a\u6027\u80fd\uff0c\u51c6\u786e\u7387\u504f\u5dee\u57281%\u4ee5\u5185\u3002\u5206\u6790\u53d1\u73b0\u9c81\u68d2\u6027\u654f\u611f\u5c42\u4e3b\u8981\u96c6\u4e2d\u5728\u7f51\u7edc\u7684\u4e2d\u95f4\u5c42\u548c\u6df1\u5c42\uff0c\u4e14\u5176\u4f4d\u7f6e\u5728\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u95f4\u6709\u6240\u53d8\u5316\u3002", "conclusion": "CREME\u6709\u6548\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u63d0\u793a\u6270\u52a8\u7684\u9c81\u68d2\u6027\uff0c\u5728\u663e\u8457\u6539\u5584\u6270\u52a8\u8f93\u5165\u6027\u80fd\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u539f\u59cb\u8f93\u5165\u7684\u7a33\u5b9a\u8868\u73b0\u3002\u7814\u7a76\u63ed\u793a\u7684\u9c81\u68d2\u6027\u654f\u611f\u5c42\u5206\u5e03\u89c4\u5f8b\u4e3a\u672a\u6765\u5f00\u53d1\u9762\u5411\u9c81\u68d2\u6027\u7684\u7f16\u8f91\u7b56\u7565\u63d0\u4f9b\u4e86\u5b9d\u8d35\u57fa\u7840\u3002"}}
{"id": "2507.16439", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.16439", "abs": "https://arxiv.org/abs/2507.16439", "authors": ["Gunnar Larsen", "Carol Wong", "Anthony Peruma"], "title": "Exploring Large Language Models for Analyzing and Improving Method Names in Scientific Code", "comment": "The 19th ACM/IEEE International Symposium on Empirical Software\n  Engineering and Measurement - Emerging Results and Vision Track", "summary": "Research scientists increasingly rely on implementing software to support\ntheir research. While previous research has examined the impact of identifier\nnames on program comprehension in traditional programming environments, limited\nwork has explored this area in scientific software, especially regarding the\nquality of method names in the code. The recent advances in Large Language\nModels (LLMs) present new opportunities for automating code analysis tasks,\nsuch as identifier name appraisals and recommendations. Our study evaluates\nfour popular LLMs on their ability to analyze grammatical patterns and suggest\nimprovements for 496 method names extracted from Python-based Jupyter\nNotebooks. Our findings show that the LLMs are somewhat effective in analyzing\nthese method names and generally follow good naming practices, like starting\nmethod names with verbs. However, their inconsistent handling of\ndomain-specific terminology and only moderate agreement with human annotations\nindicate that automated suggestions require human evaluation. This work\nprovides foundational insights for improving the quality of scientific code\nthrough AI automation.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u56db\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5206\u6790\u548c\u6539\u8fdb\u79d1\u5b66\u8f6f\u4ef6\u4e2d\u65b9\u6cd5\u540d\u8d28\u91cf\u65b9\u9762\u7684\u80fd\u529b\uff0c\u53d1\u73b0LLMs\u5728\u9075\u5faa\u826f\u597d\u547d\u540d\u5b9e\u8df5\u65b9\u9762\u8868\u73b0\u5c1a\u53ef\uff0c\u4f46\u5728\u5904\u7406\u9886\u57df\u7279\u5b9a\u672f\u8bed\u65f6\u5b58\u5728\u4e0d\u4e00\u81f4\u6027\uff0c\u9700\u8981\u4eba\u5de5\u8bc4\u4f30", "motivation": "\u968f\u7740\u7814\u7a76\u79d1\u5b66\u5bb6\u8d8a\u6765\u8d8a\u4f9d\u8d56\u8f6f\u4ef6\u652f\u6301\u7814\u7a76\uff0c\u800c\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u4f20\u7edf\u7f16\u7a0b\u73af\u5883\u4e2d\u6807\u8bc6\u7b26\u540d\u79f0\u5bf9\u7a0b\u5e8f\u7406\u89e3\u7684\u5f71\u54cd\uff0c\u5bf9\u79d1\u5b66\u8f6f\u4ef6\u4e2d\u65b9\u6cd5\u540d\u8d28\u91cf\u7684\u7814\u7a76\u6709\u9650\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8fdb\u6b65\u4e3a\u81ea\u52a8\u5316\u4ee3\u7801\u5206\u6790\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a", "method": "\u9009\u62e9\u56db\u4e2a\u6d41\u884c\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5bf9\u4ecePython Jupyter Notebooks\u4e2d\u63d0\u53d6\u7684496\u4e2a\u65b9\u6cd5\u540d\u8fdb\u884c\u8bed\u6cd5\u6a21\u5f0f\u5206\u6790\u548c\u6539\u8fdb\u5efa\u8bae\u8bc4\u4f30\uff0c\u5e76\u4e0e\u4eba\u5de5\u6807\u6ce8\u8fdb\u884c\u5bf9\u6bd4", "result": "LLMs\u5728\u5206\u6790\u65b9\u6cd5\u540d\u65b9\u9762\u5177\u6709\u4e00\u5b9a\u6548\u679c\uff0c\u901a\u5e38\u9075\u5faa\u826f\u597d\u7684\u547d\u540d\u5b9e\u8df5\uff08\u5982\u65b9\u6cd5\u540d\u4ee5\u52a8\u8bcd\u5f00\u5934\uff09\uff0c\u4f46\u5728\u5904\u7406\u9886\u57df\u7279\u5b9a\u672f\u8bed\u65f6\u8868\u73b0\u4e0d\u4e00\u81f4\uff0c\u4e0e\u4eba\u5de5\u6807\u6ce8\u7684\u4e00\u81f4\u6027\u4e2d\u7b49", "conclusion": "\u81ea\u52a8\u5316\u5efa\u8bae\u9700\u8981\u4eba\u5de5\u8bc4\u4f30\uff0c\u672c\u7814\u7a76\u4e3a\u901a\u8fc7AI\u81ea\u52a8\u5316\u63d0\u9ad8\u79d1\u5b66\u4ee3\u7801\u8d28\u91cf\u63d0\u4f9b\u4e86\u57fa\u7840\u6027\u89c1\u89e3"}}
{"id": "2507.16587", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.16587", "abs": "https://arxiv.org/abs/2507.16587", "authors": ["Giuseppe Crupi", "Rosalia Tufano", "Alejandro Velasco", "Antonio Mastropaolo", "Denys Poshyvanyk", "Gabriele Bavota"], "title": "On the Effectiveness of LLM-as-a-judge for Code Generation and Summarization", "comment": "Accepted at TSE. IEEE Transactions on Software Engineering", "summary": "Large Language Models have been recently exploited as judges for complex\nnatural language processing tasks, such as Q&A. The basic idea is to delegate\nto an LLM the assessment of the \"quality\" of the output provided by an\nautomated technique for tasks for which: (i) quantitative metrics would only\ntell part of the story, and; (ii) a large-scale human-based evaluation would be\ntoo expensive. LLMs-as-a-judge, if proven effective for a specific task, can\nalso unlock new possibilities for automation, with several LLMs proposing a\nsolution for a given instance of the task and others judging and deciding what\nis the best output to show the user. We study the effectiveness of\nLLMs-as-a-judge for two code-related tasks, namely code generation and code\nsummarization. The rationale for choosing these tasks is two-fold. First,\nquantitative metrics are usually not enough for the assessment of code\nsummarizers/generators. For example, it is well documented that metrics such as\nBLEU are quite weak proxies for the quality of the generated summaries. Second,\neven state-of-the-art techniques still struggle with handling complex instances\nof these tasks, making them good candidates for benefiting from more advanced\nsolutions envisioning collaboration among LLMs. For code generation, we check\nwhether eight LLMs are able to judge the correctness of 1,405 Java methods and\n1,281 Python functions generated by the same LLMs or implemented by humans. For\ncode summarization, we compare the judgment of five LLMs to those provided by\nnine humans for ~1.2k summaries, related to both Java and Python functions. Our\nfindings show that GPT-4-turbo is the best LLM in terms of judging capabilities\nfor both tasks, with \"smaller\" LLMs featuring tens of billions parameters not\nbeing able to cope with judging tasks. However, even the best-performing LLM\nfrequently misjudges the correctness of the code and summary quality.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u8bc4\u5224\u8005\u5728\u4ee3\u7801\u751f\u6210\u548c\u4ee3\u7801\u6458\u8981\u4efb\u52a1\u4e2d\u7684\u6548\u679c\uff0c\u53d1\u73b0GPT-4-turbo\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u5373\u4f7f\u662f\u6700\u597d\u7684\u6a21\u578b\u4e5f\u7ecf\u5e38\u51fa\u73b0\u8bef\u5224\u3002", "motivation": "\u4f20\u7edf\u7684\u5b9a\u91cf\u6307\u6807\uff08\u5982BLEU\uff09\u65e0\u6cd5\u5145\u5206\u8bc4\u4f30\u4ee3\u7801\u751f\u6210\u548c\u6458\u8981\u7684\u8d28\u91cf\uff0c\u800c\u5927\u89c4\u6a21\u4eba\u5de5\u8bc4\u4f30\u6210\u672c\u8fc7\u9ad8\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u8bc4\u5224\u8005\u7684\u53ef\u884c\u6027\uff0c\u4ee5\u5b9e\u73b0\u81ea\u52a8\u5316\u8bc4\u4f30\u5e76\u652f\u6301\u591a\u6a21\u578b\u534f\u4f5c\u3002", "method": "\u9009\u62e9\u4ee3\u7801\u751f\u6210\u548c\u4ee3\u7801\u6458\u8981\u4e24\u4e2a\u4efb\u52a1\u8fdb\u884c\u7814\u7a76\u3002\u5bf9\u4e8e\u4ee3\u7801\u751f\u6210\uff0c\u4f7f\u75288\u4e2aLLM\u8bc4\u52241,405\u4e2aJava\u65b9\u6cd5\u548c1,281\u4e2aPython\u51fd\u6570\u7684\u6b63\u786e\u6027\uff1b\u5bf9\u4e8e\u4ee3\u7801\u6458\u8981\uff0c\u5c065\u4e2aLLM\u7684\u8bc4\u5224\u7ed3\u679c\u4e0e9\u540d\u4eba\u7c7b\u4e13\u5bb6\u5bf9\u7ea61.2k\u4e2a\u6458\u8981\u7684\u8bc4\u5224\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "GPT-4-turbo\u5728\u4e24\u4e2a\u4efb\u52a1\u4e2d\u90fd\u8868\u73b0\u51fa\u6700\u4f73\u7684\u8bc4\u5224\u80fd\u529b\uff0c\u800c\u53c2\u6570\u91cf\u4e3a\u6570\u767e\u4ebf\u7684\"\u8f83\u5c0f\"LLM\u65e0\u6cd5\u80dc\u4efb\u8bc4\u5224\u4efb\u52a1\u3002\u7136\u800c\uff0c\u5373\u4f7f\u662f\u8868\u73b0\u6700\u597d\u7684LLM\u4e5f\u7ecf\u5e38\u5bf9\u4ee3\u7801\u6b63\u786e\u6027\u548c\u6458\u8981\u8d28\u91cf\u4ea7\u751f\u8bef\u5224\u3002", "conclusion": "\u867d\u7136GPT-4-turbo\u5728LLM\u8bc4\u5224\u8005\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u76ee\u524d\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u76f8\u5173\u4efb\u52a1\u7684\u8bc4\u5224\u4e0a\u4ecd\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\uff0c\u7ecf\u5e38\u51fa\u73b0\u8bef\u5224\uff0c\u8868\u660eLLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u7684\u6280\u672f\u8fd8\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002"}}
{"id": "2507.16661", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.16661", "abs": "https://arxiv.org/abs/2507.16661", "authors": ["Tan Bui", "Yan Naing Tun", "Thanh Phuc Nguyen", "Yindu Su", "Ferdian Thung", "Yikun Li", "Han Wei Ang", "Yide Yin", "Frank Liauw", "Lwin Khin Shar", "Eng Lieh Ouh", "Ting Zhang", "David Lo"], "title": "VulCoCo: A Simple Yet Effective Method for Detecting Vulnerable Code Clones", "comment": null, "summary": "Code reuse is common in modern software development, but it can also spread\nvulnerabilities when developers unknowingly copy risky code. The code fragments\nthat preserve the logic of known vulnerabilities are known as vulnerable code\nclones (VCCs). Detecting those VCCs is a critical but challenging task.\nExisting VCC detection tools often rely on syntactic similarity or produce\ncoarse vulnerability predictions without clear explanations, limiting their\npractical utility. In this paper, we propose VulCoCo, a lightweight and\nscalable approach that combines embedding-based retrieval with large language\nmodel (LLM) validation. Starting from a set of known vulnerable functions, we\nretrieve syntactically or semantically similar candidate functions from a large\ncorpus and use an LLM to assess whether the candidates retain the\nvulnerability. Given that there is a lack of reproducible vulnerable code clone\nbenchmarks, we first construct a synthetic benchmark that spans various clone\ntypes.\n  Our experiments on the benchmark show that VulCoCo outperforms prior\nstate-of-the-art methods in terms of Precision@k and mean average precision\n(MAP). In addition, we also demonstrate VulCoCo's effectiveness in real-world\nprojects by submitting 400 pull requests (PRs) to 284 open-source projects.\nAmong them, 75 PRs were merged, and 15 resulted in newly published CVEs. We\nalso provide insights to inspire future work to further improve the precision\nof vulnerable code clone detection.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faVulCoCo\uff0c\u4e00\u79cd\u7ed3\u5408\u5d4c\u5165\u5f0f\u68c0\u7d22\u548c\u5927\u8bed\u8a00\u6a21\u578b\u9a8c\u8bc1\u7684\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u6613\u53d7\u653b\u51fb\u7684\u4ee3\u7801\u514b\u9686(VCCs)\u3002\u8be5\u65b9\u6cd5\u5728\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5e76\u5728\u771f\u5b9e\u9879\u76ee\u4e2d\u6210\u529f\u63d0\u4ea4400\u4e2a\u62c9\u53d6\u8bf7\u6c42\uff0c\u5176\u4e2d75\u4e2a\u88ab\u5408\u5e76\uff0c15\u4e2a\u5bfc\u81f4\u65b0\u53d1\u5e03\u7684CVE\u3002", "motivation": "\u73b0\u4ee3\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u4ee3\u7801\u590d\u7528\u5f88\u5e38\u89c1\uff0c\u4f46\u5f53\u5f00\u53d1\u8005\u65e0\u610f\u4e2d\u590d\u5236\u6709\u98ce\u9669\u7684\u4ee3\u7801\u65f6\u4e5f\u4f1a\u4f20\u64ad\u6f0f\u6d1e\u3002\u73b0\u6709\u7684\u6613\u53d7\u653b\u51fb\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\u5de5\u5177\u5f80\u5f80\u4f9d\u8d56\u8bed\u6cd5\u76f8\u4f3c\u6027\u6216\u4ea7\u751f\u7c97\u7cd9\u7684\u6f0f\u6d1e\u9884\u6d4b\u4e14\u7f3a\u4e4f\u6e05\u6670\u89e3\u91ca\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002\u540c\u65f6\u7f3a\u4e4f\u53ef\u590d\u73b0\u7684\u6613\u53d7\u653b\u51fb\u4ee3\u7801\u514b\u9686\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u63d0\u51faVulCoCo\u65b9\u6cd5\uff0c\u7ed3\u5408\u5d4c\u5165\u5f0f\u68c0\u7d22\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u9a8c\u8bc1\u3002\u4ece\u5df2\u77e5\u6613\u53d7\u653b\u51fb\u51fd\u6570\u96c6\u5408\u5f00\u59cb\uff0c\u4ece\u5927\u578b\u8bed\u6599\u5e93\u4e2d\u68c0\u7d22\u8bed\u6cd5\u6216\u8bed\u4e49\u76f8\u4f3c\u7684\u5019\u9009\u51fd\u6570\uff0c\u7136\u540e\u4f7f\u7528LLM\u8bc4\u4f30\u5019\u9009\u51fd\u6570\u662f\u5426\u4fdd\u7559\u4e86\u6f0f\u6d1e\u3002\u6784\u5efa\u4e86\u4e00\u4e2a\u6db5\u76d6\u5404\u79cd\u514b\u9686\u7c7b\u578b\u7684\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVulCoCo\u5728Precision@k\u548c\u5e73\u5747\u7cbe\u5ea6\u5747\u503c(MAP)\u65b9\u9762\u4f18\u4e8e\u5148\u524d\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002\u5728\u771f\u5b9e\u4e16\u754c\u9879\u76ee\u4e2d\uff0c\u5411284\u4e2a\u5f00\u6e90\u9879\u76ee\u63d0\u4ea4\u4e86400\u4e2a\u62c9\u53d6\u8bf7\u6c42\uff0c\u5176\u4e2d75\u4e2a\u88ab\u5408\u5e76\uff0c15\u4e2a\u5bfc\u81f4\u65b0\u53d1\u5e03\u7684CVE\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "VulCoCo\u662f\u4e00\u79cd\u6709\u6548\u7684\u6613\u53d7\u653b\u51fb\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5728\u5408\u6210\u57fa\u51c6\u548c\u771f\u5b9e\u9879\u76ee\u4e2d\u90fd\u8868\u73b0\u51fa\u8272\u3002\u7814\u7a76\u8fd8\u4e3a\u672a\u6765\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6613\u53d7\u653b\u51fb\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\u7cbe\u5ea6\u7684\u5de5\u4f5c\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002\u8be5\u65b9\u6cd5\u5177\u6709\u8f7b\u91cf\u7ea7\u548c\u53ef\u6269\u5c55\u7684\u7279\u70b9\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2507.16685", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.16685", "abs": "https://arxiv.org/abs/2507.16685", "authors": ["Duong Nguyen", "Manh Tran-Duc", "Thanh Le-Cong", "Triet Huynh Minh Le", "M. Ali Babar", "Quyet-Thang Huynh"], "title": "VulGuard: An Unified Tool for Evaluating Just-In-Time Vulnerability Prediction Models", "comment": null, "summary": "We present VulGuard, an automated tool designed to streamline the extraction,\nprocessing, and analysis of commits from GitHub repositories for Just-In-Time\nvulnerability prediction (JIT-VP) research. VulGuard automatically mines commit\nhistories, extracts fine-grained code changes, commit messages, and software\nengineering metrics, and formats them for downstream analysis. In addition, it\nintegrates several state-of-the-art vulnerability prediction models, allowing\nresearchers to train, evaluate, and compare models with minimal setup. By\nsupporting both repository-scale mining and model-level experimentation within\na unified framework, VulGuard addresses key challenges in reproducibility and\nscalability in software security research. VulGuard can also be easily\nintegrated into the CI/CD pipeline. We demonstrate the effectiveness of the\ntool in two influential open-source projects, FFmpeg and the Linux kernel,\nhighlighting its potential to accelerate real-world JIT-VP research and promote\nstandardized benchmarking. A demo video is available at:\nhttps://youtu.be/j96096-pxbs", "AI": {"tldr": "VulGuard\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u5de5\u5177\uff0c\u7528\u4e8e\u4eceGitHub\u4ed3\u5e93\u4e2d\u63d0\u53d6\u3001\u5904\u7406\u548c\u5206\u6790\u63d0\u4ea4\u8bb0\u5f55\uff0c\u4ee5\u652f\u6301\u5373\u65f6\u6f0f\u6d1e\u9884\u6d4b\uff08JIT-VP\uff09\u7814\u7a76\uff0c\u96c6\u6210\u4e86\u591a\u79cd\u5148\u8fdb\u7684\u6f0f\u6d1e\u9884\u6d4b\u6a21\u578b\uff0c\u5e76\u5728FFmpeg\u548cLinux\u5185\u6838\u9879\u76ee\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u8f6f\u4ef6\u5b89\u5168\u7814\u7a76\u4e2d\u5373\u65f6\u6f0f\u6d1e\u9884\u6d4b\uff08JIT-VP\uff09\u9762\u4e34\u7684\u53ef\u91cd\u73b0\u6027\u548c\u53ef\u6269\u5c55\u6027\u6311\u6218\uff0c\u9700\u8981\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\u6765\u7b80\u5316\u4eceGitHub\u4ed3\u5e93\u4e2d\u63d0\u53d6\u548c\u5206\u6790\u63d0\u4ea4\u8bb0\u5f55\u7684\u8fc7\u7a0b\u3002", "method": "\u5f00\u53d1\u4e86VulGuard\u81ea\u52a8\u5316\u5de5\u5177\uff0c\u8be5\u5de5\u5177\u80fd\u591f\uff1a1\uff09\u81ea\u52a8\u6316\u6398\u63d0\u4ea4\u5386\u53f2\uff1b2\uff09\u63d0\u53d6\u7ec6\u7c92\u5ea6\u7684\u4ee3\u7801\u53d8\u66f4\u3001\u63d0\u4ea4\u6d88\u606f\u548c\u8f6f\u4ef6\u5de5\u7a0b\u6307\u6807\uff1b3\uff09\u683c\u5f0f\u5316\u6570\u636e\u7528\u4e8e\u4e0b\u6e38\u5206\u6790\uff1b4\uff09\u96c6\u6210\u591a\u4e2a\u5148\u8fdb\u7684\u6f0f\u6d1e\u9884\u6d4b\u6a21\u578b\uff1b5\uff09\u652f\u6301\u4ed3\u5e93\u7ea7\u6316\u6398\u548c\u6a21\u578b\u7ea7\u5b9e\u9a8c\u7684\u7edf\u4e00\u6846\u67b6\uff1b6\uff09\u53ef\u8f7b\u677e\u96c6\u6210\u5230CI/CD\u6d41\u6c34\u7ebf\u4e2d\u3002", "result": "\u5728\u4e24\u4e2a\u5177\u6709\u5f71\u54cd\u529b\u7684\u5f00\u6e90\u9879\u76eeFFmpeg\u548cLinux\u5185\u6838\u4e0a\u9a8c\u8bc1\u4e86\u5de5\u5177\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u771f\u5b9e\u4e16\u754cJIT-VP\u7814\u7a76\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u4fc3\u8fdb\u4e86\u6807\u51c6\u5316\u57fa\u51c6\u6d4b\u8bd5\u3002", "conclusion": "VulGuard\u6210\u529f\u5730\u4e3a\u5373\u65f6\u6f0f\u6d1e\u9884\u6d4b\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u81ea\u52a8\u5316\u3001\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u8f6f\u4ef6\u5b89\u5168\u7814\u7a76\u4e2d\u7684\u53ef\u91cd\u73b0\u6027\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u52a0\u901f\u771f\u5b9e\u4e16\u754c\u7684JIT-VP\u7814\u7a76\u8fdb\u5c55\u3002"}}
{"id": "2507.16754", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16754", "abs": "https://arxiv.org/abs/2507.16754", "authors": ["Fangjian Lei", "Mariam El Mezouar", "Shayan Noei", "Ying Zou"], "title": "Never Come Up Empty: Adaptive HyDE Retrieval for Improving LLM Developer Support", "comment": null, "summary": "Large Language Models (LLMs) have shown promise in assisting developers with\ncode-related questions; however, LLMs carry the risk of generating unreliable\nanswers. To address this, Retrieval-Augmented Generation (RAG) has been\nproposed to reduce the unreliability (i.e., hallucinations) of LLMs. However,\ndesigning effective pipelines remains challenging due to numerous design\nchoices. In this paper, we construct a retrieval corpus of over 3 million Java\nand Python related Stack Overflow posts with accepted answers, and explore\nvarious RAG pipeline designs to answer developer questions, evaluating their\neffectiveness in generating accurate and reliable responses. More specifically,\nwe (1) design and evaluate 7 different RAG pipelines and 63 pipeline variants\nto answer questions that have historically similar matches, and (2) address new\nquestions without any close prior matches by automatically lowering the\nsimilarity threshold during retrieval, thereby increasing the chance of finding\npartially relevant context and improving coverage for unseen cases. We find\nthat implementing a RAG pipeline combining hypothetical-documentation-embedding\n(HyDE) with the full-answer context performs best in retrieving and answering\nsimilarcontent for Stack Overflow questions. Finally, we apply our optimal RAG\npipeline to 4 open-source LLMs and compare the results to their zero-shot\nperformance. Our findings show that RAG with our optimal RAG pipeline\nconsistently outperforms zero-shot baselines across models, achieving higher\nscores for helpfulness, correctness, and detail with LLM-as-a-judge. These\nfindings demonstrate that our optimal RAG pipelines robustly enhance answer\nquality for a wide range of developer queries including both previously seen\nand novel questions across different LLMs", "AI": {"tldr": "\u672c\u7814\u7a76\u6784\u5efa\u4e86\u5305\u542b300\u591a\u4e07Java\u548cPython\u76f8\u5173Stack Overflow\u5e16\u5b50\u7684\u68c0\u7d22\u8bed\u6599\u5e93\uff0c\u8bbe\u8ba1\u5e76\u8bc4\u4f30\u4e867\u79cd\u4e0d\u540c\u7684RAG\u7ba1\u9053\u548c63\u4e2a\u53d8\u4f53\uff0c\u53d1\u73b0\u7ed3\u5408\u5047\u8bbe\u6587\u6863\u5d4c\u5165(HyDE)\u548c\u5b8c\u6574\u7b54\u6848\u4e0a\u4e0b\u6587\u7684RAG\u7ba1\u9053\u5728\u56de\u7b54\u5f00\u53d1\u8005\u95ee\u9898\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u76f8\u6bd4\u96f6\u6837\u672c\u57fa\u7ebf\u5728\u5e2e\u52a9\u6027\u3001\u6b63\u786e\u6027\u548c\u7ec6\u8282\u65b9\u9762\u90fd\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b(LLMs)\u5728\u534f\u52a9\u5f00\u53d1\u8005\u56de\u7b54\u4ee3\u7801\u76f8\u5173\u95ee\u9898\u65b9\u9762\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5b58\u5728\u751f\u6210\u4e0d\u53ef\u9760\u7b54\u6848\u7684\u98ce\u9669\u3002\u867d\u7136\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u88ab\u63d0\u51fa\u6765\u51cf\u5c11LLMs\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u4f46\u7531\u4e8e\u8bbe\u8ba1\u9009\u62e9\u4f17\u591a\uff0c\u6784\u5efa\u6709\u6548\u7684RAG\u7ba1\u9053\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u6784\u5efa\u4e86\u8d85\u8fc7300\u4e07\u6761Java\u548cPython\u76f8\u5173Stack Overflow\u5e16\u5b50\u7684\u68c0\u7d22\u8bed\u6599\u5e93\uff1b\u8bbe\u8ba1\u5e76\u8bc4\u4f30\u4e867\u79cd\u4e0d\u540c\u7684RAG\u7ba1\u9053\u548c63\u4e2a\u7ba1\u9053\u53d8\u4f53\uff1b\u9488\u5bf9\u5386\u53f2\u4e0a\u6709\u76f8\u4f3c\u5339\u914d\u7684\u95ee\u9898\u8fdb\u884c\u8bc4\u4f30\uff1b\u5bf9\u4e8e\u6ca1\u6709\u63a5\u8fd1\u5148\u9a8c\u5339\u914d\u7684\u65b0\u95ee\u9898\uff0c\u901a\u8fc7\u81ea\u52a8\u964d\u4f4e\u68c0\u7d22\u8fc7\u7a0b\u4e2d\u7684\u76f8\u4f3c\u5ea6\u9608\u503c\u6765\u63d0\u9ad8\u8986\u76d6\u7387\uff1b\u5c06\u6700\u4f18RAG\u7ba1\u9053\u5e94\u7528\u4e8e4\u4e2a\u5f00\u6e90LLMs\u5e76\u4e0e\u96f6\u6837\u672c\u6027\u80fd\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u7ed3\u5408\u5047\u8bbe\u6587\u6863\u5d4c\u5165(HyDE)\u548c\u5b8c\u6574\u7b54\u6848\u4e0a\u4e0b\u6587\u7684RAG\u7ba1\u9053\u5728\u68c0\u7d22\u548c\u56de\u7b54Stack Overflow\u95ee\u9898\u7684\u76f8\u4f3c\u5185\u5bb9\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff1b\u6700\u4f18RAG\u7ba1\u9053\u5728\u6240\u6709\u6a21\u578b\u4e0a\u90fd\u6301\u7eed\u4f18\u4e8e\u96f6\u6837\u672c\u57fa\u7ebf\uff0c\u5728LLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u7684\u8bc4\u4f30\u4e2d\uff0c\u5728\u5e2e\u52a9\u6027\u3001\u6b63\u786e\u6027\u548c\u7ec6\u8282\u65b9\u9762\u83b7\u5f97\u4e86\u66f4\u9ad8\u7684\u5206\u6570\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u6700\u4f18RAG\u7ba1\u9053\u80fd\u591f\u7a33\u5065\u5730\u63d0\u5347\u5404\u79cd\u5f00\u53d1\u8005\u67e5\u8be2\u7684\u7b54\u6848\u8d28\u91cf\uff0c\u5305\u62ec\u4e4b\u524d\u89c1\u8fc7\u7684\u548c\u65b0\u9896\u7684\u95ee\u9898\uff0c\u5e76\u4e14\u5728\u4e0d\u540c\u7684LLMs\u4e0a\u90fd\u6709\u6548\u679c\uff0c\u4e3a\u5f00\u53d1\u8005\u95ee\u7b54\u7cfb\u7edf\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u6307\u5bfc\u3002"}}
{"id": "2507.16808", "categories": ["cs.SE", "cs.AI", "68N19, 68T05", "B.6.3; D.3.4; I.2.2; I.2.6"], "pdf": "https://arxiv.org/pdf/2507.16808", "abs": "https://arxiv.org/abs/2507.16808", "authors": ["Zhihao Xu", "Bixin Li", "Lulu Wang"], "title": "Rethinking LLM-Based RTL Code Optimization Via Timing Logic Metamorphosis", "comment": "13pages with 9 pictures and 2 tables", "summary": "Register Transfer Level(RTL) code optimization is crucial for achieving high\nperformance and low power consumption in digital circuit design. However,\ntraditional optimization methods often rely on manual tuning and heuristics,\nwhich can be time-consuming and error-prone. Recent studies proposed to\nleverage Large Language Models(LLMs) to assist in RTL code optimization. LLMs\ncan generate optimized code snippets based on natural language descriptions,\npotentially speeding up the optimization process. However, existing approaches\nhave not thoroughly evaluated the effectiveness of LLM-Based code optimization\nmethods for RTL code with complex timing logic. To address this gap, we\nconducted a comprehensive empirical investigation to assess the capability of\nLLM-Based RTL code optimization methods in handling RTL code with complex\ntiming logic. In this study, we first propose a new benchmark for RTL\noptimization evaluation. It comprises four subsets, each corresponding to a\nspecific area of RTL code optimization. Then we introduce a method based on\nmetamorphosis to systematically evaluate the effectiveness of LLM-Based RTL\ncode optimization methods.Our key insight is that the optimization\neffectiveness should remain consistent for semantically equivalent but more\ncomplex code. After intensive experiments, we revealed several key findings.\n(1) LLM-Based RTL optimization methods can effectively optimize logic\noperations and outperform existing compiler-based methods. (2) LLM-Based RTL\noptimization methods do not perform better than existing compiler-based methods\non RTL code with complex timing logic, particularly in timing control flow\noptimization and clock domain optimization. This is primarily attributed to the\nchallenges LLMs face in understanding timing logic in RTL code. Based on these\nfindings, we provide insights for further research in leveraging LLMs for RTL\ncode optimization.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5bc4\u5b58\u5668\u4f20\u8f93\u7ea7(RTL)\u4ee3\u7801\u4f18\u5316\u4e2d\u7684\u6548\u679c\uff0c\u7279\u522b\u9488\u5bf9\u590d\u6742\u65f6\u5e8f\u903b\u8f91\uff0c\u53d1\u73b0LLM\u5728\u903b\u8f91\u64cd\u4f5c\u4f18\u5316\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u590d\u6742\u65f6\u5e8f\u903b\u8f91\u4f18\u5316\u65b9\u9762\u4e0d\u5982\u4f20\u7edf\u7f16\u8bd1\u5668\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edfRTL\u4ee3\u7801\u4f18\u5316\u65b9\u6cd5\u4f9d\u8d56\u624b\u52a8\u8c03\u4f18\u548c\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u8017\u65f6\u4e14\u6613\u51fa\u9519\u3002\u867d\u7136\u6709\u7814\u7a76\u63d0\u51fa\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8f85\u52a9RTL\u4ee3\u7801\u4f18\u5316\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u672a\u5145\u5206\u8bc4\u4f30LLM\u5728\u5904\u7406\u590d\u6742\u65f6\u5e8f\u903b\u8f91RTL\u4ee3\u7801\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u56db\u4e2a\u5b50\u96c6\u7684RTL\u4f18\u5316\u8bc4\u4f30\u57fa\u51c6\uff0c\u6bcf\u4e2a\u5b50\u96c6\u5bf9\u5e94\u7279\u5b9a\u7684RTL\u4ee3\u7801\u4f18\u5316\u9886\u57df\u3002\u5f15\u5165\u57fa\u4e8e\u53d8\u5f62\u6d4b\u8bd5\u7684\u65b9\u6cd5\u6765\u7cfb\u7edf\u8bc4\u4f30LLM-Based RTL\u4ee3\u7801\u4f18\u5316\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u6838\u5fc3\u601d\u60f3\u662f\u8bed\u4e49\u7b49\u4ef7\u4f46\u66f4\u590d\u6742\u7684\u4ee3\u7801\u4f18\u5316\u6548\u679c\u5e94\u4fdd\u6301\u4e00\u81f4\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u53d1\u73b0\uff1a(1)\u57fa\u4e8eLLM\u7684RTL\u4f18\u5316\u65b9\u6cd5\u80fd\u6709\u6548\u4f18\u5316\u903b\u8f91\u64cd\u4f5c\uff0c\u4f18\u4e8e\u73b0\u6709\u7f16\u8bd1\u5668\u65b9\u6cd5\uff1b(2)\u5728\u590d\u6742\u65f6\u5e8f\u903b\u8f91\u7684RTL\u4ee3\u7801\u4e0a\uff0c\u7279\u522b\u662f\u65f6\u5e8f\u63a7\u5236\u6d41\u4f18\u5316\u548c\u65f6\u949f\u57df\u4f18\u5316\u65b9\u9762\uff0cLLM\u65b9\u6cd5\u4e0d\u5982\u73b0\u6709\u7f16\u8bd1\u5668\u65b9\u6cd5\uff0c\u4e3b\u8981\u56e0\u4e3aLLM\u96be\u4ee5\u7406\u89e3RTL\u4ee3\u7801\u4e2d\u7684\u65f6\u5e8f\u903b\u8f91\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86LLM\u5728RTL\u4ee3\u7801\u4f18\u5316\u4e2d\u7684\u80fd\u529b\u8fb9\u754c\uff0c\u4e3a\u8fdb\u4e00\u6b65\u5229\u7528LLM\u8fdb\u884cRTL\u4ee3\u7801\u4f18\u5316\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u89c1\u89e3\u548c\u6307\u5bfc\u65b9\u5411\u3002"}}
