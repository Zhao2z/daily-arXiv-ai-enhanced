{"id": "2507.17049", "categories": ["cs.SE", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.17049", "abs": "https://arxiv.org/abs/2507.17049", "authors": ["Pablo Valle", "Chengjie Lu", "Shaukat Ali", "Aitor Arrieta"], "title": "Evaluating Uncertainty and Quality of Visual Language Action-enabled Robots", "comment": null, "summary": "Visual Language Action (VLA) models are a multi-modal class of Artificial\nIntelligence (AI) systems that integrate visual perception, natural language\nunderstanding, and action planning to enable agents to interpret their\nenvironment, comprehend instructions, and perform embodied tasks autonomously.\nRecently, significant progress has been made to advance this field. These kinds\nof models are typically evaluated through task success rates, which fail to\ncapture the quality of task execution and the mode's confidence in its\ndecisions. In this paper, we propose eight uncertainty metrics and five quality\nmetrics specifically designed for VLA models for robotic manipulation tasks. We\nassess their effectiveness through a large-scale empirical study involving 908\nsuccessful task executions from three state-of-the-art VLA models across four\nrepresentative robotic manipulation tasks. Human domain experts manually\nlabeled task quality, allowing us to analyze the correlation between our\nproposed metrics and expert judgments. The results reveal that several metrics\nshow moderate to strong correlation with human assessments, highlighting their\nutility for evaluating task quality and model confidence. Furthermore, we found\nthat some of the metrics can discriminate between high-, medium-, and\nlow-quality executions from unsuccessful tasks, which can be interesting when\ntest oracles are not available. Our findings challenge the adequacy of current\nevaluation practices that rely solely on binary success rates and pave the way\nfor improved real-time monitoring and adaptive enhancement of VLA-enabled\nrobotic systems."}
{"id": "2507.17093", "categories": ["cs.SE", "68N30", "D.2.4; D.2.5; D.2.8"], "pdf": "https://arxiv.org/pdf/2507.17093", "abs": "https://arxiv.org/abs/2507.17093", "authors": ["Danushka Liyanage", "Nelum Attanayake", "Zijian Luo", "Rahul Gopinath"], "title": "Assessing Reliability of Statistical Maximum Coverage Estimators in Fuzzing", "comment": "ICSME'25 Registered Report", "summary": "Background: Fuzzers are often guided by coverage, making the estimation of\nmaximum achievable coverage a key concern in fuzzing. However, achieving 100%\ncoverage is infeasible for most real-world software systems, regardless of\neffort. While static reachability analysis can provide an upper bound, it is\noften highly inaccurate. Recently, statistical estimation methods based on\nspecies richness estimators from biostatistics have been proposed as a\npotential solution. Yet, the lack of reliable benchmarks with labeled ground\ntruth has limited rigorous evaluation of their accuracy.\n  Objective: This work examines the reliability of reachability estimators from\ntwo axes: addressing the lack of labeled ground truth and evaluating their\nreliability on real-world programs.\n  Methods: (1) To address the challenge of labeled ground truth, we propose an\nevaluation framework that synthetically generates large programs with complex\ncontrol flows, ensuring well-defined reachability and providing ground truth\nfor evaluation. (2) To address the criticism from use of synthetic benchmarks,\nwe adapt a reliability check for reachability estimators on real-world\nbenchmarks without labeled ground truth -- by varying the size of sampling\nunits, which, in theory, should not affect the estimate.\n  Results: These two studies together will help answer the question of whether\ncurrent reachability estimators are reliable, and defines a protocol to\nevaluate future improvements in reachability estimation."}
{"id": "2507.17165", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.17165", "abs": "https://arxiv.org/abs/2507.17165", "authors": ["Taher A. Ghaleb", "Dulina Rathnayake"], "title": "Can LLMs Write CI? A Study on Automatic Generation of GitHub Actions Configurations", "comment": "Accepted at the 41st IEEE International Conference on Software\n  Maintenance and Evolution 2025 (ICSME'25)", "summary": "Continuous Integration (CI) services, such as GitHub Actions, require\ndevelopers to write YAML-based configurations, which can be tedious and\nerror-prone. Despite the increasing use of Large Language Models (LLMs) to\nautomate software engineering tasks, their ability to generate CI\nconfigurations remains underexplored. This paper presents a preliminary study\nevaluating six LLMs for generating GitHub Actions configurations from natural\nlanguage descriptions. We assess three general-purpose foundation models\n(GPT-4o, Llama, and Gemma) and three code-pretrained models (GPT-4.1, Code\nLlama, and CodeGemma). We also introduce the first labeled dataset of its kind,\nconstructed from GitHub Actions documentation, pairing descriptions with\ncorresponding best-practice YAML configurations. Zero-shot prompting achieves\nup to 69% similarity with the ground truth, with only 3% perfect matches.\nCode-pretrained models slightly underperform compared to general-purpose ones\nin YAML-based CI tasks, revealing LLM limitations for CI configuration\ngeneration. Analyzing GPT-4o outputs reveals issues like missing or renamed\nsteps, misinterpreted descriptions, and unnecessary additions that may affect\nstructural and contextual correctness, indicating a gap between generation\nquality and the precision required for executable CI configurations. Our\nresearch offers insights for improving LLM alignment with configuration\nlanguages and guiding future efforts on CI automation and tooling support."}
{"id": "2507.17235", "categories": ["cs.SE", "quant-ph"], "pdf": "https://arxiv.org/pdf/2507.17235", "abs": "https://arxiv.org/abs/2507.17235", "authors": ["Andriy Miranskyy", "José Campos", "Anila Mjeda", "Lei Zhang", "Ignacio García Rodríguez de Guzmán"], "title": "On the Feasibility of Quantum Unit Testing", "comment": null, "summary": "The increasing complexity of quantum software presents significant challenges\nfor software verification and validation, particularly in the context of unit\ntesting. This work presents a comprehensive study on quantum-centric unit\ntests, comparing traditional statistical approaches with tests specifically\ndesigned for quantum circuits. These include tests that run only on a classical\ncomputer, such as the Statevector test, as well as those executable on quantum\nhardware, such as the Swap test and the novel Inverse test. Through an\nempirical study and detailed analysis on 1,796,880 mutated quantum circuits, we\ninvestigate (a) each test's ability to detect subtle discrepancies between the\nexpected and actual states of a quantum circuit, and (b) the number of\nmeasurements required to achieve high reliability. The results demonstrate that\nquantum-centric tests, particularly the Statevector test and the Inverse test,\nprovide clear advantages in terms of precision and efficiency, reducing both\nfalse positives and false negatives compared to statistical tests. This work\ncontributes to the development of more robust and scalable strategies for\ntesting quantum software, supporting the future adoption of fault-tolerant\nquantum computers and promoting more reliable practices in quantum software\nengineering."}
{"id": "2507.17264", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.17264", "abs": "https://arxiv.org/abs/2507.17264", "authors": ["Jenny T. Liang", "Chenyang Yang", "Agnia Sergeyuk", "Travis D. Breaux", "Brad A. Myers"], "title": "Understanding Prompt Programming Tasks and Questions", "comment": null, "summary": "Prompting foundation models (FMs) like large language models (LLMs) have\nenabled new AI-powered software features (e.g., text summarization) that\npreviously were only possible by fine-tuning FMs. Now, developers are embedding\nprompts in software, known as prompt programs. The process of prompt\nprogramming requires the developer to make many changes to their prompt. Yet,\nthe questions developers ask to update their prompt is unknown, despite the\nanswers to these questions affecting how developers plan their changes. With\nthe growing number of research and commercial prompt programming tools, it is\nunclear whether prompt programmers' needs are being adequately addressed. We\naddress these challenges by developing a taxonomy of 25 tasks prompt\nprogrammers do and 51 questions they ask, measuring the importance of each task\nand question. We interview 16 prompt programmers, observe 8 developers make\nprompt changes, and survey 50 developers. We then compare the taxonomy with 48\nresearch and commercial tools. We find that prompt programming is not\nwell-supported: all tasks are done manually, and 16 of the 51 questions --\nincluding a majority of the most important ones -- remain unanswered. Based on\nthis, we outline important opportunities for prompt programming tools."}
{"id": "2507.17270", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.17270", "abs": "https://arxiv.org/abs/2507.17270", "authors": ["Alessandro Aneggi", "Andrea Janes"], "title": "Lessons from a Big-Bang Integration: Challenges in Edge Computing and Machine Learning", "comment": "Accepted @ XP2025 Poster session", "summary": "This experience report analyses a one year project focused on building a\ndistributed real-time analytics system using edge computing and machine\nlearning. The project faced critical setbacks due to a big-bang integration\napproach, where all components developed by multiple geographically dispersed\npartners were merged at the final stage. The integration effort resulted in\nonly six minutes of system functionality, far below the expected 40 minutes.\nThrough root cause analysis, the study identifies technical and organisational\nbarriers, including poor communication, lack of early integration testing, and\nresistance to topdown planning. It also considers psychological factors such as\na bias toward fully developed components over mockups. The paper advocates for\nearly mock based deployment, robust communication infrastructures, and the\nadoption of topdown thinking to manage complexity and reduce risk in reactive,\ndistributed projects. These findings underscore the limitations of traditional\nAgile methods in such contexts and propose simulation-driven engineering and\nstructured integration cycles as key enablers for future success."}
{"id": "2507.17271", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.17271", "abs": "https://arxiv.org/abs/2507.17271", "authors": ["Shuaiyu Zhou", "Zhengran Zeng", "Xiaoling Zhou", "Rui Xie", "Shikun Zhang", "Wei Ye"], "title": "Seed&Steer: Guiding Large Language Models with Compilable Prefix and Branch Signals for Unit Test Generation", "comment": null, "summary": "Unit tests play a vital role in the software development lifecycle. Recent\nadvances in Large Language Model (LLM)-based approaches have significantly\nimproved automated test generation, garnering attention from both academia and\nindustry. We revisit LLM-based unit test generation from a novel perspective by\ndecoupling prefix generation and assertion generation. To characterize their\nrespective challenges, we define Initialization Complexity and adopt Cyclomatic\nComplexity to measure the difficulty of prefix and assertion generation,\nrevealing that the former primarily affects compilation success, while the\nlatter influences test coverage. To address these challenges, we propose\nSeed&Steer, a two-step approach that combines traditional unit testing\ntechniques with the capabilities of large language models. Seed&Steer leverages\nconventional unit testing tools (e.g., EvoSuite) to generate method invocations\nwith high compilation success rates, which serve as seeds to guide LLMs in\nconstructing effective test contexts. It then introduces branching cues to help\nLLMs explore diverse execution paths (e.g., normal, boundary, and exception\ncases) and generate assertions with high coverage. We evaluate Seed&Steer on\nfive real-world Java projects against state-of-the-art baselines. Results show\nthat Seed&Steer improves the compilation pass rate by approximately 7%,\nsuccessfully compiling 792 and 887 previously failing cases on two LLMs. It\nalso achieves up to ~73% branch and line coverage across focal methods of\nvarying complexity, with coverage improvements ranging from 1.09* to 1.26*. Our\ncode, dataset, and experimental scripts will be publicly released to support\nfuture research and reproducibility."}
{"id": "2507.17293", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.17293", "abs": "https://arxiv.org/abs/2507.17293", "authors": ["Saiful Khan", "Joyraj Chakraborty", "Philip Beaucamp", "Niraj Bhujel", "Min Chen"], "title": "Data Virtualization for Machine Learning", "comment": null, "summary": "Nowadays, machine learning (ML) teams have multiple concurrent ML workflows\nfor different applications. Each workflow typically involves many experiments,\niterations, and collaborative activities and commonly takes months and\nsometimes years from initial data wrangling to model deployment.\nOrganizationally, there is a large amount of intermediate data to be stored,\nprocessed, and maintained. \\emph{Data virtualization} becomes a critical\ntechnology in an infrastructure to serve ML workflows. In this paper, we\npresent the design and implementation of a data virtualization service,\nfocusing on its service architecture and service operations. The infrastructure\ncurrently supports six ML applications, each with more than one ML workflow.\nThe data virtualization service allows the number of applications and workflows\nto grow in the coming years."}
{"id": "2507.17314", "categories": ["cs.SE", "K.3.2, D.2.m, D.1.7"], "pdf": "https://arxiv.org/pdf/2507.17314", "abs": "https://arxiv.org/abs/2507.17314", "authors": ["Ricardo Hidalgo Aragón", "Jesús M. González-Barahona", "Gregorio Robles"], "title": "How Do Code Smells Affect Skill Growth in Scratch Novice Programmers?", "comment": "Registered Report accepted at ICSME 2025", "summary": "Context. Code smells, which are recurring anomalies in design or style, have\nbeen extensively researched in professional code. However, their significance\nin block-based projects created by novices is still largely unknown.\nBlock-based environments such as Scratch offer a unique, data-rich setting to\nexamine how emergent design problems intersect with the cultivation of\ncomputational-thinking (CT) skills. Objective. This research explores the\nconnection between CT proficiency and design-level code smells--issues that may\nhinder software maintenance and evolution--in programs created by Scratch\ndevelopers. We seek to identify which CT dimensions align most strongly with\nwhich code smells and whether task context moderates those associations.\nMethod. A random sample of aprox. 2 million public Scratch projects is mined.\nUsing open-source linters, we extract nine CT scores and 40 code smell\nindicators from these projects. After rigorous pre-processing, we apply\ndescriptive analytics, robust correlation tests, stratified cross-validation,\nand exploratory machine-learning models; qualitative spot-checks contextualize\nquantitative patterns. Impact. The study will deliver the first large-scale,\nfine-grained map linking specific CT competencies to concrete design flaws and\nantipatterns. Results are poised to (i) inform evidence-based curricula and\nautomated feedback systems, (ii) provide effect-size benchmarks for future\neducational interventions, and (iii) supply an open, pseudonymized dataset and\nreproducible analysis pipeline for the research community. By clarifying how\nprogramming habits influence early skill acquisition, the work advances both\ncomputing-education theory and practical tooling for sustainable software\nmaintenance and evolution."}
{"id": "2507.17369", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.17369", "abs": "https://arxiv.org/abs/2507.17369", "authors": ["Corentin Latappy", "Thomas Degueule", "Jean-Rémy Falleri", "Romain Robbes", "Lina Ochoa"], "title": "Roseau: Fast, Accurate, Source-based API Breaking Change Analysis in Java", "comment": null, "summary": "Understanding API evolution and the introduction of breaking changes (BCs) in\nsoftware libraries is essential for library maintainers to manage backward\ncompatibility and for researchers to conduct empirical studies on software\nlibrary evolution. In Java, tools such as JApiCmp and Revapi are commonly used\nto detect BCs between library releases, but their reliance on binary JARs\nlimits their applicability. This restriction hinders large-scale longitudinal\nstudies of API evolution and fine-grained analyses such as commit-level BC\ndetection. In this paper, we introduce Roseau, a novel static analysis tool\nthat constructs technology-agnostic API models from library code equipped with\nrich semantic analyses. API models can be analyzed to study API evolution and\ncompared to identify BCs between any two versions of a library (releases,\ncommits, branches, etc.). Unlike traditional approaches, Roseau can build API\nmodels from source code or bytecode, and is optimized for large-scale\nlongitudinal analyses of library histories. We assess the accuracy,\nperformance, and suitability of Roseau for longitudinal studies of API\nevolution, using JApiCmp and Revapi as baselines. We extend and refine an\nestablished benchmark of BCs and show that Roseau achieves higher accuracy (F1\n= 0.99) than JApiCmp (F1 = 0.86) and Revapi (F1 = 0.91). We analyze 60 popular\nlibraries from Maven Central and find that Roseau delivers excellent\nperformance, detecting BCs between versions in under two seconds, including in\nlibraries with hundreds of thousands of lines of code. We further illustrate\nthe limitations of JApiCmp and Revapi for longitudinal studies and the novel\nanalysis capabilities offered by Roseau by tracking the evolution of Google's\nGuava API and the introduction of BCs over 14 years and 6,839 commits, reducing\nanalysis times from a few days to a few minutes."}
{"id": "2507.17389", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17389", "abs": "https://arxiv.org/abs/2507.17389", "authors": ["Tianlin Li", "Yunxiang Wei", "Zhiming Li", "Aishan Liu", "Qing Guo", "Xianglong Liu", "Dongning Sun", "Yang Liu"], "title": "Investigating Training Data Detection in AI Coders", "comment": null, "summary": "Recent advances in code large language models (CodeLLMs) have made them\nindispensable tools in modern software engineering. However, these models\noccasionally produce outputs that contain proprietary or sensitive code\nsnippets, raising concerns about potential non-compliant use of training data,\nand posing risks to privacy and intellectual property. To ensure responsible\nand compliant deployment of CodeLLMs, training data detection (TDD) has become\na critical task. While recent TDD methods have shown promise in natural\nlanguage settings, their effectiveness on code data remains largely\nunderexplored. This gap is particularly important given code's structured\nsyntax and distinct similarity criteria compared to natural language. To\naddress this, we conduct a comprehensive empirical study of seven\nstate-of-the-art TDD methods on source code data, evaluating their performance\nacross eight CodeLLMs. To support this evaluation, we introduce CodeSnitch, a\nfunction-level benchmark dataset comprising 9,000 code samples in three\nprogramming languages, each explicitly labeled as either included or excluded\nfrom CodeLLM training. Beyond evaluation on the original CodeSnitch, we design\ntargeted mutation strategies to test the robustness of TDD methods under three\ndistinct settings. These mutation strategies are grounded in the\nwell-established Type-1 to Type-4 code clone detection taxonomy. Our study\nprovides a systematic assessment of current TDD techniques for code and offers\ninsights to guide the development of more effective and robust detection\nmethods in the future."}
{"id": "2507.17542", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.17542", "abs": "https://arxiv.org/abs/2507.17542", "authors": ["Lara Khatib", "Noble Saji Mathews", "Meiyappan Nagappan"], "title": "AssertFlip: Reproducing Bugs via Inversion of LLM-Generated Passing Tests", "comment": null, "summary": "Bug reproduction is critical in the software debugging and repair process,\nyet the majority of bugs in open-source and industrial settings lack executable\ntests to reproduce them at the time they are reported, making diagnosis and\nresolution more difficult and time-consuming. To address this challenge, we\nintroduce AssertFlip, a novel technique for automatically generating Bug\nReproducible Tests (BRTs) using large language models (LLMs). Unlike existing\nmethods that attempt direct generation of failing tests, AssertFlip first\ngenerates passing tests on the buggy behaviour and then inverts these tests to\nfail when the bug is present. We hypothesize that LLMs are better at writing\npassing tests than ones that crash or fail on purpose. Our results show that\nAssertFlip outperforms all known techniques in the leaderboard of SWT-Bench, a\nbenchmark curated for BRTs. Specifically, AssertFlip achieves a fail-to-pass\nsuccess rate of 43.6% on the SWT-Bench-Verified subset."}
{"id": "2507.17548", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.17548", "abs": "https://arxiv.org/abs/2507.17548", "authors": ["Lingxiao Tang", "He Ye", "Zhongxin Liu", "Xiaoxue Ren", "Lingfeng Bao"], "title": "CodeReasoner: Enhancing the Code Reasoning Ability with Reinforcement Learning", "comment": null, "summary": "Code reasoning is a fundamental capability for large language models (LLMs)\nin the code domain. It involves understanding and predicting a program's\nexecution behavior, such as determining the output for a given input or whether\na specific statement will be executed. This capability is essential for\ndownstream tasks like debugging, code generation, and program repair. Prior\napproaches mainly rely on supervised fine-tuning to improve performance in code\nreasoning tasks. However, they often show limited gains and fail to generalize\nacross diverse scenarios. We argue this is due to two core issues: the low\nquality of training data and the limitations of supervised fine-tuning, which\nstruggles to teach general reasoning skills. To address these challenges, we\npropose CodeReasoner, a framework that spans both dataset construction and a\ntwo-stage training process. First, we introduce a method to construct datasets\nthat focus on the core execution logic of Python programs. Next, we apply\ninstruction tuning to inject execution-specific knowledge distilled from a\npowerful teacher model. We then enhance reasoning and generalization through\nGRPO reinforcement learning on top of the fine-tuned model. Experiments on\nthree widely-used code reasoning benchmarks show that CodeReasoner improves\nperformance by 27.1% to 40.2% over prior methods using a 7B model. Notably, the\n7B model matches GPT-4o on key tasks like input/output and coverage prediction.\nWhen scaled to 14B, CodeReasoner outperforms GPT-4o across all benchmarks.\nAblation studies confirm the effectiveness of each training stage and highlight\nthe importance of reasoning chains."}
{"id": "2507.17690", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.17690", "abs": "https://arxiv.org/abs/2507.17690", "authors": ["Bo Xiong", "Linghao Zhang", "Chong Wang", "Peng Liang"], "title": "Contextual Code Retrieval for Commit Message Generation: A Preliminary Study", "comment": "The 19th ACM/IEEE International Symposium on Empirical Software\n  Engineering and Measurement (ESEM)", "summary": "A commit message describes the main code changes in a commit and plays a\ncrucial role in software maintenance. Existing commit message generation (CMG)\napproaches typically frame it as a direct mapping which inputs a code diff and\nproduces a brief descriptive sentence as output. However, we argue that relying\nsolely on the code diff is insufficient, as raw code diff fails to capture the\nfull context needed for generating high-quality and informative commit\nmessages. In this paper, we propose a contextual code retrieval-based method\ncalled C3Gen to enhance CMG by retrieving commit-relevant code snippets from\nthe repository and incorporating them into the model input to provide richer\ncontextual information at the repository scope. In the experiments, we\nevaluated the effectiveness of C3Gen across various models using four objective\nand three subjective metrics. Meanwhile, we design and conduct a human\nevaluation to investigate how C3Gen-generated commit messages are perceived by\nhuman developers. The results show that by incorporating contextual code into\nthe input, C3Gen enables models to effectively leverage additional information\nto generate more comprehensive and informative commit messages with greater\npractical value in real-world development scenarios. Further analysis\nunderscores concerns about the reliability of similaritybased metrics and\nprovides empirical insights for CMG."}
{"id": "2507.17691", "categories": ["cs.SE", "cs.AI", "cs.CR", "cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2507.17691", "abs": "https://arxiv.org/abs/2507.17691", "authors": ["Shan Jiang", "Pranoy Kovuri", "David Tao", "Zhixun Tan"], "title": "CASCADE: LLM-Powered JavaScript Deobfuscator at Google", "comment": null, "summary": "Software obfuscation, particularly prevalent in JavaScript, hinders code\ncomprehension and analysis, posing significant challenges to software testing,\nstatic analysis, and malware detection. This paper introduces CASCADE, a novel\nhybrid approach that integrates the advanced coding capabilities of Gemini with\nthe deterministic transformation capabilities of a compiler Intermediate\nRepresentation (IR), specifically JavaScript IR (JSIR). By employing Gemini to\nidentify critical prelude functions, the foundational components underlying the\nmost prevalent obfuscation techniques, and leveraging JSIR for subsequent code\ntransformations, CASCADE effectively recovers semantic elements like original\nstrings and API names, and reveals original program behaviors. This method\novercomes limitations of existing static and dynamic deobfuscation techniques,\neliminating hundreds to thousands of hardcoded rules while achieving\nreliability and flexibility. CASCADE is already deployed in Google's production\nenvironment, demonstrating substantial improvements in JavaScript deobfuscation\nefficiency and reducing reverse engineering efforts."}
{"id": "2507.17743", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.17743", "abs": "https://arxiv.org/abs/2507.17743", "authors": ["Andre Menolli", "Bruno Strik"], "title": "Educational Insights from Code: Mapping Learning Challenges in Object-Oriented Programming through Code-Based Evidence", "comment": null, "summary": "Object-Oriented programming is frequently challenging for undergraduate\nComputer Science students, particularly in understanding abstract concepts such\nas encapsulation, inheritance, and polymorphism. Although the literature\noutlines various methods to identify potential design and coding issues in\nobject-oriented programming through source code analysis, such as code smells\nand SOLID principles, few studies explore how these code-level issues relate to\nlearning difficulties in Object-Oriented Programming. In this study, we explore\nthe relationship of the code issue indicators with common challenges\nencountered during the learning of object-oriented programming. Using\nqualitative analysis, we identified the main categories of learning\ndifficulties and, through a literature review, established connections between\nthese difficulties, code smells, and violations of the SOLID principles. As a\nresult, we developed a conceptual map that links code-related issues to\nspecific learning challenges in Object-Oriented Programming. The model was then\nevaluated by an expert who applied it in the analysis of the student code to\nassess its relevance and applicability in educational contexts."}
