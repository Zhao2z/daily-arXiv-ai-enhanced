<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 10]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [The State of Computational Science in Fission and Fusion Energy](https://arxiv.org/abs/2507.08061)
*Andrea Morales Coto,Aditi Verma*

Main category: cs.SE

TL;DR: 调查显示，核工程领域的计算科学家更倾向于使用现代编程语言、开源代码和模块化软件，预示着未来5到10年的行业趋势。


<details>
  <summary>Details</summary>
Motivation: 研究核工程领域计算科学家使用的软件工具及其开发体验，以了解行业趋势。

Method: 对103位从事聚变和裂变能代码开发的科学家进行问卷调查。

Result: 发现现代编程语言（如Python和C++）、开源代码和模块化软件的使用增加，FORTRAN使用减少，代码开发预算高达5000万美元。

Conclusion: 核工程代码未来将趋向模块化、计算量小且更受组织重视。

Abstract: The tools used to engineer something are just as important as the thing that
is actually being engineered. In fact, in many cases, the tools can indeed
determine what is engineerable. In fusion and fission1 energy engineering,
software has become the dominant tool for design. For that reason, in 2024, for
the first time ever, we asked 103 computational scientists developing the codes
used in fusion and fission energy about the problems they are attempting to
solve with their codes, the tools available to them to solve them, and their
end to end developer experience with said tools.
  The results revealed a changing tide in software tools in fusion and fission,
with more and more computational scientists preferring modern programming
languages, open-source codes, and modular software. These trends represent a
peek into what will happen 5 to 10 years in the future of nuclear engineering.
Since the majority of our respondents belonged to US national labs and
universities, these results hint at the most cutting-edge trends in the
industry. The insights included in the State of Computational Science in
Fission and Fusion Energy indicate a dramatic shift toward multiphysics codes,
a drop-off in the use of FORTRAN in favor of more modern languages like Python
and C++, and ever-rising budgets for code development, at times reaching $50M
in a single organization.
  Our survey paints a future of nuclear engineering codes that is modular in
nature, small in terms of compute, and increasingly prioritized by
organizations. Access to our results in web form are available online.

</details>


### [2] [Code with Me or for Me? How Increasing AI Automation Transforms Developer Workflows](https://arxiv.org/abs/2507.08149)
*Valerie Chen,Ameet Talwalkar,Robert Brennan,Graham Neubig*

Main category: cs.SE

TL;DR: 研究探讨了开发者与自动化编码代理的互动，发现其潜力超越传统copilot，但也面临用户理解不足等挑战。


<details>
  <summary>Details</summary>
Motivation: 探索更自主的AI工具（如编码代理）对开发者生产力和体验的影响，并与现有copilot进行比较。

Method: 通过实验评估GitHub Copilot和OpenHands两种工具，招募常用Copilot的开发者参与。

Result: 编码代理能完成传统copilot无法完成的任务并减少用户工作量，但用户对其行为的理解是推广的挑战。

Conclusion: 研究揭示了编码代理对开发者工作流的改变，并提出了构建新代理的建议，同时指出推广中的关键挑战。

Abstract: Developers now have access to a growing array of increasingly autonomous AI
tools to support software development. While numerous studies have examined
developer use of copilots, which can provide chat assistance or code
completions, evaluations of coding agents, which can automatically write files
and run code, still largely rely on static benchmarks without
humans-in-the-loop. In this work, we conduct the first academic study to
explore developer interactions with coding agents and characterize how more
autonomous AI tools affect user productivity and experience, compared to
existing copilots. We evaluate two leading copilot and agentic coding
assistants, GitHub Copilot and OpenHands, recruiting participants who regularly
use the former. Our results show agents have the potential to assist developers
in ways that surpass copilots (e.g., completing tasks that humans might not
have accomplished before) and reduce the user effort required to complete
tasks. However, there are challenges involved in enabling their broader
adoption, including how to ensure users have an adequate understanding of agent
behaviors. Our results not only provide insights into how developer workflows
change as a result of coding agents but also highlight how user interactions
with agents differ from those with existing copilots, motivating a set of
recommendations for researchers building new agents. Given the broad set of
developers who still largely rely on copilot-like systems, our work highlights
key challenges of adopting more agentic systems into developer workflows.

</details>


### [3] [The Impact of Generative AI on Code Expertise Models: An Exploratory Study](https://arxiv.org/abs/2507.08160)
*Otávio Cury,Guilherme Avelino*

Main category: cs.SE

TL;DR: 论文探讨了生成式AI工具对源代码知识模型和开发者专长指标的影响，发现其使用会降低当前专长度量的可靠性。


<details>
  <summary>Details</summary>
Motivation: 生成式AI工具在提升软件开发效率的同时，可能导致开发者对生成代码的理解不足，进而影响知识模型和专长指标的准确性。

Method: 通过收集ChatGPT生成代码在GitHub项目中的统计数据，并模拟不同GenAI贡献程度的情景进行分析。

Result: 大多数情景下，GenAI的使用对知识模型和专长度量产生了可测量的影响。

Conclusion: 随着GenAI在开发流程中的普及，现有专长度量的可靠性可能会下降。

Abstract: Generative Artificial Intelligence (GenAI) tools for source code generation
have significantly boosted productivity in software development. However, they
also raise concerns, particularly the risk that developers may rely heavily on
these tools, reducing their understanding of the generated code. We hypothesize
that this loss of understanding may be reflected in source code knowledge
models, which are used to identify developer expertise. In this work, we
present an exploratory analysis of how a knowledge model and a Truck Factor
algorithm built upon it can be affected by GenAI usage. To investigate this, we
collected statistical data on the integration of ChatGPT-generated code into
GitHub projects and simulated various scenarios by adjusting the degree of
GenAI contribution. Our findings reveal that most scenarios led to measurable
impacts, indicating the sensitivity of current expertise metrics. This suggests
that as GenAI becomes more integrated into development workflows, the
reliability of such metrics may decrease.

</details>


### [4] [Leveraging Large Language Models for Classifying App Users' Feedback](https://arxiv.org/abs/2507.08250)
*Yasaman Abedini,Abbas Heydarnoori*

Main category: cs.SE

TL;DR: 论文评估了四种先进LLM（GPT-3.5-Turbo、GPT-4、Flan-T5和Llama3-70b）在用户反馈分类中的表现，旨在解决标记数据不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有监督学习方法依赖大量标记数据，但获取这些数据耗时耗力，因此探索LLM在用户反馈分类中的潜力。

Method: 在八个已标记数据集上实验，利用LLM作为标注工具增强数据集，并分析其在粗细粒度分类中的表现。

Result: LLM在精心设计的提示下能有效分类粗粒度反馈，且通过LLM共识标记的数据增强显著提升分类器性能。

Conclusion: LLM可作为高效标注工具，提升用户反馈分类性能，尤其在数据有限的情况下。

Abstract: In recent years, significant research has been conducted into classifying
application (app) user feedback, primarily relying on supervised machine
learning algorithms. However, fine-tuning more generalizable classifiers based
on existing labeled datasets remains an important challenge, as creating large
and accurately labeled datasets often requires considerable time and resources.
In this paper, we evaluate the capabilities of four advanced LLMs, including
GPT-3.5-Turbo, GPT-4, Flan-T5, and Llama3-70b, to enhance user feedback
classification and address the challenge of the limited labeled dataset. To
achieve this, we conduct several experiments on eight datasets that have been
meticulously labeled in prior research. These datasets include user reviews
from app stores, posts from the X platform, and discussions from the public
forums, widely recognized as representative sources of app user feedback. We
analyze the performance of various LLMs in identifying both fine-grained and
coarse-grained user feedback categories. Given the substantial volume of daily
user feedback and the computational limitations of LLMs, we leverage these
models as an annotation tool to augment labeled datasets with general and
app-specific data. This augmentation aims to enhance the performance of
state-of-the-art BERT-based classification models. Our findings indicate that
LLMs when guided by well-crafted prompts, can effectively classify user
feedback into coarse-grained categories. Moreover, augmenting the training
dataset with datasets labeled using the consensus of LLMs can significantly
enhance classifier performance.

</details>


### [5] [Computing Floating-Point Errors by Injecting Perturbations](https://arxiv.org/abs/2507.08467)
*Youshuai Tan,Zhanwei Zhang,Jinfu Chen,Zishuo Ding,Jifeng Xuan,Weiyi Shang*

Main category: cs.SE

TL;DR: PI-detector是一种新方法，用于高效计算浮点误差，通过扰动原子操作并比较结果来检测误差。


<details>
  <summary>Details</summary>
Motivation: 浮点误差可能导致严重后果，现有工具如ATOMU和FPCC存在假阳性或速度慢的问题。

Method: PI-detector通过扰动原子操作的输入并比较原始与扰动结果来计算浮点误差。

Result: 实验表明，PI-detector能高效准确地计算浮点误差。

Conclusion: PI-detector解决了现有工具的局限性，提供了一种更优的浮点误差检测方法。

Abstract: Floating-point programs form the foundation of modern science and
engineering, providing the essential computational framework for a wide range
of applications, such as safety-critical systems, aerospace engineering, and
financial analysis. Floating-point errors can lead to severe consequences.
Although floating-point errors widely exist, only a subset of inputs may
trigger significant errors in floating-point programs. Therefore, it is crucial
to determine whether a given input could produce such errors. Researchers tend
to take the results of high-precision floating-point programs as oracles for
detecting floating-point errors, which introduces two main limitations: (1)
difficulty of implementation and (2) prolonged execution time. The two recent
tools, ATOMU and FPCC, can partially address these issues. However, ATOMU
suffers from false positives; while FPCC, though eliminating false positives,
operates at a considerably slower speed.
  To address these two challenges, we propose a novel approach named
PI-detector to computing floating-point errors effectively and efficiently. Our
approach is based on the observation that floating-point errors stem from large
condition numbers in atomic operations (such as addition and subtraction),
which then propagate and accumulate. PI-detector injects small perturbations
into the operands of individual atomic operations within the program and
compares the outcomes of the original program with the perturbed version to
compute floating-point errors. We evaluate PI-detector with datasets from ATOMU
and HSED, as well as a complex linear system-solving program. Experimental
results demonstrate that PI-detector can perform efficient and accurate
floating-point error computation.

</details>


### [6] [InferLog: Accelerating LLM Inference for Online Log Parsing via ICL-oriented Prefix Caching](https://arxiv.org/abs/2507.08523)
*Yilun Wang,Pengfei Chen,Haiyu Huang,Zilong He,Gou Tan,Chuanfu Zhang,Jingkai He,Zibin Zheng*

Main category: cs.SE

TL;DR: InferLog是一种针对在线日志解析的LLM推理优化方法，通过改进上下文学习和配置调优，显著提升了推理效率，同时保持了解析精度。


<details>
  <summary>Details</summary>
Motivation: 现代软件系统生成大量运行时日志，需要高效准确的日志解析以支持异常检测等任务。现有LLM方法存在隐私风险和延迟问题，无法满足高吞吐量需求。

Method: InferLog设计了前缀感知的上下文学习优化策略和基于元学习的快速配置调优管道，以提升推理效率。

Result: 在Loghub数据集和vLLM上的实验表明，InferLog显著优于现有优化方法，加速了LLM日志解析器且未损失精度。

Conclusion: InferLog通过优化推理效率，解决了LLM在日志解析中的延迟问题，为高吞吐量日志处理提供了可行方案。

Abstract: Modern software systems generate massive volumes of runtime logs,
necessitating efficient and accurate log parsing to enable critical downstream
tasks such as anomaly detection and root cause analysis. Recently, large
language models (LLMs) have achieved advanced accuracy on log parsing, but
their deployment in production environments faces two major limitations: (1)
the privacy risks associated with commercial LLMs, driving the adoption of
local deployment, and (2) the stringent latency and throughput requirements
imposed by high-volume log streams, which existing LLM-based parsers fail to
meet. Although recent efforts have reduced the number of LLM queries, they
overlook the high latency of the LLM invocations, where concurrent log parsing
requests can cause serve performance degradation of LLM inference system.
  In this study, we present InferLog, the first LLM inference optimization
method for online log parsing. Our key insight is that the inference efficiency
emerges as the vital bottleneck in LLM-based online log parsing, rather than
parsing accuracy. InferLog accelerates inference by designing (1) A
Prefix-aware ICL Refinement policy to refine the examples and permutation of
in-context learning to improve the prefix caching efficiency. (2) A rapid and
task-specific configuration tuning pipeline based on meta-learning to find the
optimal LLM scheduling-related configuration for dynamic log parsing workloads.
The experimental results based on Loghub dataset and vLLM demonstrate that
InferLog significantly outperforms existing inference optimization methods and
markedly accelerates the state-of-the-art LLM-based log parser without
compromising parsing accuracy.

</details>


### [7] [Generating Proto-Personas through Prompt Engineering: A Case Study on Efficiency, Effectiveness and Empathy](https://arxiv.org/abs/2507.08594)
*Fernando Ayach,Vitor Lameirão,Raul Leão,Jerfferson Felizardo,Rafael Sobrinho,Vanessa Borges,Patrícia Matsubara,Awdren Fontão*

Main category: cs.SE

TL;DR: 论文提出了一种基于提示工程的生成式AI方法，用于自动生成原型人物角色，以提高效率、减少偏见，并评估其在产品发现中的效果。


<details>
  <summary>Details</summary>
Motivation: 手动创建原型人物角色耗时、费力且易受偏见影响，因此研究如何利用生成式AI改进这一过程。

Method: 采用提示工程方法结合生成式AI，通过案例研究（19名参与者）评估其效率、效果和用户接受度。

Result: 方法显著提高了效率和质量，用户接受度高，但在泛化和领域特异性方面存在局限，情感和行为同理心表现不一。

Conclusion: 生成式AI可有效整合到产品发现中，但需解决泛化和同理心差异等挑战。

Abstract: Proto-personas are commonly used during early-stage Product Discovery, such
as Lean Inception, to guide product definition and stakeholder alignment.
However, the manual creation of proto-personas is often time-consuming,
cognitively demanding, and prone to bias. In this paper, we propose and
empirically investigate a prompt engineering-based approach to generate
proto-personas with the support of Generative AI (GenAI). Our goal is to
evaluate the approach in terms of efficiency, effectiveness, user acceptance,
and the empathy elicited by the generated personas. We conducted a case study
with 19 participants embedded in a real Lean Inception, employing a qualitative
and quantitative methods design. The results reveal the approach's efficiency
by reducing time and effort and improving the quality and reusability of
personas in later discovery phases, such as Minimum Viable Product (MVP)
scoping and feature refinement. While acceptance was generally high, especially
regarding perceived usefulness and ease of use, participants noted limitations
related to generalization and domain specificity. Furthermore, although
cognitive empathy was strongly supported, affective and behavioral empathy
varied significantly across participants. These results contribute novel
empirical evidence on how GenAI can be effectively integrated into software
Product Discovery practices, while also identifying key challenges to be
addressed in future iterations of such hybrid design processes.

</details>


### [8] [NL in the Middle: Code Translation with LLMs and Intermediate Representations](https://arxiv.org/abs/2507.08627)
*Chi-en Amy Tai,Pengyu Nie,Lukasz Golab,Alexander Wong*

Main category: cs.SE

TL;DR: 研究发现，大型语言模型（LLM）在代码翻译中容易出错。通过自然语言（NL）和抽象语法树（AST）的中间表示，可以提高翻译准确性。实验表明，链式思维（CoT）提示结合NL摘要效果最佳，Open Gpt4 8X7B模型的翻译成功率提升了13.8%和6.7%。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在代码翻译中的错误问题，探索中间表示对翻译准确性的提升作用。

Method: 使用NL和AST作为中间表示，尝试从零样本到链式思维（CoT）提示的不同方法，在Open Gpt4 8X7B、StarCoder和CodeGen模型上进行实验。

Result: CoT结合NL摘要的提示方法效果最好，Open Gpt4 8X7B模型的翻译成功率分别提升了13.8%和6.7%。

Conclusion: 中间表示（尤其是NL摘要）结合CoT提示能显著提升LLM的代码翻译准确性。

Abstract: Studies show that large language models (LLMs) produce buggy code
translations. One avenue to improve translation accuracy is through
intermediate representations, which could provide structured insights to guide
the model's understanding. We explore whether code translation using LLMs can
benefit from intermediate representations via natural language (NL) and
abstract syntax trees (ASTs). Since prompt engineering greatly affects LLM
performance, we consider several ways to integrate these representations, from
one-shot to chain-of-thought (CoT) prompting. Using Open Gpt4 8X7B and
specialized StarCoder and CodeGen models on popular code translation benchmarks
(CodeNet and AVATAR), we find that CoT with an intermediate NL summary performs
best, with an increase of 13.8% and 6.7%, respectively, in successful
translations for the best-performing model (Open Gpt4 8X7B) compared to the
zero-shot prompt.

</details>


### [9] [LLMCup: Ranking-Enhanced Comment Updating with LLMs](https://arxiv.org/abs/2507.08671)
*Hua Ge,Juan Zhai,Minxue Pan,Fusen He,Ziyue Tan*

Main category: cs.SE

TL;DR: 论文提出了一种名为LLMCup的新框架，利用大型语言模型（LLM）和排名模型CupRank来自动更新代码注释，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 开发者常更新代码但忽略注释，导致注释过时或不一致。现有方法（如CUP和HebCup）在复杂场景下表现不佳，因此需要更高效的解决方案。

Method: LLMCup结合多种提示策略生成候选注释，并通过排名模型CupRank选择最佳注释。

Result: 实验显示LLMCup在准确性、BLEU-4等指标上显著优于基线方法，部分情况下甚至优于人工更新。

Conclusion: LLMCup展示了LLM在注释更新任务中的潜力，并强调了人工评估的重要性。

Abstract: While comments are essential for enhancing code readability and
maintainability in modern software projects, developers are often motivated to
update code but not comments, leading to outdated or inconsistent documentation
that hinders future understanding and maintenance. Recent approaches such as
CUP and HebCup have attempted automatic comment updating using neural
sequence-to-sequence models and heuristic rules, respectively. However, these
methods can miss or misinterpret crucial information during comment updating,
resulting in inaccurate comments, and they often struggle with complex update
scenarios. Given these challenges, a promising direction lies in leveraging
large language models (LLMs), which have shown impressive performance in
software engineering tasks such as comment generation, code synthesis, and
program repair. This suggests their strong potential to capture the logic
behind code modifications - an ability that is crucial for the task of comment
updating. Nevertheless, selecting an appropriate prompt strategy for an LLM on
each update case remains challenging. To address this, we propose a novel
comment updating framework, LLMCup, which first uses multiple prompt strategies
to provide diverse candidate updated comments via an LLM, and then employs a
ranking model, CupRank, to select the best candidate as final updated comment.
Experimental results demonstrate the effectiveness of LLMCup, with improvements
over state-of-the-art baselines (CUP and HebCup) by 49.0%-116.9% in Accuracy,
10.8%-20% in BLEU-4, 4.6% in METEOR, 0.9%-1.9% in F1, and 2.1%-3.4% in
SentenceBert similarity. Furthermore, a user study shows that comments updated
by LLMCup sometimes surpass human-written updates, highlighting the importance
of incorporating human evaluation in comment quality assessment.

</details>


### [10] [Dually Hierarchical Drift Adaptation for Online Configuration Performance Learning](https://arxiv.org/abs/2507.08730)
*Zezhen Xiang,Jingzhi Gong,Tao Chen*

Main category: cs.SE

TL;DR: DHDA是一个在线配置性能学习框架，旨在动态环境中捕捉和适应全局和局部概念漂移，通过双重层次化适应实现高效更新。


<details>
  <summary>Details</summary>
Motivation: 现代可配置软件系统在动态环境中运行时，工作负载变化、硬件更新等会导致概念漂移，现有离线或迁移学习方法难以实时适应这些变化。

Method: DHDA采用双重层次化适应：上层通过重新划分数据并局部重训练处理全局漂移；下层通过局部模型异步检测和适应局部漂移，结合增量更新和定期全重训练以平衡效率。

Result: 在八个软件系统中评估，DHDA显著提升了准确性，适应漂移能力提高2倍，同时保持合理开销。

Conclusion: DHDA能有效处理动态环境中的概念漂移，显著优于现有方法，适用于实时配置性能学习。

Abstract: Modern configurable software systems need to learn models that correlate
configuration and performance. However, when the system operates in dynamic
environments, the workload variations, hardware changes, and system updates
will inevitably introduce concept drifts at different levels - global drifts,
which reshape the performance landscape of the entire configuration space; and
local drifts, which only affect certain sub-regions of that space. As such,
existing offline and transfer learning approaches can struggle to adapt to
these implicit and unpredictable changes in real-time, rendering configuration
performance learning challenging. To address this, we propose DHDA, an online
configuration performance learning framework designed to capture and adapt to
these drifts at different levels. The key idea is that DHDA adapts to both the
local and global drifts using dually hierarchical adaptation: at the upper
level, we redivide the data into different divisions, within each of which the
local model is retrained, to handle global drifts only when necessary. At the
lower level, the local models of the divisions can detect local drifts and
adapt themselves asynchronously. To balance responsiveness and efficiency, DHDA
combines incremental updates with periodic full retraining to minimize
redundant computation when no drifts are detected. Through evaluating eight
software systems and against state-of-the-art approaches, we show that DHDA
achieves considerably better accuracy and can effectively adapt to drifts with
up to 2x improvements, while incurring reasonable overhead and is able to
improve different local models in handling concept drift.

</details>
