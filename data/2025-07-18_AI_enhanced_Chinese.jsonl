{"id": "2507.12472", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12472", "abs": "https://arxiv.org/abs/2507.12472", "authors": ["Lingzhe Zhang", "Tong Jia", "Mengxi Jia", "Yifan Wu", "Aiwei Liu", "Yong Yang", "Zhonghai Wu", "Xuming Hu", "Philip S. Yu", "Ying Li"], "title": "A Survey of AIOps in the Era of Large Language Models", "comment": "Accepted By CSUR, an extended version of \"A Survey of AIOps for\n  Failure Management in the Era of Large Language Models\" [arXiv:2406.11213]", "summary": "As large language models (LLMs) grow increasingly sophisticated and\npervasive, their application to various Artificial Intelligence for IT\nOperations (AIOps) tasks has garnered significant attention. However, a\ncomprehensive understanding of the impact, potential, and limitations of LLMs\nin AIOps remains in its infancy. To address this gap, we conducted a detailed\nsurvey of LLM4AIOps, focusing on how LLMs can optimize processes and improve\noutcomes in this domain. We analyzed 183 research papers published between\nJanuary 2020 and December 2024 to answer four key research questions (RQs). In\nRQ1, we examine the diverse failure data sources utilized, including advanced\nLLM-based processing techniques for legacy data and the incorporation of new\ndata sources enabled by LLMs. RQ2 explores the evolution of AIOps tasks,\nhighlighting the emergence of novel tasks and the publication trends across\nthese tasks. RQ3 investigates the various LLM-based methods applied to address\nAIOps challenges. Finally, RQ4 reviews evaluation methodologies tailored to\nassess LLM-integrated AIOps approaches. Based on our findings, we discuss the\nstate-of-the-art advancements and trends, identify gaps in existing research,\nand propose promising directions for future exploration.", "AI": {"tldr": "\u672c\u6587\u662f\u4e00\u7bc7\u5173\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u5728IT\u8fd0\u7ef4\u4eba\u5de5\u667a\u80fd(AIOps)\u9886\u57df\u5e94\u7528\u7684\u7efc\u5408\u8c03\u7814\uff0c\u901a\u8fc7\u5206\u6790183\u7bc72020-2024\u5e74\u53d1\u8868\u7684\u8bba\u6587\uff0c\u4ece\u6545\u969c\u6570\u636e\u6e90\u3001\u4efb\u52a1\u6f14\u5316\u3001\u65b9\u6cd5\u5e94\u7528\u548c\u8bc4\u4f30\u65b9\u6cd5\u56db\u4e2a\u7ef4\u5ea6\u5168\u9762\u63a2\u8ba8\u4e86LLM4AIOps\u7684\u73b0\u72b6\u3001\u6f5c\u529b\u548c\u5c40\u9650\u6027\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u65e5\u76ca\u590d\u6742\u548c\u666e\u53ca\uff0c\u5176\u5728AIOps\u5404\u79cd\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u5f15\u8d77\u4e86\u5e7f\u6cdb\u5173\u6ce8\uff0c\u4f46\u5bf9LLM\u5728AIOps\u9886\u57df\u7684\u5f71\u54cd\u3001\u6f5c\u529b\u548c\u5c40\u9650\u6027\u7f3a\u4e4f\u5168\u9762\u7406\u89e3\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u8c03\u7814\u6765\u586b\u8865\u8fd9\u4e00\u77e5\u8bc6\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u8be6\u7ec6\u8c03\u7814\u5206\u67902020\u5e741\u6708\u81f32024\u5e7412\u6708\u671f\u95f4\u53d1\u8868\u7684183\u7bc7\u7814\u7a76\u8bba\u6587\uff0c\u56f4\u7ed5\u56db\u4e2a\u5173\u952e\u7814\u7a76\u95ee\u9898\u8fdb\u884c\u7cfb\u7edf\u5206\u6790\uff1aRQ1\u63a2\u8ba8\u591a\u6837\u5316\u6545\u969c\u6570\u636e\u6e90\u7684\u5229\u7528\uff1bRQ2\u5206\u6790AIOps\u4efb\u52a1\u7684\u6f14\u5316\uff1bRQ3\u7814\u7a76\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\uff1bRQ4\u5ba1\u67e5\u8bc4\u4f30\u65b9\u6cd5\u8bba\u3002", "result": "\u8bc6\u522b\u4e86LLM\u5728AIOps\u4e2d\u7684\u591a\u79cd\u6570\u636e\u6e90\u5904\u7406\u6280\u672f\uff0c\u5305\u62ec\u5bf9\u4f20\u7edf\u6570\u636e\u7684\u9ad8\u7ea7LLM\u5904\u7406\u548c\u65b0\u6570\u636e\u6e90\u7684\u6574\u5408\uff1b\u53d1\u73b0\u4e86\u65b0\u5174AIOps\u4efb\u52a1\u548c\u53d1\u8868\u8d8b\u52bf\uff1b\u603b\u7ed3\u4e86\u5404\u79cdLLM\u65b9\u6cd5\u5728\u89e3\u51b3AIOps\u6311\u6218\u4e2d\u7684\u5e94\u7528\uff1b\u68b3\u7406\u4e86\u4e13\u95e8\u9488\u5bf9LLM\u96c6\u6210AIOps\u65b9\u6cd5\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "conclusion": "\u57fa\u4e8e\u8c03\u7814\u53d1\u73b0\uff0c\u8ba8\u8bba\u4e86\u6700\u65b0\u8fdb\u5c55\u548c\u8d8b\u52bf\uff0c\u8bc6\u522b\u4e86\u73b0\u6709\u7814\u7a76\u7684\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u63a2\u7d22\u7684\u6709\u524d\u666f\u65b9\u5411\uff0c\u4e3aLLM4AIOps\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2507.12480", "categories": ["cs.SE", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2507.12480", "abs": "https://arxiv.org/abs/2507.12480", "authors": ["Nazanin Siavash", "Armin Moin"], "title": "LLM-Powered Quantum Code Transpilation", "comment": "IEEE International Conference on Quantum Computing and Engineering\n  (QCE) 2025 - Extended Abstract", "summary": "There exist various Software Development Kits (SDKs) tailored to different\nquantum computing platforms. These are known as Quantum SDKs (QSDKs). Examples\ninclude but are not limited to Qiskit, Cirq, and PennyLane. However, this\ndiversity presents significant challenges for interoperability and\ncross-platform development of hybrid quantum-classical software systems.\nTraditional rule-based transpilers for translating code between QSDKs are\ntime-consuming to design and maintain, requiring deep expertise and rigid\nmappings in the source and destination code. In this study, we explore the use\nof Large Language Models (LLMs) as a flexible and automated solution.\nLeveraging their pretrained knowledge and contextual reasoning capabilities, we\nposition LLMs as programming language-agnostic transpilers capable of\nconverting quantum programs from one QSDK to another while preserving\nfunctional equivalence. Our approach eliminates the need for manually defined\ntransformation rules and offers a scalable solution to quantum software\nportability. This work represents a step toward enabling intelligent,\ngeneral-purpose transpilation in the quantum computing ecosystem.", "AI": {"tldr": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4f5c\u4e3a\u91cf\u5b50\u8f6f\u4ef6\u5f00\u53d1\u5de5\u5177\u5305\uff08QSDKs\uff09\u95f4\u7684\u81ea\u52a8\u8f6c\u8bd1\u5668\uff0c\u89e3\u51b3\u8de8\u5e73\u53f0\u5f00\u53d1\u7684\u4e92\u64cd\u4f5c\u6027\u95ee\u9898\u3002", "motivation": "\u91cf\u5b50\u8ba1\u7b97\u5e73\u53f0\u591a\u6837\u6027\u5bfc\u81f4QSDKs\u95f4\u4e92\u64cd\u4f5c\u6027\u5dee\uff0c\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u7684\u8f6c\u8bd1\u5668\u8bbe\u8ba1\u7ef4\u62a4\u6210\u672c\u9ad8\u3002", "method": "\u5229\u7528LLMs\u7684\u9884\u8bad\u7ec3\u77e5\u8bc6\u548c\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\uff0c\u5b9e\u73b0\u7f16\u7a0b\u8bed\u8a00\u65e0\u5173\u7684\u91cf\u5b50\u7a0b\u5e8f\u8f6c\u8bd1\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u624b\u52a8\u5b9a\u4e49\u8f6c\u6362\u89c4\u5219\u7684\u53ef\u6269\u5c55\u91cf\u5b50\u8f6f\u4ef6\u79fb\u690d\u65b9\u6848\u3002", "conclusion": "LLMs\u4e3a\u91cf\u5b50\u8ba1\u7b97\u751f\u6001\u4e2d\u7684\u667a\u80fd\u901a\u7528\u8f6c\u8bd1\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2507.12482", "categories": ["cs.SE", "cs.AI", "cs.CE", "cs.LG", "68N30, 68T05, 68T50", "D.2.5; D.2.7; F.3.2; I.2.6; I.2.7"], "pdf": "https://arxiv.org/pdf/2507.12482", "abs": "https://arxiv.org/abs/2507.12482", "authors": ["Ishraq Khan", "Assad Chowdary", "Sharoz Haseeb", "Urvish Patel"], "title": "Kodezi Chronos: A Debugging-First Language Model for Repository-Scale, Memory-Driven Code Understanding", "comment": "10 pages, 10 figures, 7 tables, IEEE Conference format, Q4 2025 model\n  release, Q1 2026 Kodezi OS deployment", "summary": "Large Language Models (LLMs) have advanced code generation and software\nautomation, but are fundamentally constrained by limited inference-time context\nand lack of explicit code structure reasoning. We introduce Kodezi Chronos, a\nnext-generation architecture for autonomous code understanding, debugging, and\nmaintenance, designed to operate across ultra-long contexts comprising entire\ncodebases, histories, and documentation, all without fixed window limits.\nKodezi Chronos leverages a multi-level embedding memory engine, combining\nvector and graph-based indexing with continuous code-aware retrieval. This\nenables efficient and accurate reasoning over millions of lines of code,\nsupporting repository-scale comprehension, multi-file refactoring, and\nreal-time self-healing actions. Our evaluation introduces a novel Multi Random\nRetrieval benchmark, specifically tailored to the software engineering domain.\nUnlike classical retrieval benchmarks, this method requires the model to\nresolve arbitrarily distant and obfuscated associations across code artifacts,\nsimulating realistic tasks such as variable tracing, dependency migration, and\nsemantic bug localization. Chronos outperforms prior LLMs and code models,\ndemonstrating a 23% improvement in real-world bug detection and reducing\ndebugging cycles by up to 40% compared to traditional sequence-based\napproaches. By natively interfacing with IDEs and CI/CD workflows, Chronos\nenables seamless, autonomous software maintenance, elevating code reliability\nand productivity while reducing manual effort. These results mark a critical\nadvance toward self-sustaining, continuously optimized software ecosystems.", "AI": {"tldr": "Kodezi Chronos\u662f\u4e00\u79cd\u65b0\u578b\u67b6\u6784\uff0c\u7528\u4e8e\u81ea\u4e3b\u4ee3\u7801\u7406\u89e3\u3001\u8c03\u8bd5\u548c\u7ef4\u62a4\uff0c\u652f\u6301\u8d85\u957f\u4e0a\u4e0b\u6587\uff0c\u663e\u8457\u63d0\u5347\u4ee3\u7801\u53ef\u9760\u6027\u548c\u751f\u4ea7\u529b\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u548c\u8f6f\u4ef6\u81ea\u52a8\u5316\u4e2d\u56e0\u4e0a\u4e0b\u6587\u9650\u5236\u548c\u7f3a\u4e4f\u663e\u5f0f\u4ee3\u7801\u7ed3\u6784\u63a8\u7406\u800c\u53d7\u5230\u7684\u6839\u672c\u7ea6\u675f\u3002", "method": "\u91c7\u7528\u591a\u7ea7\u5d4c\u5165\u5185\u5b58\u5f15\u64ce\uff0c\u7ed3\u5408\u5411\u91cf\u548c\u56fe\u7d22\u5f15\uff0c\u5b9e\u73b0\u9ad8\u6548\u51c6\u786e\u7684\u4ee3\u7801\u63a8\u7406\uff0c\u652f\u6301\u4ed3\u5e93\u7ea7\u7406\u89e3\u548c\u5b9e\u65f6\u81ea\u4fee\u590d\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u9519\u8bef\u68c0\u6d4b\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u63d0\u534723%\uff0c\u8c03\u8bd5\u5468\u671f\u51cf\u5c1140%\u3002", "conclusion": "Kodezi Chronos\u6807\u5fd7\u7740\u5411\u81ea\u6301\u7eed\u3001\u6301\u7eed\u4f18\u5316\u7684\u8f6f\u4ef6\u751f\u6001\u7cfb\u7edf\u7684\u5173\u952e\u8fdb\u5c55\u3002"}}
{"id": "2507.12483", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.12483", "abs": "https://arxiv.org/abs/2507.12483", "authors": ["Dong Wang", "Hanmo You", "Lingwei Zhu", "Kaiwei Lin", "Zheng Chen", "Chen Yang", "Junji Yu", "Zan Wang", "Junjie Chen"], "title": "A Survey of Reinforcement Learning for Software Engineering", "comment": null, "summary": "Reinforcement Learning (RL) has emerged as a powerful paradigm for sequential\ndecision-making and has attracted growing interest across various domains,\nparticularly following the advent of Deep Reinforcement Learning (DRL) in 2015.\nSimultaneously, the rapid advancement of Large Language Models (LLMs) has\nfurther fueled interest in integrating RL with LLMs to enable more adaptive and\nintelligent systems. In the field of software engineering (SE), the increasing\ncomplexity of systems and the rising demand for automation have motivated\nresearchers to apply RL to a broad range of tasks, from software design and\ndevelopment to quality assurance and maintenance. Despite growing research in\nRL-for-SE, there remains a lack of a comprehensive and systematic survey of\nthis evolving field. To address this gap, we reviewed 115 peer-reviewed studies\npublished across 22 premier SE venues since the introduction of DRL. We\nconducted a comprehensive analysis of publication trends, categorized SE topics\nand RL algorithms, and examined key factors such as dataset usage, model design\nand optimization, and evaluation practices. Furthermore, we identified open\nchallenges and proposed future research directions to guide and inspire ongoing\nwork in this evolving area. To summarize, this survey offers the first\nsystematic mapping of RL applications in software engineering, aiming to\nsupport both researchers and practitioners in navigating the current landscape\nand advancing the field. Our artifacts are publicly available:\nhttps://github.com/KaiWei-Lin-lanina/RL4SE.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5bf9\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728\u8f6f\u4ef6\u5de5\u7a0b\uff08SE\uff09\u4e2d\u7684\u5e94\u7528\u8fdb\u884c\u4e86\u9996\u6b21\u7cfb\u7edf\u6027\u7efc\u8ff0\uff0c\u5206\u6790\u4e86115\u7bc7\u7814\u7a76\uff0c\u603b\u7ed3\u4e86\u8d8b\u52bf\u3001\u7b97\u6cd5\u5206\u7c7b\u53ca\u6311\u6218\u3002", "motivation": "\u968f\u7740RL\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u53d1\u5c55\uff0cSE\u9886\u57df\u7684\u590d\u6742\u6027\u589e\u52a0\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u7f3a\u4e4f\u5bf9RL-for-SE\u7684\u5168\u9762\u8c03\u67e5\u3002", "method": "\u7efc\u8ff0\u4e86115\u7bc7\u540c\u884c\u8bc4\u5ba1\u7814\u7a76\uff0c\u5206\u6790\u4e86\u53d1\u8868\u8d8b\u52bf\u3001SE\u4e3b\u9898\u5206\u7c7b\u3001RL\u7b97\u6cd5\u3001\u6570\u636e\u96c6\u4f7f\u7528\u3001\u6a21\u578b\u8bbe\u8ba1\u548c\u8bc4\u4f30\u5b9e\u8df5\u3002", "result": "\u603b\u7ed3\u4e86RL\u5728SE\u4e2d\u7684\u5e94\u7528\u73b0\u72b6\uff0c\u8bc6\u522b\u4e86\u5f00\u653e\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u8be5\u8c03\u67e5\u4e3a\u7814\u7a76\u8005\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86RL-for-SE\u7684\u7cfb\u7edf\u6027\u6307\u5357\uff0c\u5e76\u516c\u5f00\u4e86\u76f8\u5173\u8d44\u6e90\u3002"}}
{"id": "2507.12558", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.12558", "abs": "https://arxiv.org/abs/2507.12558", "authors": ["Tien P. T. Le", "Anh M. T. Bui", "Huy N. D. Pham", "Alessio Bucaioni", "Phuong T. Nguyen"], "title": "When Retriever Meets Generator: A Joint Model for Code Comment Generation", "comment": "The paper has been peer-reviewed and accepted for publication in the\n  proceedings of the 19th ACM/IEEE International Symposium on Empirical\n  Software Engineering and Measurement (ESEM 2025)", "summary": "Automatically generating concise, informative comments for source code can\nlighten documentation effort and accelerate program comprehension.\nRetrieval-augmented approaches first fetch code snippets with existing comments\nand then synthesize a new comment, yet retrieval and generation are typically\noptimized in isolation, allowing irrelevant neighbors topropagate noise\ndownstream. To tackle the issue, we propose a novel approach named RAGSum with\nthe aim of both effectiveness and efficiency in recommendations. RAGSum is\nbuilt on top offuse retrieval and generation using a single CodeT5 backbone. We\nreport preliminary results on a unified retrieval-generation framework built on\nCodeT5. A contrastive pre-training phase shapes code embeddings for\nnearest-neighbor search; these weights then seed end-to-end training with a\ncomposite loss that (i) rewards accurate top-k retrieval; and (ii) minimizes\ncomment-generation error. More importantly, a lightweight self-refinement loop\nis deployed to polish the final output. We evaluated theframework on three\ncross-language benchmarks (Java, Python, C), and compared it with three\nwell-established baselines. The results show that our approach substantially\noutperforms thebaselines with respect to BLEU, METEOR, and ROUTE-L. These\nfindings indicate that tightly coupling retrieval and generationcan raise the\nceiling for comment automation and motivateforthcoming replications and\nqualitative developer studies.", "AI": {"tldr": "RAGSum\u662f\u4e00\u79cd\u7ed3\u5408\u68c0\u7d22\u4e0e\u751f\u6210\u7684\u4ee3\u7801\u6ce8\u91ca\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7CodeT5\u6846\u67b6\u5b9e\u73b0\u9ad8\u6548\u63a8\u8350\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u68c0\u7d22\u589e\u5f3a\u65b9\u6cd5\u4e2d\u68c0\u7d22\u4e0e\u751f\u6210\u5206\u79bb\u5bfc\u81f4\u7684\u566a\u58f0\u4f20\u64ad\u95ee\u9898\uff0c\u63d0\u5347\u4ee3\u7801\u6ce8\u91ca\u751f\u6210\u7684\u6548\u7387\u4e0e\u8d28\u91cf\u3002", "method": "\u57fa\u4e8eCodeT5\u6846\u67b6\uff0c\u7ed3\u5408\u5bf9\u6bd4\u9884\u8bad\u7ec3\u548c\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u4f7f\u7528\u590d\u5408\u635f\u5931\u51fd\u6570\u4f18\u5316\u68c0\u7d22\u4e0e\u751f\u6210\uff0c\u5e76\u5f15\u5165\u8f7b\u91cf\u7ea7\u81ea\u4f18\u5316\u5faa\u73af\u3002", "result": "\u5728Java\u3001Python\u3001C\u4e09\u79cd\u8bed\u8a00\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRAGSum\u5728BLEU\u3001METEOR\u548cROUTE-L\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u7d27\u5bc6\u8026\u5408\u68c0\u7d22\u4e0e\u751f\u6210\u80fd\u63d0\u5347\u6ce8\u91ca\u81ea\u52a8\u5316\u7684\u4e0a\u9650\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u548c\u5f00\u5c55\u5f00\u53d1\u8005\u7814\u7a76\u3002"}}
{"id": "2507.12561", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.12561", "abs": "https://arxiv.org/abs/2507.12561", "authors": ["Samal Nursapa", "Anastassiya Samuilova", "Alessio Bucaioni. Phuong T. Nguyen"], "title": "ROSE: Transformer-Based Refactoring Recommendation for Architectural Smells", "comment": "The paper has been peer-reviewed and accepted for publication in the\n  proceedings of the 19th ACM/IEEE International Symposium on Empirical\n  Software Engineering and Measurement (ESEM 2025)", "summary": "Architectural smells such as God Class, Cyclic Dependency, and Hub-like\nDependency degrade software quality and maintainability. Existing tools detect\nsuch smells but rarely suggest how to fix them. This paper explores the use of\npre-trained transformer models--CodeBERT and CodeT5--for recommending suitable\nrefactorings based on detected smells. We frame the task as a three-class\nclassification problem and fine-tune both models on over 2 million refactoring\ninstances mined from 11,149 open-source Java projects. CodeT5 achieves 96.9%\naccuracy and 95.2% F1, outperforming CodeBERT and traditional baselines. Our\nresults show that transformer-based models can effectively bridge the gap\nbetween smell detection and actionable repair, laying the foundation for future\nrefactoring recommendation systems. We release all code, models, and data under\nan open license to support reproducibility and further research.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528\u9884\u8bad\u7ec3Transformer\u6a21\u578b\uff08CodeBERT\u548cCodeT5\uff09\u4e3a\u68c0\u6d4b\u5230\u7684\u67b6\u6784\u5f02\u5473\u63a8\u8350\u91cd\u6784\u65b9\u6cd5\uff0cCodeT5\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u73b0\u6709\u5de5\u5177\u80fd\u68c0\u6d4b\u67b6\u6784\u5f02\u5473\u4f46\u7f3a\u4e4f\u4fee\u590d\u5efa\u8bae\uff0c\u9700\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5c06\u4efb\u52a1\u5b9a\u4e49\u4e3a\u4e09\u5206\u7c7b\u95ee\u9898\uff0c\u572811000\u591a\u4e2a\u5f00\u6e90Java\u9879\u76ee\u7684200\u591a\u4e07\u4e2a\u91cd\u6784\u5b9e\u4f8b\u4e0a\u5fae\u8c03\u6a21\u578b\u3002", "result": "CodeT5\u8fbe\u523096.9%\u51c6\u786e\u7387\u548c95.2% F1\u5206\u6570\uff0c\u4f18\u4e8eCodeBERT\u548c\u4f20\u7edf\u57fa\u7ebf\u3002", "conclusion": "Transformer\u6a21\u578b\u80fd\u6709\u6548\u8fde\u63a5\u5f02\u5473\u68c0\u6d4b\u4e0e\u4fee\u590d\uff0c\u4e3a\u672a\u6765\u91cd\u6784\u63a8\u8350\u7cfb\u7edf\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2507.12642", "categories": ["cs.SE", "cs.AI", "quant-ph"], "pdf": "https://arxiv.org/pdf/2507.12642", "abs": "https://arxiv.org/abs/2507.12642", "authors": ["Kiana Kheiri", "Aamna Aamir", "Andriy Miranskyy", "Chen Ding"], "title": "QSpark: Towards Reliable Qiskit Code Generation", "comment": null, "summary": "Quantum circuits must be error-resilient, yet LLMs like Granite-20B-Code and\nStarCoder often output flawed Qiskit code. We fine-tuned a 32 B model with two\nRL methods, Group Relative Policy Optimization (GRPO) and Odds-Ratio Preference\nOptimization (ORPO), using a richly annotated synthetic dataset. On the Qiskit\nHumanEval benchmark, ORPO reaches 56.29\\% Pass@1 ($\\approx+10$ pp over\nGranite-8B-QK) and GRPO hits 49\\%, both beating all general-purpose baselines;\non the original HumanEval they score 65.90\\% and 63.00\\%. GRPO excels on basic\ntasks (42/54), ORPO on intermediate ones (41/68), and neither solves the five\nadvanced tasks, highlighting clear gains yet room for progress in AI-assisted\nquantum programming.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5982\u4f55\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08GRPO\u548cORPO\uff09\u4f18\u5316\u91cf\u5b50\u7f16\u7a0b\u6a21\u578b\uff0c\u5728Qiskit HumanEval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u901a\u7528\u57fa\u7ebf\u6a21\u578b\uff0c\u4f46\u5728\u9ad8\u7ea7\u4efb\u52a1\u4e0a\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "motivation": "\u91cf\u5b50\u7535\u8def\u9700\u8981\u5177\u5907\u5bb9\u9519\u80fd\u529b\uff0c\u4f46\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\uff08\u5982Granite-20B-Code\u548cStarCoder\uff09\u751f\u6210\u7684Qiskit\u4ee3\u7801\u5b58\u5728\u7f3a\u9677\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u3002", "method": "\u4f7f\u7528\u4e24\u79cd\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08GRPO\u548cORPO\uff09\u5bf932B\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u5229\u7528\u4e30\u5bcc\u7684\u5408\u6210\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728Qiskit HumanEval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cORPO\u8fbe\u523056.29% Pass@1\uff0cGRPO\u4e3a49%\uff0c\u5747\u4f18\u4e8e\u901a\u7528\u57fa\u7ebf\uff1b\u5728\u539f\u59cbHumanEval\u4e2d\u5206\u522b\u4e3a65.90%\u548c63.00%\u3002GRPO\u5728\u57fa\u7840\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u597d\uff0842/54\uff09\uff0cORPO\u5728\u4e2d\u7b49\u4efb\u52a1\u4e2d\u66f4\u4f18\uff0841/68\uff09\uff0c\u4f46\u5747\u672a\u80fd\u89e3\u51b3\u9ad8\u7ea7\u4efb\u52a1\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728AI\u8f85\u52a9\u91cf\u5b50\u7f16\u7a0b\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4f46\u5728\u9ad8\u7ea7\u4efb\u52a1\u4e0a\u4ecd\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002"}}
{"id": "2507.12649", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.12649", "abs": "https://arxiv.org/abs/2507.12649", "authors": ["Christine van Stiphoudt", "Sergio Potenciano Menci", "Gilbert Fridgen"], "title": "A Three-Phase Evaluation Approach for new Information and Data Models in the Smart Grid Domain", "comment": null, "summary": "The ongoing digitalisation of the smart grid is resulting in an increase in\nautomated information exchanges across distributed energy systems. This process\nhas led to the development of new information and data models when the existing\nones fall short. To prevent potential disruptions caused by flaws in the newly\ndesigned information and data models, it is essential to evaluate them during\nthe design process before they are implemented in operation.\n  Currently, general explicit evaluation approaches outside the smart grid\ndomain stay at a high level without defining clear steps. Meanwhile, implicit\nevaluation approaches in the smart grid domain focus on testing systems that\nutilise information and data models already in use for functionality in terms\nof conformance and interoperability. Notably, no combination of explicit and\nimplicit evaluation approaches for newly designed information and data models\noffers a clearly defined set of steps during their design process in the smart\ngrid context.\n  Consequently, we design a three-phase evaluation approach using design\nscience research to address this gap. Our evaluation approach combines explicit\nand implicit evaluation methods and is applicable when developing new\ninformation and data models. We use the development of an information model and\ndata model focused on industrial flexibility descriptions to refine our\nevaluation approach. Additionally, we provide lessons learned from our\nexperience.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u663e\u6027\u548c\u9690\u6027\u8bc4\u4f30\u65b9\u6cd5\u7684\u4e09\u9636\u6bb5\u8bc4\u4f30\u65b9\u6cd5\uff0c\u7528\u4e8e\u667a\u80fd\u7535\u7f51\u4e2d\u65b0\u8bbe\u8ba1\u7684\u4fe1\u606f\u548c\u6570\u636e\u6a21\u578b\u7684\u5f00\u53d1\u8fc7\u7a0b\u3002", "motivation": "\u667a\u80fd\u7535\u7f51\u6570\u5b57\u5316\u5bfc\u81f4\u4fe1\u606f\u4ea4\u6362\u589e\u52a0\uff0c\u73b0\u6709\u6a21\u578b\u4e0d\u8db3\uff0c\u9700\u5728\u5b9e\u65bd\u524d\u8bc4\u4f30\u65b0\u6a21\u578b\u4ee5\u907f\u514d\u6f5c\u5728\u95ee\u9898\u3002", "method": "\u91c7\u7528\u8bbe\u8ba1\u79d1\u5b66\u7814\u7a76\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7ed3\u5408\u663e\u6027\u548c\u9690\u6027\u8bc4\u4f30\u7684\u4e09\u9636\u6bb5\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u4ee5\u5de5\u4e1a\u7075\u6d3b\u6027\u63cf\u8ff0\u7684\u4fe1\u606f\u548c\u6570\u636e\u6a21\u578b\u4e3a\u4f8b\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u65b0\u6a21\u578b\u5f00\u53d1\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9645\u6848\u4f8b\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u586b\u8865\u4e86\u667a\u80fd\u7535\u7f51\u9886\u57df\u65b0\u6a21\u578b\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u7ecf\u9a8c\u6559\u8bad\u3002"}}
{"id": "2507.12653", "categories": ["cs.SE", "cs.CL", "H.4.m"], "pdf": "https://arxiv.org/pdf/2507.12653", "abs": "https://arxiv.org/abs/2507.12653", "authors": ["Jo\u00e3o Granja-Correia", "Remedios Hern\u00e1ndez-Linares", "Luca Ferranti", "Arm\u00e9nio Rego"], "title": "A Fuzzy Approach to Project Success: Measuring What Matters", "comment": "3 pages, 1 figure, presented at FUZZ-IEEE 2025", "summary": "This paper introduces a novel approach to project success evaluation by\nintegrating fuzzy logic into an existing construct. Traditional Likert-scale\nmeasures often overlook the context-dependent and multifaceted nature of\nproject success. The proposed hierarchical Type-1 Mamdani fuzzy system\nprioritizes sustained positive impact for end-users, reducing emphasis on\nsecondary outcomes like stakeholder satisfaction and internal project success.\nThis dynamic approach may provide a more accurate measure of project success\nand could be adaptable to complex evaluations. Future research will focus on\nempirical testing and broader applications of fuzzy logic in social science.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u7cca\u903b\u8f91\u7684\u9879\u76ee\u6210\u529f\u8bc4\u4f30\u65b0\u65b9\u6cd5\uff0c\u4f18\u4e8e\u4f20\u7edf\u7684Likert\u91cf\u8868\u3002", "motivation": "\u4f20\u7edfLikert\u91cf\u8868\u5ffd\u89c6\u4e86\u9879\u76ee\u6210\u529f\u7684\u591a\u7ef4\u6027\u548c\u60c5\u5883\u4f9d\u8d56\u6027\uff0c\u9700\u8981\u66f4\u52a8\u6001\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u5206\u5c42Type-1 Mamdani\u6a21\u7cca\u7cfb\u7edf\uff0c\u4f18\u5148\u8003\u8651\u5bf9\u7ec8\u7aef\u7528\u6237\u7684\u6301\u7eed\u79ef\u6781\u5f71\u54cd\u3002", "result": "\u8be5\u65b9\u6cd5\u53ef\u80fd\u66f4\u51c6\u786e\u5730\u8861\u91cf\u9879\u76ee\u6210\u529f\uff0c\u5e76\u9002\u7528\u4e8e\u590d\u6742\u8bc4\u4f30\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u5c06\u805a\u7126\u4e8e\u6a21\u7cca\u903b\u8f91\u5728\u793e\u4f1a\u79d1\u5b66\u4e2d\u7684\u5b9e\u8bc1\u6d4b\u8bd5\u548c\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2507.12665", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.12665", "abs": "https://arxiv.org/abs/2507.12665", "authors": ["Salvador D. Escobedo"], "title": "Single Conversation Methodology: A Human-Centered Protocol for AI-Assisted Software Development", "comment": "Style reviewed by a LLM for improving clarity and English syntax", "summary": "We propose the Single Conversation Methodology (SCM), a novel and pragmatic\napproach to software development using large language models (LLMs). In\ncontrast to ad hoc interactions with generative AI, SCM emphasizes a structured\nand persistent development dialogue, where all stages of a project - from\nrequirements to architecture and implementation - unfold within a single,\nlong-context conversation. The methodology is grounded on principles of\ncognitive clarity, traceability, modularity, and documentation. We define its\nphases, best practices, and philosophical stance, while arguing that SCM offers\na necessary correction to the passive reliance on LLMs prevalent in current\npractices. We aim to reassert the active role of the developer as architect and\nsupervisor of the intelligent tool.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSCM\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u4e00\u5bf9\u8bdd\u7ed3\u6784\u5229\u7528LLMs\u8fdb\u884c\u8f6f\u4ef6\u5f00\u53d1\uff0c\u5f3a\u8c03\u7ed3\u6784\u5316\u5bf9\u8bdd\u548c\u5f00\u53d1\u8005\u4e3b\u5bfc\u3002", "motivation": "\u7ea0\u6b63\u5f53\u524d\u5bf9LLMs\u7684\u88ab\u52a8\u4f9d\u8d56\uff0c\u5f3a\u8c03\u5f00\u53d1\u8005\u5728AI\u5de5\u5177\u4e2d\u7684\u4e3b\u52a8\u89d2\u8272\u3002", "method": "\u57fa\u4e8e\u8ba4\u77e5\u6e05\u6670\u6027\u3001\u53ef\u8ffd\u6eaf\u6027\u3001\u6a21\u5757\u5316\u548c\u6587\u6863\u5316\u7684\u539f\u5219\uff0c\u5b9a\u4e49SCM\u7684\u9636\u6bb5\u3001\u6700\u4f73\u5b9e\u8df5\u548c\u54f2\u5b66\u7acb\u573a\u3002", "result": "SCM\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ed3\u6784\u5316\u4e14\u6301\u4e45\u7684\u5f00\u53d1\u5bf9\u8bdd\u65b9\u5f0f\uff0c\u8986\u76d6\u9879\u76ee\u5168\u751f\u547d\u5468\u671f\u3002", "conclusion": "SCM\u662f\u4e00\u79cd\u5fc5\u8981\u7684\u6539\u8fdb\uff0c\u91cd\u65b0\u786e\u7acb\u4e86\u5f00\u53d1\u8005\u4f5c\u4e3aAI\u5de5\u5177\u67b6\u6784\u5e08\u548c\u76d1\u7763\u8005\u7684\u89d2\u8272\u3002"}}
{"id": "2507.13035", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.13035", "abs": "https://arxiv.org/abs/2507.13035", "authors": ["Keila Lucas", "Rohit Gheyi", "M\u00e1rcio Ribeiro", "Fabio Palomba", "Luana Martins", "Elvys Soares"], "title": "Investigating the Performance of Small Language Models in Detecting Test Smells in Manual Test Cases", "comment": "7 pages, Accepted at Insightful Ideas and Emerging Results (IIER)\n  Track of the Brazilian Symposium on Software Engineering (SBES 2025)", "summary": "Manual testing, in which testers follow natural language instructions to\nvalidate system behavior, remains crucial for uncovering issues not easily\ncaptured by automation. However, these test cases often suffer from test\nsmells, quality issues such as ambiguity, redundancy, or missing checks that\nreduce test reliability and maintainability. While detection tools exist, they\ntypically require manual rule definition and lack scalability. This study\ninvestigates the potential of Small Language Models (SLMs) for automatically\ndetecting test smells. We evaluate Gemma3, Llama3.2, and Phi-4 on 143\nreal-world Ubuntu test cases, covering seven types of test smells. Phi-4\nachieved the best results, reaching a pass@2 of 97% in detecting sentences with\ntest smells, while Gemma3 and Llama3.2 reached approximately 91%. Beyond\ndetection, SLMs autonomously explained issues and suggested improvements, even\nwithout explicit prompt instructions. They enabled low-cost, concept-driven\nidentification of diverse test smells without relying on extensive rule\ndefinitions or syntactic analysis. These findings highlight the potential of\nSLMs as efficient tools that preserve data privacy and can improve test quality\nin real-world scenarios.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u5728\u81ea\u52a8\u68c0\u6d4b\u6d4b\u8bd5\u5f02\u5473\u4e2d\u7684\u6f5c\u529b\uff0cPhi-4\u8868\u73b0\u6700\u4f73\uff0c\u51c6\u786e\u7387\u8fbe97%\u3002", "motivation": "\u624b\u52a8\u6d4b\u8bd5\u4e2d\u7684\u6d4b\u8bd5\u5f02\u5473\uff08\u5982\u6a21\u7cca\u6027\u3001\u5197\u4f59\uff09\u964d\u4f4e\u4e86\u6d4b\u8bd5\u7684\u53ef\u9760\u6027\u548c\u53ef\u7ef4\u62a4\u6027\uff0c\u73b0\u6709\u5de5\u5177\u7f3a\u4e4f\u6269\u5c55\u6027\u3002", "method": "\u8bc4\u4f30\u4e86Gemma3\u3001Llama3.2\u548cPhi-4\u5728143\u4e2aUbuntu\u6d4b\u8bd5\u6848\u4f8b\u4e2d\u7684\u8868\u73b0\uff0c\u8986\u76d6\u4e03\u79cd\u6d4b\u8bd5\u5f02\u5473\u7c7b\u578b\u3002", "result": "Phi-4\u5728\u68c0\u6d4b\u6d4b\u8bd5\u5f02\u5473\u53e5\u5b50\u4e2d\u8fbe\u523097%\u7684\u51c6\u786e\u7387\uff0cGemma3\u548cLlama3.2\u7ea6\u4e3a91%\u3002SLMs\u8fd8\u80fd\u81ea\u4e3b\u89e3\u91ca\u95ee\u9898\u5e76\u63d0\u51fa\u6539\u8fdb\u5efa\u8bae\u3002", "conclusion": "SLMs\u662f\u9ad8\u6548\u5de5\u5177\uff0c\u53ef\u4f4e\u6210\u672c\u8bc6\u522b\u6d4b\u8bd5\u5f02\u5473\uff0c\u63d0\u5347\u6d4b\u8bd5\u8d28\u91cf\uff0c\u540c\u65f6\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u3002"}}
{"id": "2507.13081", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.13081", "abs": "https://arxiv.org/abs/2507.13081", "authors": ["Dongming Jin", "Weisong Sun", "Jiangping Huang", "Peng Liang", "Jifeng Xuan", "Yang Liu", "Zhi Jin"], "title": "iReDev: A Knowledge-Driven Multi-Agent Framework for Intelligent Requirements Development", "comment": "22pages, 4 figures", "summary": "Requirements development is a critical phase as it is responsible for\nproviding a clear understanding of what stakeholders need. It involves\ncollaboration among stakeholders to extract explicit requirements and address\npotential conflicts, which is time-consuming and labor-intensive. Recently,\nmulti-agent systems for software development have attracted much attention.\nHowever, existing research provides limited support for requirements\ndevelopment and overlooks the injection of human knowledge into agents and the\nhuman-agent collaboration. % To address these issues, this paper proposes a\nknowledge-driven multi-agent framework for intelligent requirement development,\nnamed iReDev. iReDev features: iReDev consists of six knowledge-driven agents\nto support the entire requirements development. They collaboratively perform\nvarious tasks to produce a software requirements specification. iReDev focuses\non integrating human knowledge for agents, enabling them to simulate real-world\nstakeholders. iReDev uses an event-driven communication mechanism based on an\nartifact pool. Agents continuously monitor the pool and autonomously trigger\nthe next action based on its changes, enabling iReDev to handle new\nrequirements quickly. iReDev introduces a human-in-the-loop mechanism to\nsupport human-agent collaboration, ensuring that the generated artifacts align\nwith the expectations of stakeholders. We evaluated the generated artifacts and\nresults show that iReDev outperforms existing baselines in multiple aspects. We\nfurther envision three key directions and hope this work can facilitate the\ndevelopment of intelligent requirements development.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aiReDev\u7684\u77e5\u8bc6\u9a71\u52a8\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u667a\u80fd\u9700\u6c42\u5f00\u53d1\uff0c\u901a\u8fc7\u96c6\u6210\u4eba\u7c7b\u77e5\u8bc6\u548c\u4e8b\u4ef6\u9a71\u52a8\u673a\u5236\u63d0\u5347\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5bf9\u9700\u6c42\u5f00\u53d1\u652f\u6301\u6709\u9650\uff0c\u4e14\u5ffd\u89c6\u4e86\u4eba\u7c7b\u77e5\u8bc6\u4e0e\u667a\u80fd\u4f53\u7684\u7ed3\u5408\u53ca\u4eba\u673a\u534f\u4f5c\u3002", "method": "iReDev\u5305\u542b\u516d\u4e2a\u77e5\u8bc6\u9a71\u52a8\u667a\u80fd\u4f53\uff0c\u91c7\u7528\u4e8b\u4ef6\u9a71\u52a8\u901a\u4fe1\u673a\u5236\u548c\u4eba\u5de5\u4ecb\u5165\u673a\u5236\uff0c\u652f\u6301\u5168\u6d41\u7a0b\u9700\u6c42\u5f00\u53d1\u3002", "result": "iReDev\u5728\u751f\u6210\u7684\u9700\u6c42\u89c4\u8303\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "iReDev\u4e3a\u667a\u80fd\u9700\u6c42\u5f00\u53d1\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u6f5c\u529b\u3002"}}
{"id": "2507.13095", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.13095", "abs": "https://arxiv.org/abs/2507.13095", "authors": ["Dongming Jin", "Zhi Jin", "Linyu Li", "Xiaohong Chen"], "title": "A Conceptual Framework for Requirements Engineering of Pretrained-Model-Enabled Systems", "comment": "5pages, 1 figure", "summary": "Recent advances in large pretrained models have led to their widespread\nintegration as core components in modern software systems. The trend is\nexpected to continue in the foreseeable future. Unlike traditional software\nsystems governed by deterministic logic, systems powered by pretrained models\nexhibit distinctive and emergent characteristics, such as ambiguous capability\nboundaries, context-dependent behavior, and continuous evolution. These\nproperties fundamentally challenge long-standing assumptions in requirements\nengineering, including functional decomposability and behavioral\npredictability. This paper investigates this problem and advocates for a\nrethinking of existing requirements engineering methodologies. We propose a\nconceptual framework tailored to requirements engineering of\npretrained-model-enabled software systems and outline several promising\nresearch directions within this framework. This vision helps provide a guide\nfor researchers and practitioners to tackle the emerging challenges in\nrequirements engineering of pretrained-model-enabled systems.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u73b0\u4ee3\u8f6f\u4ef6\u7cfb\u7edf\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u53ca\u5176\u5bf9\u9700\u6c42\u5de5\u7a0b\u7684\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9\u9884\u8bad\u7ec3\u6a21\u578b\u8f6f\u4ef6\u7cfb\u7edf\u7684\u9700\u6c42\u5de5\u7a0b\u6846\u67b6\u3002", "motivation": "\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u72ec\u7279\u7279\u6027\uff08\u5982\u6a21\u7cca\u80fd\u529b\u8fb9\u754c\u3001\u4e0a\u4e0b\u6587\u4f9d\u8d56\u884c\u4e3a\u548c\u6301\u7eed\u6f14\u5316\uff09\u6311\u6218\u4e86\u4f20\u7edf\u9700\u6c42\u5de5\u7a0b\u7684\u5047\u8bbe\uff0c\u9700\u8981\u91cd\u65b0\u601d\u8003\u73b0\u6709\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9\u9884\u8bad\u7ec3\u6a21\u578b\u8f6f\u4ef6\u7cfb\u7edf\u7684\u9700\u6c42\u5de5\u7a0b\u6982\u5ff5\u6846\u67b6\uff0c\u5e76\u6982\u8ff0\u4e86\u76f8\u5173\u7814\u7a76\u65b9\u5411\u3002", "result": "\u6846\u67b6\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u5e94\u5bf9\u9884\u8bad\u7ec3\u6a21\u578b\u8f6f\u4ef6\u7cfb\u7edf\u9700\u6c42\u5de5\u7a0b\u6311\u6218\u7684\u6307\u5bfc\u3002", "conclusion": "\u8bba\u6587\u547c\u5401\u91cd\u65b0\u601d\u8003\u9700\u6c42\u5de5\u7a0b\u65b9\u6cd5\uff0c\u4ee5\u9002\u5e94\u9884\u8bad\u7ec3\u6a21\u578b\u8f6f\u4ef6\u7cfb\u7edf\u7684\u7279\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2507.13117", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.13117", "abs": "https://arxiv.org/abs/2507.13117", "authors": ["Andreas Pointner", "Josef Pichler", "Herbert Pr\u00e4hofer"], "title": "Inferring Attributed Grammars from Parser Implementations", "comment": "Accepted to ICSME 2025", "summary": "Software systems that process structured inputs often lack complete and\nup-to-date specifications, which specify the input syntax and the semantics of\ninput processing. While grammar mining techniques have focused on recovering\nsyntactic structures, the semantics of input processing remains largely\nunexplored. In this work, we introduce a novel approach for inferring\nattributed grammars from parser implementations. Given an input grammar, our\ntechnique dynamically analyzes the implementation of recursive descent parsers\nto reconstruct the semantic aspects of input handling, resulting in\nspecifications in the form of attributed grammars. By observing program\nexecutions and mapping the program's runtime behavior to the grammar, we\nsystematically extract and embed semantic actions into the grammar rules. This\nenables comprehensive specification recovery. We demonstrate the feasibility of\nour approach using an initial set of programs, showing that it can accurately\nreproduce program behavior through the generated attributed grammars.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u9012\u5f52\u4e0b\u964d\u89e3\u6790\u5668\u5b9e\u73b0\u4e2d\u63a8\u65ad\u5c5e\u6027\u6587\u6cd5\u7684\u65b0\u65b9\u6cd5\uff0c\u4ee5\u6062\u590d\u8f93\u5165\u5904\u7406\u7684\u8bed\u4e49\u89c4\u8303\u3002", "motivation": "\u73b0\u6709\u8bed\u6cd5\u6316\u6398\u6280\u672f\u4e3b\u8981\u5173\u6ce8\u8bed\u6cd5\u7ed3\u6784\u6062\u590d\uff0c\u800c\u8f93\u5165\u5904\u7406\u7684\u8bed\u4e49\u89c4\u8303\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u52a8\u6001\u5206\u6790\u9012\u5f52\u4e0b\u964d\u89e3\u6790\u5668\u7684\u5b9e\u73b0\uff0c\u901a\u8fc7\u89c2\u5bdf\u7a0b\u5e8f\u6267\u884c\u5c06\u8fd0\u884c\u65f6\u884c\u4e3a\u6620\u5c04\u5230\u6587\u6cd5\uff0c\u5e76\u5d4c\u5165\u8bed\u4e49\u52a8\u4f5c\u5230\u6587\u6cd5\u89c4\u5219\u4e2d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u51c6\u786e\u901a\u8fc7\u751f\u6210\u7684\u5c5e\u6027\u6587\u6cd5\u91cd\u73b0\u7a0b\u5e8f\u884c\u4e3a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6062\u590d\u8f93\u5165\u5904\u7406\u7684\u8bed\u4e49\u89c4\u8303\u63d0\u4f9b\u4e86\u53ef\u884c\u9014\u5f84\u3002"}}
{"id": "2507.13123", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.13123", "abs": "https://arxiv.org/abs/2507.13123", "authors": ["Xin Yin", "Xinrui Li", "Chao Ni", "Xiaodan Xu", "Xiaohu Yang"], "title": "Detecting LLM-generated Code with Subtle Modification by Adversarial Training", "comment": null, "summary": "With the rapid development of Large Language Models (LLMs), their powerful\ncode-generation capabilities have been widely applied in tasks like code\ncompletion and automated development, demonstrating the value of improving\ncoding efficiency. However, the extensive use of LLM-generated code also raises\nseveral new challenges. On the one hand, issues such as the regulation of code\nprovenance, copyright disputes, and code quality have become increasingly\nconcerning. How to effectively detect LLM-generated code and ensure its\ncompliant and responsible use has become a critical and urgent issue. On the\nother hand, in practical applications, LLM-generated code is often subject to\nmanual modifications, such as variable renaming or structural adjustments.\nAlthough some recent studies have proposed training-based and zero-shot methods\nfor detecting LLM-generated code, these approaches show insufficient robustness\nwhen facing modified LLM-generated code, and there is a lack of an effective\nsolution. To address the real-world scenario where LLM-generated code may\nundergo minor modifications, we propose CodeGPTSensor+, an enhanced version of\nCodeGPTSensor, which employs adversarial training to improve robustness against\ninput perturbations. CodeGPTSensor+ integrates an adversarial sample generation\nmodule, Multi-objective Identifier and Structure Transformation (MIST), which\nsystematically generates both high-quality and representative adversarial\nsamples. This module effectively enhances the model's resistance against\ndiverse adversarial attacks. Experimental results on the HMCorp dataset\ndemonstrate that CodeGPTSensor+ significantly improves detection accuracy on\nthe adversarial test set while maintaining high accuracy on the original test\nset, showcasing superior robustness compared to CodeGPTSensor.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCodeGPTSensor+\uff0c\u901a\u8fc7\u5bf9\u6297\u8bad\u7ec3\u63d0\u5347\u5bf9\u4fee\u6539\u540eLLM\u751f\u6210\u4ee3\u7801\u7684\u68c0\u6d4b\u9c81\u68d2\u6027\u3002", "motivation": "\u968f\u7740LLM\u751f\u6210\u4ee3\u7801\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u68c0\u6d4b\u9762\u4e34\u4ee3\u7801\u4fee\u6539\u5e26\u6765\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u9c81\u68d2\u6027\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u5bf9\u6297\u8bad\u7ec3\uff0c\u96c6\u6210MIST\u6a21\u5757\u751f\u6210\u9ad8\u8d28\u91cf\u5bf9\u6297\u6837\u672c\u3002", "result": "\u5728HMCorp\u6570\u636e\u96c6\u4e0a\uff0cCodeGPTSensor+\u663e\u8457\u63d0\u5347\u5bf9\u6297\u6d4b\u8bd5\u96c6\u7684\u68c0\u6d4b\u51c6\u786e\u7387\u3002", "conclusion": "CodeGPTSensor+\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\u7684\u540c\u65f6\uff0c\u5c55\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002"}}
