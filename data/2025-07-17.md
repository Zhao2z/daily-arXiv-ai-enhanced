<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 11]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Decision Models for Selecting Architecture Patterns and Strategies in Quantum Software Systems](https://arxiv.org/abs/2507.11671)
*Mst Shamima Aktar,Peng Liang,Muhammad Waseem,Amjed Tahir,Mojtaba Shahin,Muhammad Azeem Akbar,Arif Ali Khan,Aakash Ahmad,Musengamana Jean de Dieu,Ruiyin Li*

Main category: cs.SE

TL;DR: 该研究提出了量子软件系统中六个关键设计领域的决策模型，帮助开发者选择合适的设计模式和策略。


<details>
  <summary>Details</summary>
Motivation: 量子软件开发者面临选择和实施适当模式和策略的挑战，缺乏指导。

Method: 通过数据挖掘和系统文献综述收集相关模式和策略，构建决策模型，并通过半结构化访谈评估其有效性。

Result: 决策模型在熟悉度、可理解性、完整性和实用性方面得到验证，能有效辅助开发者。

Conclusion: 该研究为量子软件系统架构设计提供了实用工具，数据集公开供社区使用。

Abstract: Quantum software represents disruptive technologies in terms of
quantum-specific software systems, services, and applications - leverage the
principles of quantum mechanics via programmable quantum bits (Qubits) that
manipulate quantum gates (QuGates) - to achieve quantum supremacy in computing.
Quantum software architecture enables quantum software developers to abstract
away implementation-specific details (i.e., mapping of Qubits and QuGates to
high-level architectural components and connectors). Architectural patterns and
strategies can provide reusable knowledge and best practices to engineer
quantum software systems effectively and efficiently. However, quantum software
practitioners face significant challenges in selecting and implementing
appropriate patterns and strategies due to the complexity of quantum software
systems and the lack of guidelines. To address these challenges, this study
proposes decision models for selecting patterns and strategies in six critical
design areas in quantum software systems: Communication, Decomposition, Data
Processing, Fault Tolerance, Integration and Optimization, and Algorithm
Implementation. These decision models are constructed based on data collected
from both a mining study (i.e., GitHub and Stack Exchange) and a Systematic
Literature Review, which were used to identify relevant patterns and strategies
with their involved Quality Attributes (QAs). We then conducted semi-structured
interviews with 16 quantum software practitioners to evaluate the familiarity,
understandability, completeness, and usefulness of the proposed decision
models. The results show that the proposed decision models can aid
practitioners in selecting suitable patterns and strategies to address the
challenges related to the architecture design of quantum software systems. The
dataset is available at [6], allowing the community to reproduce and build upon
our findings.

</details>


### [2] [MetaLint: Generalizable Idiomatic Code Quality Analysis through Instruction-Following and Easy-to-Hard Generalization](https://arxiv.org/abs/2507.11687)
*Atharva Naik,Lawanya Baghel,Dhakshin Govindarajan,Darsh Agrawal,Daniel Fried,Carolyn Rose*

Main category: cs.SE

TL;DR: MetaLint是一个基于指令调优的框架，用于代码质量分析，能够适应新的代码模式而无需重新训练，显著提升了对未见过代码习惯的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代码生成方面表现优异，但在代码质量分析上受限于静态训练数据，难以适应不断发展的最佳实践。

Method: MetaLint通过指令调优在合成数据上训练模型，支持从易到难的泛化，无需重新训练即可适应新代码模式。

Result: MetaLint在未见过PEP习惯的检测中达到70.37%的F分数，定位任务中表现也优于同类模型。

Conclusion: MetaLint展示了在代码质量分析中的潜力，尤其适用于未来不断变化的编码标准。

Abstract: Large Language Models, though successful in code generation, struggle with
code quality analysis because they are limited by static training data and
can't easily adapt to evolving best practices. We introduce MetaLint, a new
instruction-following framework that formulates code quality analysis as the
task of detecting and fixing problematic semantic code fragments or code idioms
based on high-level specifications. Unlike conventional approaches that train
models on static, rule-based data, MetaLint employs instruction tuning on
synthetic linter-generated data to support easy-to-hard generalization,
enabling models to adapt to novel or complex code patterns without retraining.
To evaluate this, we construct a benchmark of challenging idioms inspired by
real-world coding standards such as Python Enhancement Proposals (PEPs) and
assess whether MetaLint-trained models reason adaptively or simply memorize.
Our results show that MetaLint improves generalization to unseen PEP idioms,
achieving a 70.37% F-score on idiom detection with the highest recall (70.43%)
among all evaluated models. It also achieves 26.73% on localization,
competitive for its 4B parameter size and comparable to larger state-of-the-art
models like o3-mini, highlighting its potential for future-proof code quality
analysis.

</details>


### [3] [REST in Pieces: RESTful Design Rule Violations in Student-Built Web Apps](https://arxiv.org/abs/2507.11689)
*Sergio Di Meglio,Valeria Pontillo,Luigi Libero Lucio Starace*

Main category: cs.SE

TL;DR: 研究发现计算机科学本科课程中软件质量常被忽视，导致学生代码质量不佳，尤其是REST API设计规则违反频繁。


<details>
  <summary>Details</summary>
Motivation: 了解学生代码质量，为教育和招聘实践提供参考。

Method: 通过自动化静态分析管道评估40个全栈Web应用程序的REST API设计规则遵守情况。

Result: 常见违反包括路径缺少连字符（98%）、复数形式错误（88%）和HTTP方法误用（83%）。

Conclusion: 需加强API设计教学并采用自动化工具提升学生代码质量。

Abstract: In Computer Science Bachelor's programs, software quality is often
underemphasized due to limited time and a focus on foundational skills, leaving
many students unprepared for industry expectations. To better understand the
typical quality of student code and inform both education and hiring practices,
we analyze 40 full-stack web applications developed in a third-year Web
Technologies course. Using an automated static analysis pipeline, we assess
adherence to REST API design rules. Results reveal frequent violations of
foundational conventions, such as missing hyphens in endpoint paths (98%),
incorrect pluralization (88%), and misuse of HTTP methods (83%). These findings
highlight the need for more focused instruction on API design and support the
adoption of automated tools to improve code quality in student projects.

</details>


### [4] [Extremal Testing for Network Software using LLMs](https://arxiv.org/abs/2507.11898)
*Rathin Singha,Harry Qian,Srinath Saikrishnan,Tracy Zhao,Ryan Beckett,Siva Kesava Reddy Kakarla,George Varghese*

Main category: cs.SE

TL;DR: 利用LLM自动化网络软件的极端测试，生成违反约束的测试用例，发现新漏洞，并扩展到集中式网络软件和过滤代码生成。


<details>
  <summary>Details</summary>
Motivation: 传统极端测试依赖人工，效率低且难以覆盖所有边界情况。

Method: 分两步：1) 用LLM生成输入约束；2) 用LLM生成违反约束的测试用例。

Result: 在HTTP、BGP和DNS实现中发现新漏洞，并验证了方法的扩展性。

Conclusion: LLM生成的极端测试超越了传统边界值分析，未来可结合AI代理进一步自动化。

Abstract: Physicists often manually consider extreme cases when testing a theory. In
this paper, we show how to automate extremal testing of network software using
LLMs in two steps: first, ask the LLM to generate input constraints (e.g., DNS
name length limits); then ask the LLM to generate tests that violate the
constraints. We demonstrate how easy this process is by generating extremal
tests for HTTP, BGP and DNS implementations, each of which uncovered new bugs.
We show how this methodology extends to centralized network software such as
shortest path algorithms, and how LLMs can generate filtering code to reject
extremal input. We propose using agentic AI to further automate extremal
testing. LLM-generated extremal testing goes beyond an old technique in
software testing called Boundary Value Analysis.

</details>


### [5] [A Task Taxonomy for Conformance Checking](https://arxiv.org/abs/2507.11976)
*Jana-Rebecca Rehse,Michael Grohs,Finn Klessascheck,Lisa-Marie Klein,Tatiana von Landesberger,Luise Pufahl*

Main category: cs.SE

TL;DR: 论文提出了一种任务分类法，用于系统化一致性检查中的可视化目的，以支持研究人员评估其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前一致性检查工具的可视化目的不明确，缺乏系统性理解，难以评估其有效性。

Method: 提出任务分类法，从目标、手段、约束类型、数据特征、数据目标和数据基数等方面分类一致性检查任务。

Result: 分类法有助于明确可视化目的，促进过程挖掘与可视化分析的跨学科合作。

Conclusion: 任务分类法为一致性检查的可视化研究提供了系统性框架，支持跨学科协作。

Abstract: Conformance checking is a sub-discipline of process mining, which compares
observed process traces with a process model to analyze whether the process
execution conforms with or deviates from the process design. Organizations can
leverage this analysis, for example to check whether their processes comply
with internal or external regulations or to identify potential improvements.
Gaining these insights requires suitable visualizations, which make complex
results accessible and actionable. So far, however, the development of
conformance checking visualizations has largely been left to tool vendors. As a
result, current tools offer a wide variety of visual representations for
conformance checking, but the analytical purposes they serve often remain
unclear. However, without a systematic understanding of these purposes, it is
difficult to evaluate the visualizations' usefulness. Such an evaluation hence
requires a deeper understanding of conformance checking as an analysis domain.
To this end, we propose a task taxonomy, which categorizes the tasks that can
occur when conducting conformance checking analyses. This taxonomy supports
researchers in determining the purpose of visualizations, specifying relevant
conformance checking tasks in terms of their goal, means, constraint type, data
characteristics, data target, and data cardinality. Combining concepts from
process mining and visual analytics, we address researchers from both
disciplines to enable and support closer collaborations.

</details>


### [6] [LLAMA: Multi-Feedback Smart Contract Fuzzing Framework with LLM-Guided Seed Generation](https://arxiv.org/abs/2507.12084)
*Keke Gai,Haochen Liang,Jing Yu,Liehuang Zhu,Dusit Niyato*

Main category: cs.SE

TL;DR: LLAMA是一个基于大型语言模型（LLMs）的多反馈智能合约模糊测试框架，通过结合LLMs、进化突变策略和混合测试技术，显著提升了覆盖率和漏洞检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有模糊测试工具主要关注种子调度和生成，而突变调度未被充分研究，影响了测试效果。LLAMA旨在填补这一空白。

Method: LLAMA采用分层提示策略生成初始种子，结合多反馈优化机制和进化模糊引擎，动态调整突变操作概率。

Result: 实验显示，LLAMA在指令覆盖率和分支覆盖率上分别达到91%和90%，并检测出132个已知漏洞。

Conclusion: LLAMA在智能合约安全测试中表现出高效性、适应性和实用性。

Abstract: Smart contracts play a pivotal role in blockchain ecosystems, and fuzzing
remains an important approach to securing smart contracts. Even though mutation
scheduling is a key factor influencing fuzzing effectiveness, existing fuzzers
have primarily explored seed scheduling and generation, while mutation
scheduling has been rarely addressed by prior work. In this work, we propose a
Large Language Models (LLMs)-based Multi-feedback Smart Contract Fuzzing
framework (LLAMA) that integrates LLMs, evolutionary mutation strategies, and
hybrid testing techniques. Key components of the proposed LLAMA include: (i) a
hierarchical prompting strategy that guides LLMs to generate semantically valid
initial seeds, coupled with a lightweight pre-fuzzing phase to select
high-potential inputs; (ii) a multi-feedback optimization mechanism that
simultaneously improves seed generation, seed selection, and mutation
scheduling by leveraging runtime coverage and dependency feedback; and (iii) an
evolutionary fuzzing engine that dynamically adjusts mutation operator
probabilities based on effectiveness, while incorporating symbolic execution to
escape stagnation and uncover deeper vulnerabilities. Our experiments
demonstrate that LLAMA outperforms state-of-the-art fuzzers in both coverage
and vulnerability detection. Specifically, it achieves 91% instruction coverage
and 90% branch coverage, while detecting 132 out of 148 known vulnerabilities
across diverse categories. These results highlight LLAMA's effectiveness,
adaptability, and practicality in real-world smart contract security testing
scenarios.

</details>


### [7] [From Static to Intelligent: Evolving SaaS Pricing with LLMs](https://arxiv.org/abs/2507.12104)
*Francisco Javier Cavero,Juan C. Alonso,Antonio Ruiz-Cortés*

Main category: cs.SE

TL;DR: 论文提出了一种基于LLM的自动化工具AI4Pricing2Yaml，用于将静态HTML定价转换为动态可读的智能定价（iPricing），以解决SaaS定价管理的复杂性和效率问题。


<details>
  <summary>Details</summary>
Motivation: SaaS市场的快速扩张导致定价管理复杂化，人工操作效率低且易出错，缺乏自动化工具限制了定价模型的优化和扩展。

Method: 采用LLM驱动的技术，结合网页抓取和信息提取，将静态HTML定价转换为iPricing，并通过AI4Pricing2Yaml工具实现。

Result: 在30个商业SaaS网站上的验证表明，系统能有效提取定价要素，但仍面临幻觉、复杂结构和动态内容的挑战。

Conclusion: 自动化智能定价转换有望提升SaaS定价管理的一致性和可扩展性，未来研究将优化提取能力和系统适应性。

Abstract: The SaaS paradigm has revolutionized software distribution by offering
flexible pricing options to meet diverse customer needs. However, the rapid
expansion of the SaaS market has introduced significant complexity for DevOps
teams, who must manually manage and evolve pricing structures, an approach that
is both time-consuming and prone to errors. The absence of automated tools for
pricing analysis restricts the ability to efficiently evaluate, optimize, and
scale these models. This paper proposes leveraging intelligent pricing
(iPricing), dynamic, machine-readable pricing models, as a solution to these
challenges. Intelligent pricing enables competitive analysis, streamlines
operational decision-making, and supports continuous pricing evolution in
response to market dynamics, leading to improved efficiency and accuracy. We
present an LLM-driven approach that automates the transformation of static HTML
pricing into iPricing, significantly improving efficiency and consistency while
minimizing human error. Our implementation, AI4Pricing2Yaml, features a basic
Information Extractor that uses web scraping and LLMs technologies to extract
essential pricing components, plans, features, usage limits, and add-ons, from
SaaS websites. Validation against a dataset of 30 distinct commercial SaaS,
encompassing over 150 intelligent pricings, demonstrates the system's
effectiveness in extracting the desired elements across all steps. However,
challenges remain in addressing hallucinations, complex structures, and dynamic
content. This work highlights the potential of automating intelligent pricing
transformation to streamline SaaS pricing management, offering implications for
improved consistency and scalability in an increasingly intricate pricing
landscape. Future research will focus on refining extraction capabilities and
enhancing the system's adaptability to a wider range of SaaS websites.

</details>


### [8] [An Online A/B Testing Decision Support System for Web Usability Assessment Based on a Linguistic Decision-making Methodology: Case of Study a Virtual Learning Environment](https://arxiv.org/abs/2507.12118)
*Noe Zermeño,Cristina Zuheros,Lucas Daniel Del Rosso Calache,Francisco Herrera,Rosana Montes*

Main category: cs.SE

TL;DR: 提出了一种基于用户中心方法的网页可用性评估方法，结合A/B测试和角色扮演，应用于实际案例。


<details>
  <summary>Details</summary>
Motivation: 提升用户界面满意度，解决现有工具在评估设计时支持不足的问题。

Method: 结合设计思维和语言决策方法，进行角色扮演和可用性测试（如SUS），并整合到A/B测试决策支持系统中。

Result: 在墨西哥瓜达拉哈拉大学的Moodle平台上进行了实际案例评估。

Conclusion: 该方法有效支持网页可用性评估，结合用户参与和A/B测试。

Abstract: In recent years, attention has increasingly focused on enhancing user
satisfaction with user interfaces, spanning both mobile applications and
websites. One fundamental aspect of human-machine interaction is the concept of
web usability. In order to assess web usability, the A/B testing technique
enables the comparison of data between two designs. Expanding the scope of
tests to include the designs being evaluated, in conjunction with the
involvement of both real and fictional users, presents a challenge for which
few online tools offer support. We propose a methodology for web usability
evaluation based on user-centered approaches such as design thinking and
linguistic decision-making, named Linguistic Decision-Making for Web Usability
Evaluation. This engages people in role-playing scenarios and conducts a number
of usability tests, including the widely recognized System Usability Scale. We
incorporate the methodology into a decision support system based on A/B
testing. We use real users in a case study to assess three Moodle platforms at
the University of Guadalajara, Mexico.

</details>


### [9] [MERA Code: A Unified Framework for Evaluating Code Generation Across Tasks](https://arxiv.org/abs/2507.12284)
*Artem Chervyakov,Alexander Kharitonov,Pavel Zadorozhny,Adamenko Pavel,Rodion Levichev,Dmitrii Vorobev,Dmitrii Salikhov,Aidar Valeev,Alena Pestova,Maria Dziuba,Ilseyar Alimova,Artem Zavgorodnev,Aleksandr Medvedev,Stanislav Moiseev,Elena Bruches,Daniil Grebenkin,Roman Derunets,Vikulov Vladimir,Anton Emelyanov,Dmitrii Babaev,Vladimir V. Ivanov,Valentin Malykh,Alena Fenogenova*

Main category: cs.SE

TL;DR: 论文提出MERA Code基准，专注于评估俄语代码生成LLM的实际编码能力，填补现有评测忽略代码质量的空白。


<details>
  <summary>Details</summary>
Motivation: 现有评测主要关注自然语言任务，忽略了代码质量和实际性能，导致对LLM在真实生产环境中的能力和风险理解不足。

Method: 提出MERA Code基准，包含11个任务和8种编程语言，提供开源代码库、评分系统和平台（排行榜与提交系统）。

Result: 评估了开源和前沿API模型，发现其在非英语语言的实际编码任务中存在局限性。

Conclusion: 公开MERA Code以指导未来研究，推动模型开发创新，并标准化评测流程。

Abstract: Advancements in LLMs have enhanced task automation in software engineering;
however, current evaluations primarily focus on natural language tasks,
overlooking code quality. Most benchmarks prioritize high-level reasoning over
executable code and real-world performance, leaving gaps in understanding true
capabilities and risks associated with these models in production. To address
this issue, we propose MERA Code, a new addition to the MERA benchmark family,
specifically focused on evaluating code for the latest code generation LLMs in
Russian. This benchmark includes 11 evaluation tasks that span 8 programming
languages. Our proposed evaluation methodology features a taxonomy that
outlines the practical coding skills necessary for models to complete these
tasks. The benchmark comprises an open-source codebase for users to conduct
MERA assessments, a scoring system compatible with various programming
environments, and a platform featuring a leaderboard and submission system. We
evaluate open LLMs and frontier API models, analyzing their limitations in
terms of practical coding tasks in non-English languages. We are publicly
releasing MERA to guide future research, anticipate groundbreaking features in
model development, and standardize evaluation procedures.

</details>


### [10] [GitChameleon: Evaluating AI Code Generation Against Python Library Version Incompatibilities](https://arxiv.org/abs/2507.12367)
*Diganta Misra,Nizar Islah,Victor May,Brice Rauby,Zihan Wang,Justine Gehring,Antonio Orvieto,Muawiz Chaudhary,Eilif B. Muller,Irina Rish,Samira Ebrahimi Kahou,Massimo Caccia*

Main category: cs.SE

TL;DR: GitChameleon是一个新的数据集，用于评估AI在特定库版本下的代码生成能力，现有模型表现不佳。


<details>
  <summary>Details</summary>
Motivation: 解决软件库快速更新带来的代码生成挑战，现有基准缺乏执行评估。

Method: 引入GitChameleon数据集，包含328个Python代码完成问题，附带单元测试。

Result: 当前先进模型的成功率仅为48-51%，显示任务复杂性。

Conclusion: GitChameleon为动态代码库的AI生成方法提供了评估基准，促进更可靠的方法开发。

Abstract: The rapid evolution of software libraries poses a considerable hurdle for
code generation, necessitating continuous adaptation to frequent version
updates while preserving backward compatibility. While existing code evolution
benchmarks provide valuable insights, they typically lack execution-based
evaluation for generating code compliant with specific library versions. To
address this, we introduce GitChameleon, a novel, meticulously curated dataset
comprising 328 Python code completion problems, each conditioned on specific
library versions and accompanied by executable unit tests. GitChameleon
rigorously evaluates the capacity of contemporary large language models (LLMs),
LLM-powered agents, code assistants, and RAG systems to perform
version-conditioned code generation that demonstrates functional accuracy
through execution. Our extensive evaluations indicate that state-of-the-art
systems encounter significant challenges with this task; enterprise models
achieving baseline success rates in the 48-51\% range, underscoring the
intricacy of the problem. By offering an execution-based benchmark emphasizing
the dynamic nature of code libraries, GitChameleon enables a clearer
understanding of this challenge and helps guide the development of more
adaptable and dependable AI code generation methods. We make the dataset and
evaluation code publicly available at
https://github.com/mrcabbage972/GitChameleonBenchmark.

</details>


### [11] [SWE-Perf: Can Language Models Optimize Code Performance on Real-World Repositories?](https://arxiv.org/abs/2507.12415)
*Xinyi He,Qian Liu,Mingzhe Du,Lin Yan,Zhijie Fan,Yiming Huang,Zejian Yuan,Zejun Ma*

Main category: cs.SE

TL;DR: SWE-Perf是首个专门评估LLM在真实仓库环境中代码性能优化能力的基准，包含140个实例，揭示了现有LLM与专家级优化性能之间的显著差距。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在代码生成和修复方面表现出色，但在仓库级代码性能优化方面的能力尚未充分探索。

Method: 引入SWE-Perf基准，包含140个实例，每个实例包括代码库、目标函数、性能测试、专家补丁和可执行环境。

Result: 评估显示现有LLM与专家级性能优化之间存在显著差距。

Conclusion: SWE-Perf为这一新兴领域的研究提供了重要机会。

Abstract: Code performance optimization is paramount in real-world software engineering
and critical for production-level systems. While Large Language Models (LLMs)
have demonstrated impressive capabilities in code generation and bug fixing,
their proficiency in enhancing code performance at the repository level remains
largely unexplored. To address this gap, we introduce SWE-Perf, the first
benchmark specifically designed to systematically evaluate LLMs on code
performance optimization tasks within authentic repository contexts. SWE-Perf
comprises 140 carefully curated instances, each derived from
performance-improving pull requests from popular GitHub repositories. Each
benchmark instance includes the relevant codebase, target functions,
performance-related tests, expert-authored patches, and executable
environments. Through a comprehensive evaluation of representative methods that
span file-level and repo-level approaches (e.g., Agentless and OpenHands), we
reveal a substantial capability gap between existing LLMs and expert-level
optimization performance, highlighting critical research opportunities in this
emerging field.

</details>
