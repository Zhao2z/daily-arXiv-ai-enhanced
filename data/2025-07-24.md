<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 16]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Evaluating Uncertainty and Quality of Visual Language Action-enabled Robots](https://arxiv.org/abs/2507.17049)
*Pablo Valle,Chengjie Lu,Shaukat Ali,Aitor Arrieta*

Main category: cs.SE

TL;DR: 本文提出了针对视觉语言动作(VLA)模型的8个不确定性指标和5个质量指标，通过908次成功任务执行的大规模实证研究，发现这些指标与人类专家评估具有中等到强的相关性，挑战了仅依赖二元成功率的现有评估方法。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型评估主要依赖任务成功率，无法捕捉任务执行质量和模型决策置信度。传统的二元成功率评估方法不足以全面评价VLA模型在机器人操作任务中的表现，需要更细粒度的评估指标。

Method: 提出了8个不确定性指标和5个质量指标专门用于评估VLA模型的机器人操作任务。通过大规模实证研究，使用3个最先进的VLA模型在4个代表性机器人操作任务上进行908次成功任务执行。人类领域专家手动标注任务质量，分析提出指标与专家判断之间的相关性。

Result: 研究结果显示，多个提出的指标与人类评估呈现中等到强的相关性，证明了这些指标在评估任务质量和模型置信度方面的实用性。部分指标能够区分高、中、低质量执行与失败任务，这在缺乏测试预言机时具有重要价值。

Conclusion: 研究挑战了当前仅依赖二元成功率的评估实践的充分性，为改进VLA驱动的机器人系统的实时监控和自适应增强铺平了道路。提出的指标体系为VLA模型提供了更全面、细致的评估框架。

Abstract: Visual Language Action (VLA) models are a multi-modal class of Artificial
Intelligence (AI) systems that integrate visual perception, natural language
understanding, and action planning to enable agents to interpret their
environment, comprehend instructions, and perform embodied tasks autonomously.
Recently, significant progress has been made to advance this field. These kinds
of models are typically evaluated through task success rates, which fail to
capture the quality of task execution and the mode's confidence in its
decisions. In this paper, we propose eight uncertainty metrics and five quality
metrics specifically designed for VLA models for robotic manipulation tasks. We
assess their effectiveness through a large-scale empirical study involving 908
successful task executions from three state-of-the-art VLA models across four
representative robotic manipulation tasks. Human domain experts manually
labeled task quality, allowing us to analyze the correlation between our
proposed metrics and expert judgments. The results reveal that several metrics
show moderate to strong correlation with human assessments, highlighting their
utility for evaluating task quality and model confidence. Furthermore, we found
that some of the metrics can discriminate between high-, medium-, and
low-quality executions from unsuccessful tasks, which can be interesting when
test oracles are not available. Our findings challenge the adequacy of current
evaluation practices that rely solely on binary success rates and pave the way
for improved real-time monitoring and adaptive enhancement of VLA-enabled
robotic systems.

</details>


### [2] [Assessing Reliability of Statistical Maximum Coverage Estimators in Fuzzing](https://arxiv.org/abs/2507.17093)
*Danushka Liyanage,Nelum Attanayake,Zijian Luo,Rahul Gopinath*

Main category: cs.SE

TL;DR: 本文提出了一个评估模糊测试中可达性估计器可靠性的框架，通过合成程序生成真实标签和在真实程序上的可靠性检查来解决现有评估方法的不足


<details>
  <summary>Details</summary>
Motivation: 模糊测试通常以覆盖率为指导，但估计最大可达覆盖率面临挑战：静态可达性分析不准确，基于生物统计学的统计估计方法缺乏可靠的带标签基准数据集进行严格评估

Method: 1）提出评估框架，合成生成具有复杂控制流的大型程序，确保明确定义的可达性并提供评估的真实标签；2）通过改变采样单元大小来适配真实世界基准上的可达性估计器可靠性检查方法

Result: 两项研究共同回答了当前可达性估计器是否可靠的问题，并为评估未来可达性估计改进定义了协议

Conclusion: 该工作建立了评估可达性估计器可靠性的完整框架，既解决了缺乏真实标签的问题，又提供了在真实程序上验证的方法，为未来改进可达性估计提供了评估标准

Abstract: Background: Fuzzers are often guided by coverage, making the estimation of
maximum achievable coverage a key concern in fuzzing. However, achieving 100%
coverage is infeasible for most real-world software systems, regardless of
effort. While static reachability analysis can provide an upper bound, it is
often highly inaccurate. Recently, statistical estimation methods based on
species richness estimators from biostatistics have been proposed as a
potential solution. Yet, the lack of reliable benchmarks with labeled ground
truth has limited rigorous evaluation of their accuracy.
  Objective: This work examines the reliability of reachability estimators from
two axes: addressing the lack of labeled ground truth and evaluating their
reliability on real-world programs.
  Methods: (1) To address the challenge of labeled ground truth, we propose an
evaluation framework that synthetically generates large programs with complex
control flows, ensuring well-defined reachability and providing ground truth
for evaluation. (2) To address the criticism from use of synthetic benchmarks,
we adapt a reliability check for reachability estimators on real-world
benchmarks without labeled ground truth -- by varying the size of sampling
units, which, in theory, should not affect the estimate.
  Results: These two studies together will help answer the question of whether
current reachability estimators are reliable, and defines a protocol to
evaluate future improvements in reachability estimation.

</details>


### [3] [Can LLMs Write CI? A Study on Automatic Generation of GitHub Actions Configurations](https://arxiv.org/abs/2507.17165)
*Taher A. Ghaleb,Dulina Rathnayake*

Main category: cs.SE

TL;DR: 研究评估了六个大语言模型（包括GPT-4o、Llama、Gemma等）从自然语言描述生成GitHub Actions配置文件的能力，发现零样本提示最高能达到69%的相似度但仅有3%完全匹配，揭示了LLM在CI配置生成方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 持续集成服务需要开发者编写基于YAML的配置文件，这个过程繁琐且容易出错。尽管大语言模型在软件工程任务自动化方面应用日益增多，但其生成CI配置的能力仍未得到充分探索，因此需要系统性评估LLM在这一任务上的表现。

Method: 评估了六个LLM模型：三个通用基础模型（GPT-4o、Llama、Gemma）和三个代码预训练模型（GPT-4.1、Code Llama、CodeGemma）。构建了首个此类标注数据集，从GitHub Actions文档中提取描述与对应的最佳实践YAML配置对。使用零样本提示方法进行评估，并分析生成结果的相似度和准确性。

Result: 零样本提示最高达到69%的相似度，但仅有3%的完全匹配。代码预训练模型在YAML-based CI任务上的表现略逊于通用模型。GPT-4o输出分析显示存在步骤缺失或重命名、描述误解、不必要添加等问题，影响了结构和上下文的正确性。

Conclusion: 研究揭示了LLM在CI配置生成方面的局限性，生成质量与可执行CI配置所需的精确度之间存在差距。研究为改进LLM与配置语言的对齐提供了见解，并为未来CI自动化和工具支持的努力提供指导。

Abstract: Continuous Integration (CI) services, such as GitHub Actions, require
developers to write YAML-based configurations, which can be tedious and
error-prone. Despite the increasing use of Large Language Models (LLMs) to
automate software engineering tasks, their ability to generate CI
configurations remains underexplored. This paper presents a preliminary study
evaluating six LLMs for generating GitHub Actions configurations from natural
language descriptions. We assess three general-purpose foundation models
(GPT-4o, Llama, and Gemma) and three code-pretrained models (GPT-4.1, Code
Llama, and CodeGemma). We also introduce the first labeled dataset of its kind,
constructed from GitHub Actions documentation, pairing descriptions with
corresponding best-practice YAML configurations. Zero-shot prompting achieves
up to 69% similarity with the ground truth, with only 3% perfect matches.
Code-pretrained models slightly underperform compared to general-purpose ones
in YAML-based CI tasks, revealing LLM limitations for CI configuration
generation. Analyzing GPT-4o outputs reveals issues like missing or renamed
steps, misinterpreted descriptions, and unnecessary additions that may affect
structural and contextual correctness, indicating a gap between generation
quality and the precision required for executable CI configurations. Our
research offers insights for improving LLM alignment with configuration
languages and guiding future efforts on CI automation and tooling support.

</details>


### [4] [On the Feasibility of Quantum Unit Testing](https://arxiv.org/abs/2507.17235)
*Andriy Miranskyy,José Campos,Anila Mjeda,Lei Zhang,Ignacio García Rodríguez de Guzmán*

Main category: cs.SE

TL;DR: 该研究比较了传统统计方法与量子电路专用测试方法在量子软件单元测试中的表现，通过对近180万个变异量子电路的实证研究，发现量子中心化测试（特别是状态向量测试和逆测试）在精度和效率方面具有明显优势。


<details>
  <summary>Details</summary>
Motivation: 量子软件复杂性日益增加，给软件验证和验证带来重大挑战，特别是在单元测试方面。需要开发更有效的量子软件测试策略来支持容错量子计算机的未来应用。

Method: 进行了量子中心化单元测试的综合研究，比较传统统计方法与专门为量子电路设计的测试方法，包括仅在经典计算机上运行的状态向量测试，以及可在量子硬件上执行的交换测试和新颖的逆测试。通过对1,796,880个变异量子电路的实证研究和详细分析进行评估。

Result: 量子中心化测试，特别是状态向量测试和逆测试，在精度和效率方面提供了明显优势，与统计测试相比减少了假阳性和假阴性。研究调查了每种测试检测量子电路预期状态与实际状态之间细微差异的能力，以及实现高可靠性所需的测量次数。

Conclusion: 该工作为开发更强大和可扩展的量子软件测试策略做出了贡献，支持容错量子计算机的未来应用，并促进量子软件工程中更可靠的实践。量子中心化测试方法在量子软件验证方面展现出显著的优势。

Abstract: The increasing complexity of quantum software presents significant challenges
for software verification and validation, particularly in the context of unit
testing. This work presents a comprehensive study on quantum-centric unit
tests, comparing traditional statistical approaches with tests specifically
designed for quantum circuits. These include tests that run only on a classical
computer, such as the Statevector test, as well as those executable on quantum
hardware, such as the Swap test and the novel Inverse test. Through an
empirical study and detailed analysis on 1,796,880 mutated quantum circuits, we
investigate (a) each test's ability to detect subtle discrepancies between the
expected and actual states of a quantum circuit, and (b) the number of
measurements required to achieve high reliability. The results demonstrate that
quantum-centric tests, particularly the Statevector test and the Inverse test,
provide clear advantages in terms of precision and efficiency, reducing both
false positives and false negatives compared to statistical tests. This work
contributes to the development of more robust and scalable strategies for
testing quantum software, supporting the future adoption of fault-tolerant
quantum computers and promoting more reliable practices in quantum software
engineering.

</details>


### [5] [Understanding Prompt Programming Tasks and Questions](https://arxiv.org/abs/2507.17264)
*Jenny T. Liang,Chenyang Yang,Agnia Sergeyuk,Travis D. Breaux,Brad A. Myers*

Main category: cs.SE

TL;DR: 这篇论文通过访谈、观察和调研研究了提示编程的需求，构建了包含25个任务和51个问题的分类体系，发现现有工具支持不足，并提出了改进机会


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型等基础模型的普及，开发者开始在软件中嵌入提示程序，但目前对提示编程过程中开发者的需求和面临的问题缺乏深入了解，且现有研究和商业工具是否充分满足开发者需求尚不明确

Method: 采用混合研究方法：(1)访谈16名提示编程开发者；(2)观察8名开发者进行提示修改过程；(3)调研50名开发者；(4)分析48个研究和商业工具，构建提示编程任务和问题的分类体系

Result: 构建了包含25个提示编程任务和51个相关问题的分类体系，发现所有任务都需要手动完成，51个问题中有16个（包括大部分最重要的问题）仍然没有得到解答，表明提示编程支持严重不足

Conclusion: 当前提示编程工具的支持严重不足，大量重要任务和问题未得到有效解决，基于研究结果为提示编程工具的发展提出了重要改进方向和机会

Abstract: Prompting foundation models (FMs) like large language models (LLMs) have
enabled new AI-powered software features (e.g., text summarization) that
previously were only possible by fine-tuning FMs. Now, developers are embedding
prompts in software, known as prompt programs. The process of prompt
programming requires the developer to make many changes to their prompt. Yet,
the questions developers ask to update their prompt is unknown, despite the
answers to these questions affecting how developers plan their changes. With
the growing number of research and commercial prompt programming tools, it is
unclear whether prompt programmers' needs are being adequately addressed. We
address these challenges by developing a taxonomy of 25 tasks prompt
programmers do and 51 questions they ask, measuring the importance of each task
and question. We interview 16 prompt programmers, observe 8 developers make
prompt changes, and survey 50 developers. We then compare the taxonomy with 48
research and commercial tools. We find that prompt programming is not
well-supported: all tasks are done manually, and 16 of the 51 questions --
including a majority of the most important ones -- remain unanswered. Based on
this, we outline important opportunities for prompt programming tools.

</details>


### [6] [Lessons from a Big-Bang Integration: Challenges in Edge Computing and Machine Learning](https://arxiv.org/abs/2507.17270)
*Alessandro Aneggi,Andrea Janes*

Main category: cs.SE

TL;DR: 本文分析了一个为期一年的分布式实时分析系统项目的失败经验，该项目采用边缘计算和机器学习技术，但因采用大爆炸式集成方法导致系统仅运行6分钟而非预期的40分钟，研究识别了技术和组织障碍，并提出了基于模拟的早期部署和结构化集成周期等解决方案。


<details>
  <summary>Details</summary>
Motivation: 项目团队在开发分布式实时分析系统时遇到了严重的集成问题，系统功能远低于预期。需要分析失败原因并为未来类似项目提供经验教训和改进建议。

Method: 通过根因分析方法对项目失败进行深入研究，识别技术和组织层面的障碍因素，同时考虑心理因素如对完整开发组件的偏好。分析传统敏捷方法在分布式项目中的局限性。

Result: 识别出的主要问题包括：沟通不良、缺乏早期集成测试、对自上而下规划的抗拒，以及偏好完整组件而非模型的心理偏见。系统最终仅实现6分钟功能运行，远低于40分钟的目标。

Conclusion: 提出了早期基于模型的部署、建立健壮的沟通基础设施、采用自上而下的思维方式来管理复杂性和降低风险的建议。强调传统敏捷方法的局限性，推荐采用仿真驱动工程和结构化集成周期作为未来成功的关键因素。

Abstract: This experience report analyses a one year project focused on building a
distributed real-time analytics system using edge computing and machine
learning. The project faced critical setbacks due to a big-bang integration
approach, where all components developed by multiple geographically dispersed
partners were merged at the final stage. The integration effort resulted in
only six minutes of system functionality, far below the expected 40 minutes.
Through root cause analysis, the study identifies technical and organisational
barriers, including poor communication, lack of early integration testing, and
resistance to topdown planning. It also considers psychological factors such as
a bias toward fully developed components over mockups. The paper advocates for
early mock based deployment, robust communication infrastructures, and the
adoption of topdown thinking to manage complexity and reduce risk in reactive,
distributed projects. These findings underscore the limitations of traditional
Agile methods in such contexts and propose simulation-driven engineering and
structured integration cycles as key enablers for future success.

</details>


### [7] [Seed&Steer: Guiding Large Language Models with Compilable Prefix and Branch Signals for Unit Test Generation](https://arxiv.org/abs/2507.17271)
*Shuaiyu Zhou,Zhengran Zeng,Xiaoling Zhou,Rui Xie,Shikun Zhang,Wei Ye*

Main category: cs.SE

TL;DR: 该论文提出了Seed&Steer方法，通过将前缀生成和断言生成解耦，结合传统单元测试工具和大语言模型来改进自动化测试生成，在编译成功率和测试覆盖率方面都取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 尽管基于大语言模型的自动化测试生成取得了进展，但现有方法在编译成功率和测试覆盖率方面仍存在挑战。作者从新角度重新审视这个问题，将单元测试生成分解为前缀生成和断言生成两个独立的子问题，并发现前缀生成主要影响编译成功，而断言生成影响测试覆盖率。

Method: 提出Seed&Steer两步方法：1）利用传统单元测试工具（如EvoSuite）生成具有高编译成功率的方法调用作为种子，指导LLM构建有效的测试上下文；2）引入分支提示帮助LLM探索不同的执行路径（正常、边界、异常情况）并生成高覆盖率的断言。该方法定义了初始化复杂度并采用环复杂度来衡量前缀和断言生成的难度。

Result: 在五个真实Java项目上的评估显示：编译通过率提升约7%，在两个LLM上成功编译了792和887个之前失败的案例；在不同复杂度的目标方法上实现了高达73%的分支和行覆盖率；覆盖率改进范围从1.09倍到1.26倍。

Conclusion: Seed&Steer方法通过结合传统单元测试技术和大语言模型的优势，有效解决了LLM-based单元测试生成中的编译和覆盖率问题。该方法在多个指标上超越了最先进的基线方法，为自动化测试生成领域提供了新的解决方案。

Abstract: Unit tests play a vital role in the software development lifecycle. Recent
advances in Large Language Model (LLM)-based approaches have significantly
improved automated test generation, garnering attention from both academia and
industry. We revisit LLM-based unit test generation from a novel perspective by
decoupling prefix generation and assertion generation. To characterize their
respective challenges, we define Initialization Complexity and adopt Cyclomatic
Complexity to measure the difficulty of prefix and assertion generation,
revealing that the former primarily affects compilation success, while the
latter influences test coverage. To address these challenges, we propose
Seed&Steer, a two-step approach that combines traditional unit testing
techniques with the capabilities of large language models. Seed&Steer leverages
conventional unit testing tools (e.g., EvoSuite) to generate method invocations
with high compilation success rates, which serve as seeds to guide LLMs in
constructing effective test contexts. It then introduces branching cues to help
LLMs explore diverse execution paths (e.g., normal, boundary, and exception
cases) and generate assertions with high coverage. We evaluate Seed&Steer on
five real-world Java projects against state-of-the-art baselines. Results show
that Seed&Steer improves the compilation pass rate by approximately 7%,
successfully compiling 792 and 887 previously failing cases on two LLMs. It
also achieves up to ~73% branch and line coverage across focal methods of
varying complexity, with coverage improvements ranging from 1.09* to 1.26*. Our
code, dataset, and experimental scripts will be publicly released to support
future research and reproducibility.

</details>


### [8] [Data Virtualization for Machine Learning](https://arxiv.org/abs/2507.17293)
*Saiful Khan,Joyraj Chakraborty,Philip Beaucamp,Niraj Bhujel,Min Chen*

Main category: cs.SE

TL;DR: 本文提出了一个数据虚拟化服务的设计和实现，用于支持机器学习团队的多个并发ML工作流，解决了大量中间数据存储、处理和维护的挑战。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习团队需要处理多个并发的ML工作流，每个工作流涉及大量实验、迭代和协作活动，通常需要数月甚至数年时间。这产生了大量需要存储、处理和维护的中间数据，使得数据虚拟化成为支持ML工作流基础设施的关键技术。

Method: 设计和实现了一个数据虚拟化服务，重点关注其服务架构和服务操作。该服务专门为支持机器学习工作流而设计，能够处理多个应用程序和工作流的数据需求。

Result: 该基础设施目前支持六个ML应用程序，每个应用程序都有多个ML工作流。数据虚拟化服务具有良好的可扩展性，能够支持未来应用程序和工作流数量的增长。

Conclusion: 数据虚拟化服务成功解决了ML工作流中大量中间数据管理的挑战，为多个ML应用程序提供了有效支持，并具备了良好的可扩展性以适应未来的增长需求。

Abstract: Nowadays, machine learning (ML) teams have multiple concurrent ML workflows
for different applications. Each workflow typically involves many experiments,
iterations, and collaborative activities and commonly takes months and
sometimes years from initial data wrangling to model deployment.
Organizationally, there is a large amount of intermediate data to be stored,
processed, and maintained. \emph{Data virtualization} becomes a critical
technology in an infrastructure to serve ML workflows. In this paper, we
present the design and implementation of a data virtualization service,
focusing on its service architecture and service operations. The infrastructure
currently supports six ML applications, each with more than one ML workflow.
The data virtualization service allows the number of applications and workflows
to grow in the coming years.

</details>


### [9] [How Do Code Smells Affect Skill Growth in Scratch Novice Programmers?](https://arxiv.org/abs/2507.17314)
*Ricardo Hidalgo Aragón,Jesús M. González-Barahona,Gregorio Robles*

Main category: cs.SE

TL;DR: 这项研究通过分析约200万个Scratch项目，探索了新手程序员在块式编程环境中计算思维能力与代码异味（设计缺陷）之间的关系，旨在为编程教育提供循证指导。


<details>
  <summary>Details</summary>
Motivation: 虽然代码异味在专业代码中已被广泛研究，但在新手创建的块式编程项目中的重要性仍不明确。需要了解计算思维技能培养与设计问题之间的关系，以改进编程教育和自动化反馈系统。

Method: 随机抽样约200万个公开Scratch项目，使用开源检测工具提取9个计算思维分数和40个代码异味指标。采用描述性分析、稳健相关性测试、分层交叉验证和探索性机器学习模型进行分析，并通过定性检查来理解定量模式。

Result: 将提供首个大规模、细粒度的映射，连接特定计算思维能力与具体设计缺陷和反模式。为研究社区提供开放的匿名化数据集和可重现的分析流程。

Conclusion: 通过阐明编程习惯如何影响早期技能习得，这项工作将推进计算教育理论发展，并为可持续软件维护和演进提供实用工具支持。研究结果将为循证课程设计、自动化反馈系统和未来教育干预提供效应量基准。

Abstract: Context. Code smells, which are recurring anomalies in design or style, have
been extensively researched in professional code. However, their significance
in block-based projects created by novices is still largely unknown.
Block-based environments such as Scratch offer a unique, data-rich setting to
examine how emergent design problems intersect with the cultivation of
computational-thinking (CT) skills. Objective. This research explores the
connection between CT proficiency and design-level code smells--issues that may
hinder software maintenance and evolution--in programs created by Scratch
developers. We seek to identify which CT dimensions align most strongly with
which code smells and whether task context moderates those associations.
Method. A random sample of aprox. 2 million public Scratch projects is mined.
Using open-source linters, we extract nine CT scores and 40 code smell
indicators from these projects. After rigorous pre-processing, we apply
descriptive analytics, robust correlation tests, stratified cross-validation,
and exploratory machine-learning models; qualitative spot-checks contextualize
quantitative patterns. Impact. The study will deliver the first large-scale,
fine-grained map linking specific CT competencies to concrete design flaws and
antipatterns. Results are poised to (i) inform evidence-based curricula and
automated feedback systems, (ii) provide effect-size benchmarks for future
educational interventions, and (iii) supply an open, pseudonymized dataset and
reproducible analysis pipeline for the research community. By clarifying how
programming habits influence early skill acquisition, the work advances both
computing-education theory and practical tooling for sustainable software
maintenance and evolution.

</details>


### [10] [Roseau: Fast, Accurate, Source-based API Breaking Change Analysis in Java](https://arxiv.org/abs/2507.17369)
*Corentin Latappy,Thomas Degueule,Jean-Rémy Falleri,Romain Robbes,Lina Ochoa*

Main category: cs.SE

TL;DR: 本文介绍了Roseau，一个用于检测Java库API演化和破坏性变更的静态分析工具，能够从源代码或字节码构建API模型，在准确性和性能上均优于现有工具JApiCmp和Revapi。


<details>
  <summary>Details</summary>
Motivation: 现有的Java破坏性变更检测工具（如JApiCmp和Revapi）依赖二进制JAR文件，限制了其在大规模纵向研究和细粒度分析（如提交级别的BC检测）中的应用性。

Method: 开发了Roseau静态分析工具，能够从库代码构建技术无关的API模型，配备丰富的语义分析；可以从源代码或字节码构建API模型，并针对库历史的大规模纵向分析进行了优化。

Result: Roseau在准确性上超越了基准工具，F1分数达到0.99（JApiCmp为0.86，Revapi为0.91）；在性能测试中，能在2秒内检测数十万行代码库的版本间破坏性变更；通过分析Google Guava API的14年演化历程和6,839次提交，将分析时间从几天缩短到几分钟。

Conclusion: Roseau为API演化研究提供了更准确、高效的解决方案，克服了传统工具在纵向研究中的局限性，为大规模API演化分析开辟了新的可能性。

Abstract: Understanding API evolution and the introduction of breaking changes (BCs) in
software libraries is essential for library maintainers to manage backward
compatibility and for researchers to conduct empirical studies on software
library evolution. In Java, tools such as JApiCmp and Revapi are commonly used
to detect BCs between library releases, but their reliance on binary JARs
limits their applicability. This restriction hinders large-scale longitudinal
studies of API evolution and fine-grained analyses such as commit-level BC
detection. In this paper, we introduce Roseau, a novel static analysis tool
that constructs technology-agnostic API models from library code equipped with
rich semantic analyses. API models can be analyzed to study API evolution and
compared to identify BCs between any two versions of a library (releases,
commits, branches, etc.). Unlike traditional approaches, Roseau can build API
models from source code or bytecode, and is optimized for large-scale
longitudinal analyses of library histories. We assess the accuracy,
performance, and suitability of Roseau for longitudinal studies of API
evolution, using JApiCmp and Revapi as baselines. We extend and refine an
established benchmark of BCs and show that Roseau achieves higher accuracy (F1
= 0.99) than JApiCmp (F1 = 0.86) and Revapi (F1 = 0.91). We analyze 60 popular
libraries from Maven Central and find that Roseau delivers excellent
performance, detecting BCs between versions in under two seconds, including in
libraries with hundreds of thousands of lines of code. We further illustrate
the limitations of JApiCmp and Revapi for longitudinal studies and the novel
analysis capabilities offered by Roseau by tracking the evolution of Google's
Guava API and the introduction of BCs over 14 years and 6,839 commits, reducing
analysis times from a few days to a few minutes.

</details>


### [11] [Investigating Training Data Detection in AI Coders](https://arxiv.org/abs/2507.17389)
*Tianlin Li,Yunxiang Wei,Zhiming Li,Aishan Liu,Qing Guo,Xianglong Liu,Dongning Sun,Yang Liu*

Main category: cs.SE

TL;DR: 本文针对代码大语言模型（CodeLLMs）中的训练数据检测（TDD）问题进行了全面的实证研究，引入了CodeSnitch基准数据集，并评估了七种最先进的TDD方法在八个CodeLLMs上的性能表现。


<details>
  <summary>Details</summary>
Motivation: 代码大语言模型有时会输出包含专有或敏感代码片段的内容，这引发了对训练数据非合规使用的担忧，并带来隐私和知识产权风险。虽然现有的TDD方法在自然语言环境中表现良好，但它们在代码数据上的有效性仍未得到充分探索，特别是考虑到代码具有结构化语法和与自然语言不同的相似性标准。

Method: 研究者对七种最先进的TDD方法在源代码数据上进行了全面的实证研究，评估了它们在八个CodeLLMs上的性能。引入了CodeSnitch函数级基准数据集，包含9,000个代码样本（三种编程语言），每个样本都明确标记为包含或排除在CodeLLM训练中。设计了基于Type-1到Type-4代码克隆检测分类法的目标突变策略，在三种不同设置下测试TDD方法的鲁棒性。

Result: 研究提供了当前代码TDD技术的系统性评估，通过CodeSnitch基准数据集和突变策略测试，揭示了不同TDD方法在代码数据上的性能表现和鲁棒性差异。

Conclusion: 该研究为代码领域的训练数据检测提供了系统性的评估框架，并为未来开发更有效和鲁棒的检测方法提供了指导性见解。CodeSnitch基准数据集的引入为该领域的进一步研究奠定了基础。

Abstract: Recent advances in code large language models (CodeLLMs) have made them
indispensable tools in modern software engineering. However, these models
occasionally produce outputs that contain proprietary or sensitive code
snippets, raising concerns about potential non-compliant use of training data,
and posing risks to privacy and intellectual property. To ensure responsible
and compliant deployment of CodeLLMs, training data detection (TDD) has become
a critical task. While recent TDD methods have shown promise in natural
language settings, their effectiveness on code data remains largely
underexplored. This gap is particularly important given code's structured
syntax and distinct similarity criteria compared to natural language. To
address this, we conduct a comprehensive empirical study of seven
state-of-the-art TDD methods on source code data, evaluating their performance
across eight CodeLLMs. To support this evaluation, we introduce CodeSnitch, a
function-level benchmark dataset comprising 9,000 code samples in three
programming languages, each explicitly labeled as either included or excluded
from CodeLLM training. Beyond evaluation on the original CodeSnitch, we design
targeted mutation strategies to test the robustness of TDD methods under three
distinct settings. These mutation strategies are grounded in the
well-established Type-1 to Type-4 code clone detection taxonomy. Our study
provides a systematic assessment of current TDD techniques for code and offers
insights to guide the development of more effective and robust detection
methods in the future.

</details>


### [12] [AssertFlip: Reproducing Bugs via Inversion of LLM-Generated Passing Tests](https://arxiv.org/abs/2507.17542)
*Lara Khatib,Noble Saji Mathews,Meiyappan Nagappan*

Main category: cs.SE

TL;DR: AssertFlip是一种利用大语言模型自动生成Bug可复现测试的新技术，通过先生成通过测试再反转的方式，在SWT-Bench基准测试中达到43.6%的成功率，超越了现有所有方法。


<details>
  <summary>Details</summary>
Motivation: 开源和工业环境中大多数bug在报告时缺乏可执行的复现测试，这使得诊断和修复变得更加困难和耗时。现有方法直接生成失败测试的效果不佳，需要更好的自动化bug复现测试生成方法。

Method: AssertFlip采用与现有方法不同的策略：首先在有bug的代码上生成能够通过的测试，然后将这些测试反转，使其在存在bug时失败。该方法基于假设：大语言模型更擅长编写通过的测试，而不是故意崩溃或失败的测试。

Result: AssertFlip在SWT-Bench基准测试的排行榜上超越了所有已知技术，在SWT-Bench-Verified子集上实现了43.6%的失败到通过成功率。

Conclusion: 通过反转测试生成策略，AssertFlip证明了大语言模型在自动生成Bug可复现测试方面的有效性，为软件调试和修复过程提供了更好的解决方案。

Abstract: Bug reproduction is critical in the software debugging and repair process,
yet the majority of bugs in open-source and industrial settings lack executable
tests to reproduce them at the time they are reported, making diagnosis and
resolution more difficult and time-consuming. To address this challenge, we
introduce AssertFlip, a novel technique for automatically generating Bug
Reproducible Tests (BRTs) using large language models (LLMs). Unlike existing
methods that attempt direct generation of failing tests, AssertFlip first
generates passing tests on the buggy behaviour and then inverts these tests to
fail when the bug is present. We hypothesize that LLMs are better at writing
passing tests than ones that crash or fail on purpose. Our results show that
AssertFlip outperforms all known techniques in the leaderboard of SWT-Bench, a
benchmark curated for BRTs. Specifically, AssertFlip achieves a fail-to-pass
success rate of 43.6% on the SWT-Bench-Verified subset.

</details>


### [13] [CodeReasoner: Enhancing the Code Reasoning Ability with Reinforcement Learning](https://arxiv.org/abs/2507.17548)
*Lingxiao Tang,He Ye,Zhongxin Liu,Xiaoxue Ren,Lingfeng Bao*

Main category: cs.SE

TL;DR: 提出了CodeReasoner框架，通过构建高质量数据集和两阶段训练（指令微调+强化学习）来提升大语言模型的代码推理能力，在代码推理基准测试中相比现有方法提升27.1%-40.2%。


<details>
  <summary>Details</summary>
Motivation: 现有的代码推理方法主要依赖监督微调，但存在训练数据质量低和监督微调难以教授通用推理技能的问题，导致性能提升有限且泛化能力差。

Method: 提出CodeReasoner框架，包含：1）构建专注于Python程序核心执行逻辑的数据集；2）两阶段训练过程：先通过指令微调注入从强教师模型蒸馏的执行特定知识，再通过GRPO强化学习增强推理和泛化能力。

Result: 在三个广泛使用的代码推理基准测试中，7B模型相比现有方法提升27.1%-40.2%，在输入/输出和覆盖率预测等关键任务上与GPT-4o性能相当；14B模型在所有基准测试中都超越了GPT-4o。

Conclusion: CodeReasoner通过高质量数据集构建和两阶段训练有效提升了大语言模型的代码推理能力，消融研究证实了各训练阶段的有效性和推理链的重要性。

Abstract: Code reasoning is a fundamental capability for large language models (LLMs)
in the code domain. It involves understanding and predicting a program's
execution behavior, such as determining the output for a given input or whether
a specific statement will be executed. This capability is essential for
downstream tasks like debugging, code generation, and program repair. Prior
approaches mainly rely on supervised fine-tuning to improve performance in code
reasoning tasks. However, they often show limited gains and fail to generalize
across diverse scenarios. We argue this is due to two core issues: the low
quality of training data and the limitations of supervised fine-tuning, which
struggles to teach general reasoning skills. To address these challenges, we
propose CodeReasoner, a framework that spans both dataset construction and a
two-stage training process. First, we introduce a method to construct datasets
that focus on the core execution logic of Python programs. Next, we apply
instruction tuning to inject execution-specific knowledge distilled from a
powerful teacher model. We then enhance reasoning and generalization through
GRPO reinforcement learning on top of the fine-tuned model. Experiments on
three widely-used code reasoning benchmarks show that CodeReasoner improves
performance by 27.1% to 40.2% over prior methods using a 7B model. Notably, the
7B model matches GPT-4o on key tasks like input/output and coverage prediction.
When scaled to 14B, CodeReasoner outperforms GPT-4o across all benchmarks.
Ablation studies confirm the effectiveness of each training stage and highlight
the importance of reasoning chains.

</details>


### [14] [Contextual Code Retrieval for Commit Message Generation: A Preliminary Study](https://arxiv.org/abs/2507.17690)
*Bo Xiong,Linghao Zhang,Chong Wang,Peng Liang*

Main category: cs.SE

TL;DR: 本文提出了C3Gen方法，通过检索代码库中相关代码片段来增强提交消息生成，解决了仅依赖代码差异无法捕获完整上下文的问题，生成更全面、更有实用价值的提交消息。


<details>
  <summary>Details</summary>
Motivation: 现有的提交消息生成方法仅依赖代码差异进行直接映射，但原始代码差异无法捕获生成高质量、信息丰富的提交消息所需的完整上下文信息。

Method: 提出了基于上下文代码检索的C3Gen方法，通过从代码库中检索与提交相关的代码片段，并将其整合到模型输入中，在代码库范围内提供更丰富的上下文信息。

Result: 实验结果表明，通过将上下文代码纳入输入，C3Gen能够使模型有效利用额外信息，生成更全面、更有信息量的提交消息，在实际开发场景中具有更大的实用价值。人工评估也验证了开发者对C3Gen生成消息的积极感知。

Conclusion: C3Gen方法通过引入上下文代码检索显著改善了提交消息生成质量，进一步分析强调了基于相似度指标可靠性的担忧，并为提交消息生成领域提供了经验见解。

Abstract: A commit message describes the main code changes in a commit and plays a
crucial role in software maintenance. Existing commit message generation (CMG)
approaches typically frame it as a direct mapping which inputs a code diff and
produces a brief descriptive sentence as output. However, we argue that relying
solely on the code diff is insufficient, as raw code diff fails to capture the
full context needed for generating high-quality and informative commit
messages. In this paper, we propose a contextual code retrieval-based method
called C3Gen to enhance CMG by retrieving commit-relevant code snippets from
the repository and incorporating them into the model input to provide richer
contextual information at the repository scope. In the experiments, we
evaluated the effectiveness of C3Gen across various models using four objective
and three subjective metrics. Meanwhile, we design and conduct a human
evaluation to investigate how C3Gen-generated commit messages are perceived by
human developers. The results show that by incorporating contextual code into
the input, C3Gen enables models to effectively leverage additional information
to generate more comprehensive and informative commit messages with greater
practical value in real-world development scenarios. Further analysis
underscores concerns about the reliability of similaritybased metrics and
provides empirical insights for CMG.

</details>


### [15] [CASCADE: LLM-Powered JavaScript Deobfuscator at Google](https://arxiv.org/abs/2507.17691)
*Shan Jiang,Pranoy Kovuri,David Tao,Zhixun Tan*

Main category: cs.SE

TL;DR: 本文提出CASCADE，一种结合Gemini大模型和JavaScript中间表示(JSIR)的混合JavaScript反混淆方法，通过识别关键序言函数并进行代码转换，有效恢复原始代码语义，已在Google生产环境中部署并显著提升反混淆效率。


<details>
  <summary>Details</summary>
Motivation: JavaScript代码混淆严重阻碍了代码理解和分析，对软件测试、静态分析和恶意软件检测造成重大挑战。现有的静态和动态反混淆技术存在局限性，需要大量硬编码规则，缺乏可靠性和灵活性。

Method: 提出CASCADE混合方法，结合Gemini大模型的先进编码能力和JavaScript中间表示(JSIR)的确定性转换能力。使用Gemini识别关键序言函数（主流混淆技术的基础组件），然后利用JSIR进行后续代码转换，恢复原始字符串、API名称等语义元素。

Result: CASCADE有效克服了现有静态和动态反混淆技术的局限性，消除了数百到数千条硬编码规则，实现了可靠性和灵活性的平衡。方法已在Google生产环境中部署，在JavaScript反混淆效率方面取得显著改进，大幅减少了逆向工程工作量。

Conclusion: CASCADE成功将大模型能力与编译器中间表示技术相结合，为JavaScript反混淆提供了新的解决方案。该方法不仅提高了反混淆的准确性和效率，还具有良好的实用性，已在实际生产环境中验证了其有效性，为代码分析和恶意软件检测领域带来了重要进展。

Abstract: Software obfuscation, particularly prevalent in JavaScript, hinders code
comprehension and analysis, posing significant challenges to software testing,
static analysis, and malware detection. This paper introduces CASCADE, a novel
hybrid approach that integrates the advanced coding capabilities of Gemini with
the deterministic transformation capabilities of a compiler Intermediate
Representation (IR), specifically JavaScript IR (JSIR). By employing Gemini to
identify critical prelude functions, the foundational components underlying the
most prevalent obfuscation techniques, and leveraging JSIR for subsequent code
transformations, CASCADE effectively recovers semantic elements like original
strings and API names, and reveals original program behaviors. This method
overcomes limitations of existing static and dynamic deobfuscation techniques,
eliminating hundreds to thousands of hardcoded rules while achieving
reliability and flexibility. CASCADE is already deployed in Google's production
environment, demonstrating substantial improvements in JavaScript deobfuscation
efficiency and reducing reverse engineering efforts.

</details>


### [16] [Educational Insights from Code: Mapping Learning Challenges in Object-Oriented Programming through Code-Based Evidence](https://arxiv.org/abs/2507.17743)
*Andre Menolli,Bruno Strik*

Main category: cs.SE

TL;DR: 本研究探索了面向对象编程中代码问题指标与学习困难之间的关系，通过定性分析和文献综述，开发了一个将代码问题与学习挑战相关联的概念地图，并通过专家评估验证了其在教育环境中的适用性。


<details>
  <summary>Details</summary>
Motivation: 面向对象编程对计算机科学本科生来说具有挑战性，特别是在理解封装、继承和多态等抽象概念方面。虽然文献中概述了通过源代码分析识别面向对象编程中潜在设计和编码问题的各种方法（如代码异味和SOLID原则），但很少有研究探索这些代码级别的问题如何与面向对象编程的学习困难相关联。

Method: 使用定性分析方法识别学习困难的主要类别，通过文献综述建立学习困难、代码异味和SOLID原则违反之间的联系，开发了一个将代码相关问题与面向对象编程中特定学习挑战相链接的概念地图。

Result: 成功开发了一个概念地图，该地图将代码相关问题与面向对象编程中的特定学习挑战相关联。模型经过专家评估，专家将其应用于学生代码分析中以评估其相关性和适用性。

Conclusion: 研究建立了代码问题指标与面向对象编程学习困难之间的关系，开发的概念地图在教育环境中具有相关性和适用性，为理解和解决学生在面向对象编程学习中遇到的困难提供了新的视角。

Abstract: Object-Oriented programming is frequently challenging for undergraduate
Computer Science students, particularly in understanding abstract concepts such
as encapsulation, inheritance, and polymorphism. Although the literature
outlines various methods to identify potential design and coding issues in
object-oriented programming through source code analysis, such as code smells
and SOLID principles, few studies explore how these code-level issues relate to
learning difficulties in Object-Oriented Programming. In this study, we explore
the relationship of the code issue indicators with common challenges
encountered during the learning of object-oriented programming. Using
qualitative analysis, we identified the main categories of learning
difficulties and, through a literature review, established connections between
these difficulties, code smells, and violations of the SOLID principles. As a
result, we developed a conceptual map that links code-related issues to
specific learning challenges in Object-Oriented Programming. The model was then
evaluated by an expert who applied it in the analysis of the student code to
assess its relevance and applicability in educational contexts.

</details>
