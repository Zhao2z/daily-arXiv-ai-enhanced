<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 35]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Impact of Code Context and Prompting Strategies on Automated Unit Test Generation with Modern General-Purpose Large Language Models](https://arxiv.org/abs/2507.14256)
*Jakub Walczak,Piotr Tomalak,Artur Laskowski*

Main category: cs.SE

TL;DR: 论文研究了代码上下文和提示策略对大型语言模型（LLM）生成单元测试质量和充分性的影响，发现包含文档字符串显著提升代码充分性，而链式思维提示策略效果最佳。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用生成式AI自动生成单元测试，以提高软件开发阶段的效率。

Method: 评估不同LLM在代码上下文和提示策略下的表现，重点关注文档字符串和链式思维提示。

Result: 链式思维提示策略表现最佳，达到96.3%分支覆盖率和57%平均变异分数，M5（Gemini 2.5 Pro）模型表现最优。

Conclusion: 代码上下文和提示策略对生成单元测试的质量有显著影响，链式思维提示和文档字符串是关键因素。

Abstract: Generative AI is gaining increasing attention in software engineering, where
testing remains an indispensable reliability mechanism. According to the widely
adopted testing pyramid, unit tests constitute the majority of test cases and
are often schematic, requiring minimal domain expertise. Automatically
generating such tests under the supervision of software engineers can
significantly enhance productivity during the development phase of the software
lifecycle.
  This paper investigates the impact of code context and prompting strategies
on the quality and adequacy of unit tests generated by various large language
models (LLMs) across several families. The results show that including
docstrings notably improves code adequacy, while further extending context to
the full implementation yields definitely smaller gains. Notably, the
chain-of-thought prompting strategy -- applied even to 'reasoning' models --
achieves the best results, with up to 96.3\% branch coverage, a 57\% average
mutation score, and near-perfect compilation success rate. Among the evaluated
models, M5 (Gemini 2.5 Pro) demonstrated superior performance in both mutation
score and branch coverage being still in top in terms of compilation success
rate.
  All the code and resulting test suites are publicly available at
https://github.com/peetery/LLM-analysis.

</details>


### [2] [Leveraging LLMs for Formal Software Requirements -- Challenges and Prospects](https://arxiv.org/abs/2507.14330)
*Arshad Beg,Diarmuid O'Donoghue,Rosemary Monahan*

Main category: cs.SE

TL;DR: VERIFAI项目旨在通过NLP、本体建模和LLM等技术，自动化或半自动化地从非正式需求生成可验证的规范，解决形式化验证中的关键挑战。


<details>
  <summary>Details</summary>
Motivation: 形式化验证在安全关键系统开发中至关重要，但从非正式需求生成形式化规范仍是一个主要挑战。

Method: 结合NLP、本体建模、构件重用和LLM等技术，探索自动化或半自动化的解决方案。

Result: 初步文献综述揭示了生成可验证规范中的常见挑战和潜在研究方向。

Conclusion: VERIFAI项目为填补非正式需求与形式化规范之间的鸿沟提供了前瞻性研究框架。

Abstract: Software correctness is ensured mathematically through formal verification,
which involves the resources of generating formal requirement specifications
and having an implementation that must be verified. Tools such as
model-checkers and theorem provers ensure software correctness by verifying the
implementation against the specification. Formal methods deployment is
regularly enforced in the development of safety-critical systems e.g.
aerospace, medical devices and autonomous systems. Generating these
specifications from informal and ambiguous natural language requirements
remains the key challenge. Our project, VERIFAI^{1}, aims to investigate
automated and semi-automated approaches to bridge this gap, using techniques
from Natural Language Processing (NLP), ontology-based domain modelling,
artefact reuse, and large language models (LLMs). This position paper presents
a preliminary synthesis of relevant literature to identify recurring challenges
and prospective research directions in the generation of verifiable
specifications from informal requirements.

</details>


### [3] [Developing Shared Vocabulary System For Collaborative Software Engineering](https://arxiv.org/abs/2507.14396)
*Carey Lai Zheng Hui,Johnson Britto Jessia Esther Leena,Kumuthini Subramanian,Zhao Chenyu,Shubham Rajeshkumar Jariwala*

Main category: cs.SE

TL;DR: 研究探讨了软件工程中沟通障碍的技术因素，并提出通过共享词汇系统改善文档和代码库的沟通效果。采用设计科学研究框架，通过问题识别、方法开发和实证验证三阶段，发现共享词汇系统显著提升了信息密度、文档清晰度和协作效率。


<details>
  <summary>Details</summary>
Motivation: 软件工程中的沟通障碍常导致误解、低效和缺陷，研究旨在探索技术因素并提出改进方法。

Method: 采用设计科学研究框架，分三阶段：问题识别（主题分析和半结构化访谈）、方法开发（基于扎根理论设计词汇系统）、实证验证（控制实验）。

Result: 共享词汇系统虽初期增加开销，但显著提升了信息密度、文档清晰度和协作效率。

Conclusion: 研究为改善软件工程沟通实践提供了可行见解，并指出了未来研究方向。

Abstract: Effective communication is a critical factor in successful software
engineering collaboration. However, communication gaps remain a persistent
challenge, often leading to misunderstandings, inefficiencies, and defects.
This research investigates the technical factors contributing to such
misunderstandings and explores the measurable benefits of establishing shared
vocabulary systems within software documentation and codebases. Using a Design
Science Research (DSR) framework, the study was structured into three iterative
phases: problem identification, method development, and empirical validation.
The problem identification phase involved thematic analysis of communication
data and semi-structured interviews, revealing key factors such as ambiguous
messaging, misalignment in documentation, inconsistent code review feedback,
and API integration miscommunication. Grounded Theory principles were employed
to design a structured methodology for collaborative vocabulary development.
Empirical validation through controlled experiments demonstrated that while
initial adoption introduced overhead, the shared vocabulary system
significantly improved information density, documentation clarity, and
collaboration efficiency over time. Findings offer actionable insights for
improving communication practices in software engineering, while also
identifying limitations and directions for future research.

</details>


### [4] [On the Effect of Token Merging on Pre-trained Models for Code](https://arxiv.org/abs/2507.14423)
*Mootez Saad,Hao Li,Tushar Sharma,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 研究了合并属于同一语义单元的隐藏表示对代码语言模型的影响，提出了两种策略并验证了其在计算效率和下游任务中的效果。


<details>
  <summary>Details</summary>
Motivation: 传统代码语言模型的标记化输出较长，可能导致计算开销增加，因此探索合并语义单元隐藏表示的方法。

Method: 提出两种策略：基于平均表示和学习的方法，并在六个代码语言模型和三个软件工程任务中实验。

Result: 计算操作减少1%至19%，下游任务中漏洞检测F1下降1.82分，代码翻译CodeBLEU提升2.47分。

Conclusion: 合并隐藏表示可提升计算效率，但对不同任务效果不一，为代码语言模型优化提供了新方向。

Abstract: Tokenization is a fundamental component of language models for code. It
involves breaking down the input into units that are later passed to the
language model stack to learn high-dimensional representations used in various
contexts, from classification to generation. However, the output of these
tokenizers is often longer than that traditionally used in compilers and
interpreters. This could result in undesirable effects, such as increased
computational overhead. In this work, we investigate the effect of merging the
hidden representations of subtokens that belong to the same semantic unit, such
as subtokens that form a single identifier. We propose two strategies: one
based on averaging the representations and another that leverages a
learning-based approach. Both methods can be seamlessly integrated with
existing language models for code. We conduct experiments using six language
models for code: CodeBERT, GraphCodeBERT, UniXCoder, CdoeT5, CodeT5+ (220M),
and CodeT5+ (770M), across three software engineering tasks: vulnerability
detection, code classification, and code translation. Results show that these
strategies can reduce the number of floating-point operations by $1\%$ to
$19\%$. Regarding downstream performance, the most significant degradation was
observed in the vulnerability detection task, where the F1 score decreased by
$1.82$ points compared to the baseline. In contrast, for code translation, we
observed an improvement of $2.47$ points in CodeBLEU. This work contributes to
the broader effort of improving language models for code across multiple
dimensions, including both computational efficiency and downstream performance.

</details>


### [5] [Architectural Degradation: Definition, Motivations, Measurement and Remediation Approaches](https://arxiv.org/abs/2507.14547)
*Noman Ahmad,Ruoyu Su,Matteo Esposito,Andrea Janes,Valentina Lenarduzzi,Davide Taibi*

Main category: cs.SE

TL;DR: 该研究通过多声部文献综述，统一了架构退化的定义、原因、指标和修复策略，揭示了其从技术问题到社会技术问题的转变，并指出当前工具在持续修复方面的不足。


<details>
  <summary>Details</summary>
Motivation: 架构退化影响系统质量和可维护性，但现有文献定义和修复策略分散，研究旨在统一理解并填补研究空白。

Method: 对108项研究进行多声部文献综述，提取定义、原因、指标、测量方法、工具和修复策略，并开发分类法。

Result: 架构退化从低层问题转为社会技术问题，定义涵盖代码违规、设计漂移和结构衰退。识别了54种指标和31种测量技术，但工具多用于检测而非持续修复。

Conclusion: 研究呼吁整合指标、工具和修复逻辑，推动可持续架构的整体主动策略。

Abstract: Architectural degradation, also known as erosion, decay, or aging, impacts
system quality, maintainability, and adaptability. Although widely
acknowledged, current literature shows fragmented definitions, metrics, and
remediation strategies. Our study aims to unify understanding of architectural
degradation by identifying its definitions, causes, metrics, tools, and
remediation approaches across academic and gray literature. We conducted a
multivocal literature review of 108 studies extracting definitions, causes,
metrics, measurement approaches, tools, and remediation strategies. We
developed a taxonomy encompassing architectural, code, and process debt to
explore definition evolution, methodological trends, and research gaps.
Architectural degradation has shifted from a low-level issue to a
socio-technical concern. Definitions now address code violations, design drift,
and structural decay. Causes fall under architectural (e.g., poor
documentation), code (e.g., hasty fixes), and process debt (e.g., knowledge
loss). We identified 54 metrics and 31 measurement techniques, focused on
smells, cohesion/coupling, and evolution. Yet, most tools detect issues but
rarely support ongoing or preventive remediation. Degradation is both technical
and organizational. While detection is well-studied, continuous remediation
remains lacking. Our study reveals missed integration between metrics, tools,
and repair logic, urging holistic, proactive strategies for sustainable
architecture.

</details>


### [6] [Emerging Trends in Software Architecture from the Practitioners Perspective: A Five Year Review](https://arxiv.org/abs/2507.14554)
*Ruoyu Su,Noman ahmad,Matteo Esposito,Andrea Janes,Davide Taibi,Valentina Lenarduzzi*

Main category: cs.SE

TL;DR: 研究分析了五年内八个行业会议中的软件架构趋势，发现Kubernetes、Serverless等技术主导当前实践，主要应用于DevOps后期阶段。


<details>
  <summary>Details</summary>
Motivation: 理解云计算、微服务和容器等技术对软件架构实践的多样化影响。

Method: 分析5,677个行业会议演讲，使用大语言模型和专家验证提取技术及其用途和上下文。

Result: 发现450种技术中，Kubernetes、Cloud Native等技术占据主导地位，主要涉及部署、通信和AI等领域。

Conclusion: 少数核心技术主导当前架构实践，研究需更全面关注架构设计、质量和演变。

Abstract: Software architecture plays a central role in the design, development, and
maintenance of software systems. With the rise of cloud computing,
microservices, and containers, architectural practices have diversified.
Understanding these shifts is vital. This study analyzes software architecture
trends across eight leading industry conferences over five years. We
investigate the evolution of software architecture by analyzing talks from top
practitioner conferences, focusing on the motivations and contexts driving
technology adoption. We analyzed 5,677 talks from eight major industry
conferences, using large language models and expert validation to extract
technologies, their purposes, and usage contexts. We also explored how
technologies interrelate and fit within DevOps and deployment pipelines. Among
450 technologies, Kubernetes, Cloud Native, Serverless, and Containers dominate
by frequency and centrality. Practitioners present technology mainly related to
deployment, communication, AI, and observability. We identify five technology
communities covering automation, coordination, cloud AI, monitoring, and
cloud-edge. Most technologies span multiple DevOps stages and support hybrid
deployment. Our study reveals that a few core technologies, like Kubernetes and
Serverless, dominate the contemporary software architecture practice. These are
mainly applied in later DevOps stages, with limited focus on early phases like
planning and coding. We also show how practitioners frame technologies by
purpose and context, reflecting evolving industry priorities. Finally, we
observe how only research can provide a more holistic lens on architectural
design, quality, and evolution.

</details>


### [7] [Harnessing LLMs for Document-Guided Fuzzing of OpenCV Library](https://arxiv.org/abs/2507.14558)
*Bin Duan,Tarek Mahmud,Meiru Che,Yan Yan,Naipeng Dong,Dan Dongseong Kim,Guowei Yang*

Main category: cs.SE

TL;DR: VISTAFUZZ是一种利用大型语言模型（LLMs）对OpenCV库进行文档引导模糊测试的新技术，成功检测并修复了多个bug。


<details>
  <summary>Details</summary>
Motivation: OpenCV库作为最流行的开源计算机视觉库，其可靠性至关重要。现有方法在测试其API时存在不足，需要更高效的测试技术。

Method: VISTAFUZZ利用LLMs解析API文档，提取参数约束和依赖关系，生成系统化的测试输入。

Result: 测试了330个API，发现17个新bug，其中10个被确认，5个已修复。

Conclusion: VISTAFUZZ显著提升了OpenCV库的测试效率和可靠性，为类似系统的测试提供了新思路。

Abstract: The combination of computer vision and artificial intelligence is
fundamentally transforming a broad spectrum of industries by enabling machines
to interpret and act upon visual data with high levels of accuracy. As the
biggest and by far the most popular open-source computer vision library, OpenCV
library provides an extensive suite of programming functions supporting
real-time computer vision. Bugs in the OpenCV library can affect the downstream
computer vision applications, and it is critical to ensure the reliability of
the OpenCV library. This paper introduces VISTAFUZZ, a novel technique for
harnessing large language models (LLMs) for document-guided fuzzing of the
OpenCV library. VISTAFUZZ utilizes LLMs to parse API documentation and obtain
standardized API information. Based on this standardized information, VISTAFUZZ
extracts constraints on individual input parameters and dependencies between
these. Using these constraints and dependencies, VISTAFUZZ then generates new
input values to systematically test each target API. We evaluate the
effectiveness of VISTAFUZZ in testing 330 APIs in the OpenCV library, and the
results show that VISTAFUZZ detected 17 new bugs, where 10 bugs have been
confirmed, and 5 of these have been fixed.

</details>


### [8] [A first look at License Variants in the PyPI Ecosystem](https://arxiv.org/abs/2507.14594)
*Weiwei Xu,Hengzhi Ye,Kai Gao,Minghui Zhou*

Main category: cs.SE

TL;DR: 论文研究了开源许可证变体在PyPI生态系统中的影响，发现文本变体常见但实质性修改较少，但仍导致显著的合规问题。提出了LV-Parser和LV-Compat工具，显著提高了分析效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 开源许可证变体在现代软件系统中普遍存在且影响重大，但现有工具未能有效处理，导致许可证分析的效率和效果受限。

Method: 通过实证研究分析PyPI生态系统中的许可证变体，并开发了基于差异分析和大语言模型的LV-Parser工具，以及自动化检测许可证不兼容性的LV-Compat流程。

Result: 研究发现许可证文本变体常见，但仅2%为实质性修改；LV-Parser准确率达0.936且计算成本降低30%，LV-Compat检测到的不兼容包数量是现有方法的5.2倍，精度为0.98。

Conclusion: 该研究填补了许可证变体知识的空白，并为开发者提供了实用工具，以应对开源许可证的复杂性。

Abstract: Open-source licenses establish the legal foundation for software reuse, yet
license variants, including both modified standard licenses and custom-created
alternatives, introduce significant compliance complexities. Despite their
prevalence and potential impact, these variants are poorly understood in modern
software systems, and existing tools do not account for their existence,
leading to significant challenges in both effectiveness and efficiency of
license analysis. To fill this knowledge gap, we conduct a comprehensive
empirical study of license variants in the PyPI ecosystem. Our findings show
that textual variations in licenses are common, yet only 2% involve substantive
modifications. However, these license variants lead to significant compliance
issues, with 10.7% of their downstream dependencies found to be
license-incompatible.
  Inspired by our findings, we introduce LV-Parser, a novel approach for
efficient license variant analysis leveraging diff-based techniques and large
language models, along with LV-Compat, an automated pipeline for detecting
license incompatibilities in software dependency networks. Our evaluation
demonstrates that LV-Parser achieves an accuracy of 0.936 while reducing
computational costs by 30%, and LV-Compat identifies 5.2 times more
incompatible packages than existing methods with a precision of 0.98.
  This work not only provides the first empirical study into license variants
in software packaging ecosystem but also equips developers and organizations
with practical tools for navigating the complex landscape of open-source
licensing.

</details>


### [9] [An Efficient Algorithm for Generating Minimal Unique-Cause MC/DC Test cases for Singular Boolean Expressions](https://arxiv.org/abs/2507.14687)
*Robin Lee,Youngho Nam*

Main category: cs.SE

TL;DR: 本文提出了一种名为'Robin's Rule'的确定性算法，用于高效生成满足Unique-Cause MC/DC的最小测试集，适用于Singular Boolean Expressions（SBEs），验证了其优于商业工具的效率和覆盖能力。


<details>
  <summary>Details</summary>
Motivation: 尽管Unique-Cause MC/DC在关键系统中具有最高保障，但其高效测试生成方法的研究不足，尤其是在SBEs占主导的实际系统中。

Method: 提出'Robin's Rule'算法，直接构建N + 1个测试用例的最小集，确保SBEs的100% Unique-Cause MC/DC覆盖，无需生成完整真值表。

Result: 通过TCAS-II规范的SBEs重构验证，算法始终实现100%覆盖，且测试用例数量为理论最小值，效率高于商业工具。

Conclusion: 'Robin's Rule'为安全关键系统验证提供了实用且理论最优的解决方案，兼具严谨性和高效性。

Abstract: Modified Condition/Decision Coverage (MC/DC) is a mandatory structural
coverage criterion for ensuring the reliability and safety of critical systems.
While its strictest form, Unique-Cause MC/DC, offers the highest assurance,
research on its efficient test generation has been lacking. This gap is
particularly significant, as an analysis of large-scale avionics systems shows
that 99.7% of all conditional decisions are, in fact, Singular Boolean
Expressions (SBEs) the ideal structure for applying Unique-Cause MC/DC. This
paper proposes 'Robin's Rule', a deterministic algorithm that directly
constructs a minimal test set of N + 1 cases to guarantee 100% Unique-Cause
MC/DC for SBEs with N conditions, without generating a full truth table. To
validate our approach, we constructed a benchmark by reformulating the TCAS-II
specifications into SBEs and verified the results using an industry-standard,
certified commercial tool. The results confirm that our method consistently
achieves 100% coverage with the theoretical minimum number of tests and is more
efficient than the commercial tool. This work provides a practical and provably
optimal solution for verifying safety-critical systems, ensuring both rigor and
efficiency.

</details>


### [10] [HistoryFinder: Advancing Method-Level Source Code History Generation with Accurate Oracles and Enhanced Algorithm](https://arxiv.org/abs/2507.14716)
*Shahidul Islam,Ashik Aowal,Md Sharif Uddin,Shaiful Chowdhury*

Main category: cs.SE

TL;DR: 论文提出了一种新工具HistoryFinder，用于高效准确地重建方法变更历史，并通过系统构建的新oracle验证其优于现有工具的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法历史生成工具的评估因oracle不准确而受限，需改进工具以提升准确性和运行效率。

Method: 构建两个新oracle（修正的CodeShovel oracle和HistoryFinder oracle），结合自动分析和专家验证，开发HistoryFinder工具。

Result: 在400个方法的评估中，HistoryFinder在精度、召回率和F1分数上优于CodeShovel、CodeTracker等工具，且运行时间表现优异。

Conclusion: HistoryFinder在准确性和效率上均表现最佳，适合需要高精度和高效率的场景，并提供多种使用方式。

Abstract: Reconstructing a method's change history efficiently and accurately is
critical for many software engineering tasks, including maintenance,
refactoring, and comprehension. Despite the availability of method history
generation tools such as CodeShovel and CodeTracker, existing evaluations of
their effectiveness are limited by inaccuracies in the ground truth oracles
used. In this study, we systematically construct two new oracles -- the
corrected CodeShovel oracle and a newly developed HistoryFinder oracle -- by
combining automated analysis with expert-guided manual validation. We also
introduce HistoryFinder, a new method history generation tool designed to
improve not only the accuracy and completeness of method change histories but
also to offer competitive runtime performance. Through extensive evaluation
across 400 methods from 40 open-source repositories, we show that HistoryFinder
consistently outperforms CodeShovel, CodeTracker, IntelliJ, and Git-based
baselines in terms of precision, recall, and F1 score. Moreover, HistoryFinder
achieves competitive runtime performance, offering the lowest mean and median
execution times among all the research-based tools.
  While Git-based tools exhibit the fastest runtimes, this efficiency comes at
the cost of significantly lower precision and recall -- leaving HistoryFinder
as the best overall choice when both accuracy and efficiency are important. To
facilitate adoption, we provide a web interface, CLI, and Java library for
flexible usage.

</details>


### [11] [Investigating the Role of LLMs Hyperparameter Tuning and Prompt Engineering to Support Domain Modeling](https://arxiv.org/abs/2507.14735)
*Vladyslav Bulhakov,Giordano d'Aloisio,Claudio Di Sipio,Antinisca Di Marco,Davide Di Ruscio*

Main category: cs.SE

TL;DR: 本文探讨了通过超参数调整和提示工程提升Llama 3.1模型在生成领域模型中的准确性，展示了在特定领域（如医疗数据模型）中的显著改进。


<details>
  <summary>Details</summary>
Motivation: 通用大型语言模型（LLMs）在领域建模中存在局限性，而微调模型需要大量计算资源且可能导致灾难性遗忘。本文旨在通过超参数调整和提示工程优化模型性能。

Method: 采用基于搜索的方法对Llama 3.1模型的超参数进行调整，并结合提示工程，针对医疗数据模型进行优化。

Result: 优化后的模型在医疗数据模型中表现显著优于基线LLM，并在十个不同应用领域中测试了优化超参数的适用性。

Conclusion: 虽然解决方案并非普遍适用，但超参数调整与提示工程的结合能显著提升大多数领域模型的生成质量。

Abstract: The introduction of large language models (LLMs) has enhanced automation in
software engineering tasks, including in Model Driven Engineering (MDE).
However, using general-purpose LLMs for domain modeling has its limitations.
One approach is to adopt fine-tuned models, but this requires significant
computational resources and can lead to issues like catastrophic forgetting.
  This paper explores how hyperparameter tuning and prompt engineering can
improve the accuracy of the Llama 3.1 model for generating domain models from
textual descriptions. We use search-based methods to tune hyperparameters for a
specific medical data model, resulting in a notable quality improvement over
the baseline LLM. We then test the optimized hyperparameters across ten diverse
application domains.
  While the solutions were not universally applicable, we demonstrate that
combining hyperparameter tuning with prompt engineering can enhance results
across nearly all examined domain models.

</details>


### [12] [Toward Inclusive AI-Driven Development: Exploring Gender Differences in Code Generation Tool Interactions](https://arxiv.org/abs/2507.14770)
*Manaal Basha,Ivan Beschastnikh,Gema Rodriguez-Perez,Cleidson R. B. de Souza*

Main category: cs.SE

TL;DR: 研究探讨代码生成工具（CGTs）在不同性别开发者中的使用差异，分析其对任务表现和认知负荷的影响，旨在推动公平和包容性设计。


<details>
  <summary>Details</summary>
Motivation: 随着代码生成工具的普及，其在不同用户群体中的公平性和包容性尚未充分研究，尤其是性别差异可能影响使用效果。

Method: 采用混合实验设计，54名参与者按性别均分，完成编程任务（使用CGT和仅互联网），收集认知负荷、任务表现等数据，并进行统计分析。

Result: 预期发现性别在CGT交互和任务表现上的差异，为工具设计提供改进方向。

Conclusion: 研究虽未完成，但为CGT的公平性、透明性和包容性设计奠定基础，推动AI工具的公平发展。

Abstract: Context: The increasing reliance on Code Generation Tools (CGTs), such as
Windsurf and GitHub Copilot, are revamping programming workflows and raising
critical questions about fairness and inclusivity. While CGTs offer potential
productivity enhancements, their effectiveness across diverse user groups have
not been sufficiently investigated. Objectives: We hypothesize that developers'
interactions with CGTs vary based on gender, influencing task outcomes and
cognitive load, as prior research suggests that gender differences can affect
technology use and cognitive processing. Methods: The study will employ a
mixed-subjects design with 54 participants, evenly divided by gender for a
counterbalanced design. Participants will complete two programming tasks
(medium to hard difficulty) with only CGT assistance and then with only
internet access. Task orders and conditions will be counterbalanced to mitigate
order effects. Data collection will include cognitive load surveys, screen
recordings, and task performance metrics such as completion time, code
correctness, and CGT interaction behaviors. Statistical analyses will be
conducted to identify statistically significant differences in CGT usage.
Expected Contributions: Our work can uncover gender differences in CGT
interaction and performance among developers. Our findings can inform future
CGT designs and help address usability and potential disparities in interaction
patterns across diverse user groups. Conclusion: While results are not yet
available, our proposal lays the groundwork for advancing fairness,
accountability, transparency, and ethics (FATE) in CGT design. The outcomes are
anticipated to contribute to inclusive AI practices and equitable tool
development for all users.

</details>


### [13] [VeriOpt: PPA-Aware High-Quality Verilog Generation via Multi-Role LLMs](https://arxiv.org/abs/2507.14776)
*Kimia Tasnia,Alexander Garcia,Tasnuva Farheen,Sazadur Rahman*

Main category: cs.SE

TL;DR: VeriOpt是一个通过角色提示和PPA优化使LLM生成高质量Verilog代码的框架，显著提升PPA指标。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在硬件设计中主要关注功能正确性，忽视了PPA指标，VeriOpt旨在填补这一空白。

Method: 采用角色提示（如规划师、程序员、评审员）和PPA感知优化，结合多模态反馈生成PPA高效代码。

Result: 实验显示，相比基线，功耗降低88%，面积减少76%，时序闭合提升73%，功能正确率达86%。

Conclusion: VeriOpt通过结合功能正确性和PPA优化，推动了AI驱动的硬件设计发展。

Abstract: The rapid adoption of large language models(LLMs) in hardware design has
primarily focused on generating functionally correct Verilog code, overlooking
critical Power Performance-Area(PPA) metrics essential for industrial-grade
designs. To bridge this gap, we propose VeriOpt, a novel framework that
leverages role-based prompting and PPA-aware optimization to enable LLMs to
produce high-quality, synthesizable Verilog. VeriOpt structures LLM
interactions into specialized roles (e.g., Planner, Programmer, Reviewer,
Evaluator) to emulate human design workflows, while integrating PPA constraints
directly into the prompting pipeline. By combining multi-modal feedback (e.g.,
synthesis reports, timing diagrams) with PPA aware prompting, VeriOpt achieves
PPA-efficient code generation without sacrificing functional correctness.
Experimental results demonstrate up to 88% reduction in power, 76% reduction in
area and 73% improvement in timing closure compared to baseline LLM-generated
RTL, validated using industry standard EDA tools. At the same time achieves 86%
success rate in functionality evaluation. Our work advances the
state-of-the-art AI-driven hardware design by addressing the critical gap
between correctness and quality, paving the way for reliable LLM adoption in
production workflows.

</details>


### [14] [Enhancing Repository-Level Code Generation with Call Chain-Aware Multi-View Context](https://arxiv.org/abs/2507.14791)
*Yang Liu,Li Zhang,Fang Liu,Zhuohang Wang,Donglin Wei,Zhishuo Yang,Kechi Zhang,Jia Li,Lin Shi*

Main category: cs.SE

TL;DR: RepoScope通过多视角上下文和结构语义图改进仓库级代码生成，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在识别仓库语义和结构关系上存在不足，导致上下文信息不充分。

Method: 构建Repository Structural Semantic Graph (RSSG)，结合四视角上下文和调用链预测方法。

Result: 在CoderEval和DevEval基准测试中，pass@1分数相对提升36.35%。

Conclusion: RepoScope无需额外训练，高效且通用，能有效提升代码生成质量。

Abstract: Repository-level code generation aims to generate code within the context of
a specified repository. Existing approaches typically employ
retrieval-augmented generation (RAG) techniques to provide LLMs with relevant
contextual information extracted from the repository. However, these approaches
often struggle with effectively identifying truly relevant contexts that
capture the rich semantics of the repository, and their contextual perspectives
remains narrow. Moreover, most approaches fail to account for the structural
relationships in the retrieved code during prompt construction, hindering the
LLM's ability to accurately interpret the context. To address these issues, we
propose RepoScope, which leverages call chain-aware multi-view context for
repository-level code generation. RepoScope constructs a Repository Structural
Semantic Graph (RSSG) and retrieves a comprehensive four-view context,
integrating both structural and similarity-based contexts. We propose a novel
call chain prediction method that utilizes the repository's structural
semantics to improve the identification of callees in the target function.
Additionally, we present a structure-preserving serialization algorithm for
prompt construction, ensuring the coherence of the context for the LLM.
Notably, RepoScope relies solely on static analysis, eliminating the need for
additional training or multiple LLM queries, thus ensuring both efficiency and
generalizability. Evaluation on widely-used repository-level code generation
benchmarks (CoderEval and DevEval) demonstrates that RepoScope outperforms
state-of-the-art methods, achieving up to a 36.35% relative improvement in
pass@1 scores. Further experiments emphasize RepoScope's potential to improve
code generation across different tasks and its ability to integrate effectively
with existing approaches.

</details>


### [15] [Think Like an Engineer: A Neuro-Symbolic Collaboration Agent for Generative Software Requirements Elicitation and Self-Review](https://arxiv.org/abs/2507.14969)
*Sai Zhang,Zhenchang Xing,Jieshan Chen,Dehai Zhao,Zizhong Zhu,Xiaowang Zhang,Zhiyong Feng,Xiaohong Li*

Main category: cs.SE

TL;DR: RequireCEG通过神经符号协作架构引入因果效应图（CEGs），解决用户需求描述模糊性问题，提升生成软件开发的覆盖率和多样性。


<details>
  <summary>Details</summary>
Motivation: 终端用户缺乏软件工程知识，其需求描述常模糊不清，现有结构化语言（如Gherkin）难以表达因果逻辑。

Method: RequireCEG使用特征树分层分析用户叙述，构建自修复CEGs，捕获原子前提与行为动作的因果关系，并优化Gherkin场景。

Result: 实验显示，该方法覆盖率达87%，多样性提升51.88%。

Conclusion: RequireCEG有效解决了需求模糊性问题，提升了生成软件开发的准确性和多样性。

Abstract: The vision of End-User Software Engineering (EUSE) is to empower
non-professional users with full control over the software development
lifecycle. It aims to enable users to drive generative software development
using only natural language requirements. However, since end-users often lack
knowledge of software engineering, their requirement descriptions are
frequently ambiguous, raising significant challenges to generative software
development. Although existing approaches utilize structured languages like
Gherkin to clarify user narratives, they still struggle to express the causal
logic between preconditions and behavior actions. This paper introduces
RequireCEG, a requirement elicitation and self-review agent that embeds
causal-effect graphs (CEGs) in a neuro-symbolic collaboration architecture.
RequireCEG first uses a feature tree to analyze user narratives hierarchically,
clearly defining the scope of software components and their system behavior
requirements. Next, it constructs the self-healing CEGs based on the elicited
requirements, capturing the causal relationships between atomic preconditions
and behavioral actions. Finally, the constructed CEGs are used to review and
optimize Gherkin scenarios, ensuring consistency between the generated Gherkin
requirements and the system behavior requirements elicited from user
narratives. To evaluate our method, we created the RGPair benchmark dataset and
conducted extensive experiments. It achieves an 87% coverage rate and raises
diversity by 51.88%.

</details>


### [16] [The Rise of AI Teammates in Software Engineering (SE) 3.0: How Autonomous Coding Agents Are Reshaping Software Engineering](https://arxiv.org/abs/2507.15003)
*Hao Li,Haoxiang Zhang,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 论文介绍了AIDev，首个大规模数据集，记录了AI编码代理在软件开发中的实际运作情况，为研究AI与人类协作提供了实证基础。


<details>
  <summary>Details</summary>
Motivation: 随着AI队友（如自主编码代理）在软件开发中的兴起，需要实证数据来研究其实际表现和影响。

Method: 通过收集456,000个拉取请求的数据，涵盖5个主流AI代理在61,000个仓库中的活动，提供结构化开放数据。

Result: AI代理在速度上优于人类，但拉取请求的接受率较低，且代码结构更简单，揭示了信任和效用差距。

Conclusion: AIDev为研究AI原生工作流和人类-AI协作提供了重要资源，支持未来软件工程的发展。

Abstract: The future of software engineering--SE 3.0--is unfolding with the rise of AI
teammates: autonomous, goal-driven systems collaborating with human developers.
Among these, autonomous coding agents are especially transformative, now
actively initiating, reviewing, and evolving code at scale. This paper
introduces AIDev, the first large-scale dataset capturing how such agents
operate in the wild. Spanning over 456,000 pull requests by five leading
agents--OpenAI Codex, Devin, GitHub Copilot, Cursor, and Claude Code--across
61,000 repositories and 47,000 developers, AIDev provides an unprecedented
empirical foundation for studying autonomous teammates in software development.
  Unlike prior work that has largely theorized the rise of AI-native software
engineering, AIDev offers structured, open data to support research in
benchmarking, agent readiness, optimization, collaboration modeling, and AI
governance. The dataset includes rich metadata on PRs, authorship, review
timelines, code changes, and integration outcomes--enabling exploration beyond
synthetic benchmarks like SWE-bench. For instance, although agents often
outperform humans in speed, their PRs are accepted less frequently, revealing a
trust and utility gap. Furthermore, while agents accelerate code
submission--one developer submitted as many PRs in three days as they had in
three years--these are structurally simpler (via code complexity metrics).
  We envision AIDev as a living resource: extensible, analyzable, and ready for
the SE and AI communities. Grounding SE 3.0 in real-world evidence, AIDev
enables a new generation of research into AI-native workflows and supports
building the next wave of symbiotic human-AI collaboration. The dataset is
publicly available at https://github.com/SAILResearch/AI_Teammates_in_SE3.
  > AI Agent, Agentic AI, Coding Agent, Agentic Coding, Software Engineering
Agent

</details>


### [17] [Survey of GenAI for Automotive Software Development: From Requirements to Executable Code](https://arxiv.org/abs/2507.15025)
*Nenad Petrovic,Vahid Zolfaghari,Andre Schamschurko,Sven Kirchner,Fengjunjie Pan,Chengdng Wu,Nils Purschke,Aleksei Velsh,Krzysztof Lebioda,Yinglei Song,Yi Zhang,Lukasz Mazur,Alois Knoll*

Main category: cs.SE

TL;DR: 探讨GenAI在汽车软件开发中的应用，重点关注需求处理、合规性和代码生成，并提出了一个通用工作流程。


<details>
  <summary>Details</summary>
Motivation: 汽车软件开发过程复杂且成本高，GenAI有望减少人工干预和成本。

Method: 综述了LLMs、RAG和VLMs等技术，并分析了提示技术在代码生成中的应用。

Result: 提出了一个通用的GenAI辅助汽车软件开发工作流程，并总结了行业调查结果。

Conclusion: GenAI在汽车软件开发中具有潜力，但仍需进一步研究和实践验证。

Abstract: Adoption of state-of-art Generative Artificial Intelligence (GenAI) aims to
revolutionize many industrial areas by reducing the amount of human
intervention needed and effort for handling complex underlying processes.
Automotive software development is considered to be a significant area for
GenAI adoption, taking into account lengthy and expensive procedures, resulting
from the amount of requirements and strict standardization. In this paper, we
explore the adoption of GenAI for various steps of automotive software
development, mainly focusing on requirements handling, compliance aspects and
code generation. Three GenAI-related technologies are covered within the
state-of-art: Large Language Models (LLMs), Retrieval Augmented Generation
(RAG), Vision Language Models (VLMs), as well as overview of adopted prompting
techniques in case of code generation. Additionally, we also derive a
generalized GenAI-aided automotive software development workflow based on our
findings from this literature review. Finally, we include a summary of a survey
outcome, which was conducted among our automotive industry partners regarding
the type of GenAI tools used for their daily work activities.

</details>


### [18] [Can LLMs Generate User Stories and Assess Their Quality?](https://arxiv.org/abs/2507.15157)
*Giovanni Quattrocchi,Liliana Pasquale,Paola Spoletini,Luciano Baresi*

Main category: cs.SE

TL;DR: 论文探讨了如何利用LLMs在敏捷框架中自动化需求获取，并评估了LLMs生成用户故事（US）的质量及其语义质量评估能力。


<details>
  <summary>Details</summary>
Motivation: 需求获取是需求工程中最具挑战性的活动之一，传统方法在语义质量评估上耗时且手动。LLMs可能提供自动化解决方案。

Method: 使用10种先进的LLMs模拟客户访谈生成US，并与人类生成的US进行质量对比，同时探索LLMs在语义质量评估中的应用。

Result: LLMs生成的US在覆盖率和风格质量上与人类相似，但多样性和创造性较低，且较少满足验收标准。LLMs在提供明确标准时可可靠评估语义质量。

Conclusion: LLMs在需求获取和语义质量评估中具有潜力，可减少人工工作量，但需进一步优化多样性和创造性。

Abstract: Requirements elicitation is still one of the most challenging activities of
the requirements engineering process due to the difficulty requirements
analysts face in understanding and translating complex needs into concrete
requirements. In addition, specifying high-quality requirements is crucial, as
it can directly impact the quality of the software to be developed. Although
automated tools allow for assessing the syntactic quality of requirements,
evaluating semantic metrics (e.g., language clarity, internal consistency)
remains a manual and time-consuming activity. This paper explores how LLMs can
help automate requirements elicitation within agile frameworks, where
requirements are defined as user stories (US). We used 10 state-of-the-art LLMs
to investigate their ability to generate US automatically by emulating customer
interviews. We evaluated the quality of US generated by LLMs, comparing it with
the quality of US generated by humans (domain experts and students). We also
explored whether and how LLMs can be used to automatically evaluate the
semantic quality of US. Our results indicate that LLMs can generate US similar
to humans in terms of coverage and stylistic quality, but exhibit lower
diversity and creativity. Although LLM-generated US are generally comparable in
quality to those created by humans, they tend to meet the acceptance quality
criteria less frequently, regardless of the scale of the LLM model. Finally,
LLMs can reliably assess the semantic quality of US when provided with clear
evaluation criteria and have the potential to reduce human effort in
large-scale assessments.

</details>


### [19] [Deep Learning Framework Testing via Heuristic Guidance Based on Multiple Model Measurements](https://arxiv.org/abs/2507.15181)
*Yinglong Zou,Juan Zhai,Chunrong Fang,Yanzhou Mu,Jiawei Liu,Zhenyu Chen*

Main category: cs.SE

TL;DR: DLMMM是一种新的深度学习框架测试方法，通过融合多种模型测量指标（如错误检测性能、算子组合多样性和执行时间）来优化测试效果。


<details>
  <summary>Details</summary>
Motivation: 现有测试方法在检测深度学习框架缺陷时存在三大局限性：无法量化算子组合多样性、忽略模型执行时间、未考虑不同测量指标间的相关性。

Method: DLMMM首先量化模型的错误检测性能、算子组合多样性和执行时间，然后基于相关性融合这些指标以实现权衡。此外，还设计了多级启发式指导以生成测试输入模型。

Result: DLMMM通过多指标融合和多级启发式指导，显著提升了深度学习框架的测试效果。

Conclusion: DLMMM解决了现有方法的局限性，为深度学习框架测试提供了更全面和高效的解决方案。

Abstract: Deep learning frameworks serve as the foundation for developing and deploying
deep learning applications. To enhance the quality of deep learning frameworks,
researchers have proposed numerous testing methods using deep learning models
as test inputs. However, existing methods predominantly measure model bug
detection effectiveness as heuristic indicators, presenting three critical
limitations: Firstly, existing methods fail to quantitatively measure model's
operator combination variety, potentially missing critical operator
combinations that could trigger framework bugs. Secondly, existing methods
neglect measuring model execution time, resulting in the omission of numerous
models potential for detecting more framework bugs within limited testing time.
Thirdly, existing methods overlook correlation between different model
measurements, relying simply on single-indicator heuristic guidance without
considering their trade-offs. To overcome these limitations, we propose DLMMM,
the first deep learning framework testing method to include multiple model
measurements into heuristic guidance and fuse these measurements to achieve
their trade-off. DLMMM firstly quantitatively measures model's bug detection
performance, operator combination variety, and model execution time. After
that, DLMMM fuses the above measurements based on their correlation to achieve
their trade-off. To further enhance testing effectiveness, DLMMM designs
multi-level heuristic guidance for test input model generation.

</details>


### [20] [Cultural Impact on Requirements Engineering Activities: Bangladeshi Practitioners' View](https://arxiv.org/abs/2507.15188)
*Chowdhury Shahriar Muzammel,Maria Spichkova,James Harland*

Main category: cs.SE

TL;DR: 研究探讨了孟加拉国文化对需求工程（RE）活动的影响，旨在避免误解并促进IT行业的多样性。


<details>
  <summary>Details</summary>
Motivation: 需求工程是软件开发中互动密集的阶段，受利益相关者国家文化影响较大。孟加拉国IT行业发展迅速但研究不足，需了解其文化对RE的影响。

Method: 研究聚焦于孟加拉国文化背景下的RE实践，分析文化因素如何影响RE活动。

Result: 研究发现孟加拉国独特的文化特征对RE活动有显著影响，需在RE实践中加以考虑。

Conclusion: 了解文化影响有助于优化RE实践，避免冲突，并促进IT行业的多样性和包容性。

Abstract: Requirements Engineering (RE) is one of the most interaction-intensive phases
of software development. This means that RE activities might be especially
impacted by stakeholders' national culture. Software development projects
increasingly have a very diverse range of stakeholders. To future-proof RE
activities, we need to help RE practitioners avoid misunderstandings and
conflicts that might arise from not understanding potential Cultural Influences
(CIs). Moreover, an awareness of CIs supports diversity and inclusion in the IT
profession. Bangladesh has a growing IT sector with some unique socio-cultural
characteristics, and has been largely overlooked in this research field. In
this study, we aim to investigate how the RE process is adopted in the context
of Bangladeshi culture and what cultural influences impact overall RE
activities.

</details>


### [21] [Towards Using Personas in Requirements Engineering: What Has Been Changed Recently?](https://arxiv.org/abs/2507.15197)
*Chowdhury Shahriar Muzammel,Maria Spichkova,James Harland*

Main category: cs.SE

TL;DR: 系统映射研究（SMS）探讨了2023年至2025年间需求工程（RE）中人物角色的最新趋势，尤其是生成式AI的应用。研究发现AI在人物角色构建和验证中的应用增加，模板化人物角色更受欢迎，验证相关研究比例上升。


<details>
  <summary>Details</summary>
Motivation: 探索需求工程中人物角色的最新研究趋势，特别是生成式AI方法的影响。

Method: 对2023年4月至2025年4月间的22篇相关文献进行系统映射研究，分析人物角色的表示、构建、验证及其在RE活动中的应用。

Result: 发现AI在人物角色构建和验证中的应用增多，模板化人物角色更流行，验证相关研究比例增加。

Conclusion: 生成式AI对人物角色的构建和验证有显著影响，模板化和验证成为研究热点。

Abstract: In requirements engineering (RE), personas are now being used to represent
user expectations and needs. This systematic mapping study (SMS) aims to
explore the most recent studies and to cover recent changes in trends,
especially related to the recent evolution of Generative AI approaches. Our SMS
covers the period between April 2023 and April 2025. We identified 22 relevant
publications and analysed persona representation, construction, validation, as
well as RE activities covered by personas. We identified that a number of
studies applied AI-based solutions for persona construction and validation. We
observed that template-based personas are becoming more popular nowadays. We
also observed an increase in the proportion of studies covering validation
aspects.

</details>


### [22] [SimdBench: Benchmarking Large Language Models for SIMD-Intrinsic Code Generation](https://arxiv.org/abs/2507.15224)
*Yibo He,Shuoran Zhao,Jiaming Huang,Yingjie Fu,Hao Yu,Cunjian Huang,Tao Xie*

Main category: cs.SE

TL;DR: SimdBench是首个专门为SIMD-intrinsic代码生成设计的基准测试，包含136个任务，评估了18个主流LLM在生成向量化代码时的表现。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成基准仅关注标量代码，LLM在SIMD-intrinsic代码生成中的表现尚不明确，因此需要填补这一空白。

Method: 提出SimdBench基准测试，针对五种代表性SIMD指令集（SSE、AVX、Neon、SVE、RVV）设计136个任务，系统评估18个LLM的正确性和性能。

Result: LLM在SIMD-intrinsic代码生成中的pass@k普遍低于标量代码生成，但分析指出了进一步改进的方向。

Conclusion: SimdBench为研究社区提供了开源工具，推动了LLM在SIMD-intrinsic代码生成领域的进步。

Abstract: SIMD (Single Instruction Multiple Data) instructions and their compiler
intrinsics are widely supported by modern processors to accelerate
performance-critical tasks. SIMD intrinsic programming, a trade-off between
coding productivity and high performance, is widely used in the development of
mainstream performance-critical libraries and daily computing tasks. Large
Language Models (LLMs), which have demonstrated strong and comprehensive
capabilities in code generation, show promise in assisting programmers with the
challenges of SIMD intrinsic programming. However, existing code-generation
benchmarks focus on only scalar code, and it is unclear how LLMs perform in
generating vectorized code using SIMD intrinsics. To fill this gap, we propose
SimdBench, the first code benchmark specifically designed for SIMD-intrinsic
code generation, comprising 136 carefully crafted tasks and targeting five
representative SIMD intrinsics: SSE (x86 Streaming SIMD Extension), AVX (x86
Advanced Vector Extension), Neon (ARM Advanced SIMD Extension), SVE (ARM
Scalable Vector Extension), and RVV (RISC-V Vector Extension). We conduct a
systematic evaluation (measuring both correctness and performance) of 18
representative LLMs on SimdBench, resulting in a series of novel and insightful
findings. Our evaluation results demonstrate that LLMs exhibit a universal
decrease in pass@k during SIMD-intrinsic code generation compared to
scalar-code generation. Our in-depth analysis highlights promising directions
for the further advancement of LLMs in the challenging domain of SIMD-intrinsic
code generation. SimdBench is fully open source at
https://anonymous.4open.science/r/SimdBench-1B3F/ to benefit the broader
research community.

</details>


### [23] [Code Clone Detection via an AlphaFold-Inspired Framework](https://arxiv.org/abs/2507.15226)
*Changguo Jia,Yi Zhan,Tianqi Zhao,Hengzhi Ye,Minghui Zhou*

Main category: cs.SE

TL;DR: AlphaCC利用AlphaFold的序列到结构建模能力，通过多语言适用的令牌序列表示代码片段，实现跨语言的代码克隆检测。


<details>
  <summary>Details</summary>
Motivation: 现有代码克隆检测方法难以捕捉代码语义或依赖语言特定分析器，AlphaCC受AlphaFold启发，利用其序列建模能力解决这一问题。

Method: AlphaCC将代码片段转换为令牌序列，构建多序列对齐增强上下文理解，采用改进的注意力编码器建模依赖关系，并通过相似度评分和二元分类检测克隆。

Result: 在多语言数据集上，AlphaCC表现优于基线方法，展示出强大的语义理解能力，同时保持高效性。

Conclusion: AlphaCC为跨语言代码克隆检测提供了一种高效且语义感知的解决方案。

Abstract: Code clone detection, which aims to identify functionally equivalent code
fragments, plays a critical role in software maintenance and vulnerability
analysis. Substantial methods have been proposed to detect code clones, but
they fall short in capturing code semantics or relying on language-specific
analyzers. Inspired by the remarkable success of AlphaFold in predicting
three-dimensional protein structures from protein sequences, in this paper, we
leverage AlphaFold for code clone detection based on the insight that protein
sequences and token sequences share a common linear sequential structure. In
particular, we propose AlphaCC, which represents code fragments as token
sequences to ensure multi-language applicability and adapts AlphaFold's
sequence-to-structure modeling capability to infer code semantics. The pipeline
of AlphaCC goes through three steps. First, AlphaCC transforms each input code
fragment into a token sequence and, motivated by AlphaFold's use of multiple
sequence alignment (MSA) to enhance contextual understanding, constructs an MSA
from lexically similar token sequences. Second, AlphaCC adopts a modified
attention-based encoder based on AlphaFold to model dependencies within and
across token sequences. Finally, unlike AlphaFold's protein structure
prediction task, AlphaCC computes similarity scores between token sequences
through a late interaction strategy and performs binary classification to
determine code clone pairs. Comprehensive evaluations on three language-diverse
datasets demonstrate AlphaCC's applicability across multiple programming
languages. On two semantic clone detection datasets, it consistently
outperforms all baselines, showing strong semantic understanding. Moreover,
AlphaCC maintains competitive efficiency, enabling practical usage in
large-scale clone detection tasks.

</details>


### [24] [FaultLine: Automated Proof-of-Vulnerability Generation Using LLM Agents](https://arxiv.org/abs/2507.15241)
*Vikram Nitin,Baishakhi Ray,Roshanak Zilouchian Moghaddam*

Main category: cs.SE

TL;DR: FaultLine是一种基于LLM代理的工作流程，用于自动生成漏洞验证测试（PoV），在跨语言环境中表现优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 软件安全漏洞报告常缺乏验证漏洞的测试（PoV），导致修复不彻底或回归问题。生成PoV测试需要复杂的程序分析，传统方法难以应对。

Method: FaultLine通过追踪输入流（从API到漏洞点）、分析分支条件，并利用反馈循环生成PoV测试，无需依赖特定语言的静态或动态分析工具。

Result: 在100个多语言漏洞数据集中，FaultLine成功生成16个PoV测试，优于CodeAct 2.1的9个，相对性能提升77%。

Conclusion: 分层推理可提升LLM代理在PoV测试生成中的表现，但问题仍具挑战性。公开代码和数据集以促进进一步研究。

Abstract: Despite the critical threat posed by software security vulnerabilities,
reports are often incomplete, lacking the proof-of-vulnerability (PoV) tests
needed to validate fixes and prevent regressions. These tests are crucial not
only for ensuring patches work, but also for helping developers understand how
vulnerabilities can be exploited. Generating PoV tests is a challenging
problem, requiring reasoning about the flow of control and data through deeply
nested levels of a program.
  We present FaultLine, an LLM agent workflow that uses a set of carefully
designed reasoning steps, inspired by aspects of traditional static and dynamic
program analysis, to automatically generate PoV test cases. Given a software
project with an accompanying vulnerability report, FaultLine 1) traces the flow
of an input from an externally accessible API ("source") to the "sink"
corresponding to the vulnerability, 2) reasons about the conditions that an
input must satisfy in order to traverse the branch conditions encountered along
the flow, and 3) uses this reasoning to generate a PoV test case in a
feedback-driven loop. FaultLine does not use language-specific static or
dynamic analysis components, which enables it to be used across programming
languages.
  To evaluate FaultLine, we collate a challenging multi-lingual dataset of 100
known vulnerabilities in Java, C and C++ projects. On this dataset, FaultLine
is able to generate PoV tests for 16 projects, compared to just 9 for CodeAct
2.1, a popular state-of-the-art open-source agentic framework. Thus, FaultLine
represents a 77% relative improvement over the state of the art. Our findings
suggest that hierarchical reasoning can enhance the performance of LLM agents
on PoV test generation, but the problem in general remains challenging. We make
our code and dataset publicly available in the hope that it will spur further
research in this area.

</details>


### [25] [Input Reduction Enhanced LLM-based Program Repair](https://arxiv.org/abs/2507.15251)
*Boyang Yang,Luyao Ren,Xin Yin,Jiadong Ren,Haoye Tian,Shunfu Jin*

Main category: cs.SE

TL;DR: ReduceFix通过自动缩减测试输入解决了LLM在长提示中丢失关键信息的问题，显著提升了程序修复性能。


<details>
  <summary>Details</summary>
Motivation: LLM在长提示中容易丢失关键信息，导致程序修复性能下降，需要一种方法自动缩减测试输入。

Method: 提出ReduceFix，通过LLM生成缩减器自动减少测试输入，保留其失败诱导行为，并用于指导补丁生成。

Result: 在LFTBench上，ReduceFix平均缩减输入89.1%，修复成功率提升53.8%。

Conclusion: 自动缩减失败输入是LLM-based APR的有效补充，显著提升其可扩展性和效果。

Abstract: Large Language Models (LLMs) have shown great potential in Automated Program
Repair (APR). Test inputs, being crucial for reasoning the root cause of
failures, are always included in the prompt for LLM-based APR. Unfortunately,
LLMs struggle to retain key information in long prompts. When the test inputs
are extensive in the prompt, this may trigger the "lost-in-the-middle" issue,
compromising repair performance. To address this, we propose ReduceFix, an
LLM-based APR approach with a built-in component that automatically reduces
test inputs while retaining their failure-inducing behavior. ReduceFix prompts
an LLM to generate a reducer that minimizes failure-inducing test inputs
without human effort, and then feeds the reduced failure-inducing inputs to
guide patch generation.
  For targeted evaluation, we constructed LFTBench, the first long-input APR
benchmark with 200 real bugs from 20 programming tasks, each paired with a
failure-inducing input whose median size is 1 MB. On this benchmark, ReduceFix
shrinks inputs by 89.1% on average and improves overall pass@10 by up to 53.8%
relative to a prompt that includes the original test, and by 17.6% compared
with omitting the test entirely. Adding the same reduction step to ChatRepair
increases its fix rate by 21.3% without other changes. Ablation studies further
highlight the impact of input length and compressed failure information on
repair success. These results underscore that automatically reducing failing
inputs is a practical and powerful complement to LLM-based APR, significantly
improving its scalability and effectiveness.

</details>


### [26] [Butterfly Effects in Toolchains: A Comprehensive Analysis of Failed Parameter Filling in LLM Tool-Agent Systems](https://arxiv.org/abs/2507.15296)
*Qian Xiong,Yuekai Huang,Ziyou Jiang,Zhiyuan Chang,Yujia Zheng,Tianhao Li,Mingyang Li*

Main category: cs.SE

TL;DR: 论文探讨了工具代理范式中参数失败的问题，提出了分类和改进建议。


<details>
  <summary>Details</summary>
Motivation: 工具代理范式扩展了大型语言模型（LLM）的能力，但参数失败问题限制了其有效性。

Method: 构建参数失败分类，通过输入扰动方法分析失败原因。

Result: 参数名称幻觉失败源于LLM固有局限，其他失败模式与输入源问题相关。

Conclusion: 建议标准化工具返回格式、改进错误反馈机制和确保参数一致性以提高可靠性。

Abstract: The emergence of the tool agent paradigm has broadened the capability
boundaries of the Large Language Model (LLM), enabling it to complete more
complex tasks. However, the effectiveness of this paradigm is limited due to
the issue of parameter failure during its execution. To explore this phenomenon
and propose corresponding suggestions, we first construct a parameter failure
taxonomy in this paper. We derive five failure categories from the invocation
chain of a mainstream tool agent. Then, we explore the correlation between
three different input sources and failure categories by applying 15 input
perturbation methods to the input. Experimental results show that parameter
name hallucination failure primarily stems from inherent LLM limitations, while
issues with input sources mainly cause other failure patterns. To improve the
reliability and effectiveness of tool-agent interactions, we propose
corresponding improvement suggestions, including standardizing tool return
formats, improving error feedback mechanisms, and ensuring parameter
consistency.

</details>


### [27] [StackTrans: From Large Language Model to Large Pushdown Automata Model](https://arxiv.org/abs/2507.15343)
*Kechi Zhang,Ge Li,Jia Li,Huangzhao Zhang,Yihong Dong,Jia Li,Jingjing Xu,Zhi Jin*

Main category: cs.SE

TL;DR: StackTrans通过引入隐藏状态栈来增强Transformer架构，解决了其无法有效捕获Chomsky层次结构（如正则表达式或确定性上下文无关文法）的问题，并在多个任务中表现优于标准Transformer模型。


<details>
  <summary>Details</summary>
Motivation: Transformer架构虽强大，但无法有效处理Chomsky层次结构，如正则表达式或确定性上下文无关文法。受下推自动机的启发，提出StackTrans以解决这一问题。

Method: StackTrans在Transformer层间引入可微分的隐藏状态栈操作（如压栈和弹栈），保持与现有框架（如flash-attention）的兼容性，并通过端到端学习优化。

Result: StackTrans在Chomsky层次结构和大规模自然语言任务中均优于标准Transformer及其他基线模型，其360M参数的预训练模型甚至超越了一些参数多2-3倍的开源大模型。

Conclusion: StackTrans通过引入栈操作显著提升了Transformer在复杂语言结构任务中的表现，展示了其高效性和推理能力。

Abstract: The Transformer architecture has emerged as a landmark advancement within the
broad field of artificial intelligence, effectively catalyzing the advent of
large language models (LLMs). However, despite its remarkable capabilities and
the substantial progress it has facilitated, the Transformer architecture still
has some limitations. One such intrinsic limitation is its inability to
effectively capture the Chomsky hierarchy, such as regular expressions or
deterministic context-free grammars. Drawing inspiration from pushdown
automata, which efficiently resolve deterministic context-free grammars using
stacks, we propose StackTrans to address the aforementioned issue within LLMs.
Unlike previous approaches that modify the attention computation, StackTrans
explicitly incorporates hidden state stacks between Transformer layers. This
design maintains compatibility with existing frameworks like flash-attention.
Specifically, our design features stack operations -- such as pushing and
popping hidden states -- that are differentiable and can be learned in an
end-to-end manner. Our comprehensive evaluation spans benchmarks for both
Chomsky hierarchies and large-scale natural languages. Across these diverse
tasks, StackTrans consistently outperforms standard Transformer models and
other baselines. We have successfully scaled StackTrans up from 360M to 7B
parameters. In particular, our from-scratch pretrained model StackTrans-360M
outperforms several larger open-source LLMs with 2-3x more parameters,
showcasing its superior efficiency and reasoning capability.

</details>


### [28] [Applying the Chinese Wall Reverse Engineering Technique to Large Language Model Code Editing](https://arxiv.org/abs/2507.15599)
*Manatsawin Hanmongkolchai*

Main category: cs.SE

TL;DR: 论文提出了一种“中国墙”技术，通过强模型生成指令指导弱模型，提升其性能，但实际应用受限于缺乏无版权问题的公开训练数据。


<details>
  <summary>Details</summary>
Motivation: 当前代码大语言模型的训练数据集未公开，存在版权问题，且部分强调数据许可的模型因数据有限性能不足。

Method: 应用“中国墙”技术，用高质量模型生成指令指导弱模型，提升其复杂任务处理能力。

Result: 实验显示，该技术使Comma v0.1 1T在CanItEdit基准上性能提升66%，Starcoder2 Instruct提升约20%。

Conclusion: 该技术能提升弱模型性能，但因缺乏无版权问题的公开训练数据，实际应用受限。

Abstract: Large language models for code (Code LLM) are increasingly utilized in
programming environments. Despite their utility, the training datasets for top
LLM remain undisclosed, raising concerns about potential copyright violations.
Some models, such as Pleias and Comma put emphasis on data curation and
licenses, however, with limited training data these models are not competitive
and only serve as proof of concepts. To improve the utility of these models, we
propose an application of the "Chinese Wall" technique, inspired by the reverse
engineering technique of the same name -- a high quality model is used to
generate detailed instructions for a weaker model. By doing so, a weaker but
ethically aligned model may be used to perform complicated tasks that,
otherwise, can only be completed by more powerful models. In our evaluation,
we've found that this technique improves Comma v0.1 1T's performance in
CanItEdit benchmark by over 66%, and Starcoder2 Instruct by roughly 20%
compared to when running the same model on the benchmark alone. The practical
application of this technique today, however, may be limited due to the lack of
models trained on public domain content without copyright restrictions.

</details>


### [29] [Hot Topics and Common Challenges: an Empirical Study of React Discussions on Stack Overflow](https://arxiv.org/abs/2507.15624)
*Yusuf Sulistyo Nugroho,Ganno Tribuana Kurniaji,Syful Islam,Mohammed Humayun Kabir,Vanesya Aura Ardity,Md. Kamal Uddin*

Main category: cs.SE

TL;DR: 分析Stack Overflow上React相关问题的研究，发现高频关键词、错误分类及用户声誉与错误的关系，提出社区需提供算法问题指导。


<details>
  <summary>Details</summary>
Motivation: 尽管React在Web开发中受欢迎，但用户面临的具体挑战尚不明确，因此研究旨在通过分析Stack Overflow问题填补这一空白。

Method: 采用探索性数据分析方法，研究React相关问题的高频关键词、错误分类及用户声誉与错误的关系。

Result: 高频关键词包括code、link等；算法错误是最常见问题，中声誉用户贡献最多（55.77%）。

Conclusion: 研究结果为React社区提供早期实施阶段的指导建议，帮助用户更有效克服挑战。

Abstract: React is a JavaScript library used to build user interfaces for single-page
applications. Although recent studies have shown the popularity and advantages
of React in web development, the specific challenges users face remain unknown.
Thus, this study aims to analyse the React-related questions shared on Stack
Overflow. The study utilizes an exploratory data analysis to investigate the
most frequently discussed keywords, error classification, and user
reputation-based errors, which is the novelty of this work. The results show
the top eight most frequently used keywords on React-related questions, namely,
code, link, vir, href, connect, azure, windows, and website. The error
classification of questions from the sample shows that algorithmic error is the
most frequent issue faced by all groups of users, where mid-reputation users
contribute the most, accounting for 55.77%. This suggests the need for the
community to provide guidance materials in solving algorithm-related problems.
We expect that the results of this study will provide valuable insight into
future research to support the React community during the early stages of
implementation, facilitating their ability to effectively overcome challenges
to adoption.

</details>


### [30] [SustainDiffusion: Optimising the Social and Environmental Sustainability of Stable Diffusion Models](https://arxiv.org/abs/2507.15663)
*Giordano d'Aloisio,Tosin Fadahunsi,Jay Choy,Rebecca Moussa,Federica Sarro*

Main category: cs.SE

TL;DR: SustainDiffusion通过搜索最佳超参数和提示结构，减少Stable Diffusion模型的性别和种族偏见及能耗，同时保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 解决Stable Diffusion模型在社会和环境可持续性方面的负面影响。

Method: 搜索优化超参数和提示结构，减少偏见和能耗。

Result: 性别偏见减少68%，种族偏见减少59%，能耗降低48%。

Conclusion: 无需调整模型架构即可提升可持续性。

Abstract: Background: Text-to-image generation models are widely used across numerous
domains. Among these models, Stable Diffusion (SD) - an open-source
text-to-image generation model - has become the most popular, producing over 12
billion images annually. However, the widespread use of these models raises
concerns regarding their social and environmental sustainability.
  Aims: To reduce the harm that SD models may have on society and the
environment, we introduce SustainDiffusion, a search-based approach designed to
enhance the social and environmental sustainability of SD models.
  Method: SustainDiffusion searches the optimal combination of hyperparameters
and prompt structures that can reduce gender and ethnic bias in generated
images while also lowering the energy consumption required for image
generation. Importantly, SustainDiffusion maintains image quality comparable to
that of the original SD model.
  Results: We conduct a comprehensive empirical evaluation of SustainDiffusion,
testing it against six different baselines using 56 different prompts. Our
results demonstrate that SustainDiffusion can reduce gender bias in SD3 by 68%,
ethnic bias by 59%, and energy consumption (calculated as the sum of CPU and
GPU energy) by 48%. Additionally, the outcomes produced by SustainDiffusion are
consistent across multiple runs and can be generalised to various prompts.
  Conclusions: With SustainDiffusion, we demonstrate how enhancing the social
and environmental sustainability of text-to-image generation models is possible
without fine-tuning or changing the model's architecture.

</details>


### [31] [Modeling CubeSat Storage Battery Discharge: Equivalent Circuit Versus Machine Learning Approaches](https://arxiv.org/abs/2507.15666)
*Igor Turkin,Lina Volobuieva,Andriy Chukhray,Oleksandr Liubimov*

Main category: cs.SE

TL;DR: 文章比较了两种建模CubeSat卫星电池放电的方法：等效电路分析和机器学习，旨在为卫星电池放电建模提供合理选择。


<details>
  <summary>Details</summary>
Motivation: 研究目的是预测自主电源系统断开后的后果，确保轨道设备的容错能力，因此具有重要性和前景。

Method: 基于CubeSat卫星的轨道数据样本，包括电压、电流和温度数据，对比了基于物理定律的等效电路分析和基于经验数据的机器学习方法。

Result: 等效电路分析透明但灵活性不足，机器学习模型更准确且能适应复杂依赖和实际条件。

Conclusion: 机器学习在建模CubeSat电池放电时表现更优，尤其在处理非标准行为和环境变化时。

Abstract: The subject of the article is the study and comparison of two approaches to
modelling the battery discharge of a CubeSat satellite: analytical using
equivalent circuit and machine learning. The article aims to make a reasoned
choice of the approach to modelling the battery discharge of a CubeSat
satellite. Modelling the battery discharge of a satellite will enable the
prediction of the consequences of disconnecting the autonomous power system and
ensure the fault tolerance of equipment in orbit. Therefore, the selected study
is relevant and promising. This study focuses on the analysis of CubeSat
satellite data, based explicitly on orbital data samples of the power system,
which include data available at the time of the article publication. The
dataset contains data on the voltage, current, and temperature of the battery
and solar panels attached to the five sides of the satellite. In this context,
two approaches are considered: analytical modelling based on physical laws and
machine learning, which uses empirical data to create a predictive model.
Results: A comparative analysis of the modeling results reveals that the
equivalent circuit approach has the advantage of transparency, as it identifies
possible parameters that facilitate understanding of the relationships.
However, the model is less flexible to environmental changes or non-standard
satellite behavior. The machine learning model demonstrated more accurate
results, as it can account for complex dependencies and adapt to actual
conditions, even when they deviate from theoretical assumptions.

</details>


### [32] [BugScope: Learn to Find Bugs Like Human](https://arxiv.org/abs/2507.15671)
*Jinyao Guo,Chengpeng Wang,Dominic Deluca,Jinjie Liu,Zhuo Zhang,Xiangyu Zhang*

Main category: cs.SE

TL;DR: BugScope是一种基于大型语言模型（LLM）的多智能体系统，通过模拟人类审计员学习新错误模式的方式，显著提升了软件错误检测的精度和召回率。


<details>
  <summary>Details</summary>
Motivation: 传统静态分析工具在覆盖范围和适应性上存在局限，而现有基于LLM的方法对复杂错误的检测能力不足，需要更灵活的系统。

Method: BugScope通过程序切片提取相关检测上下文，并构建定制化的检测提示，指导LLM进行准确推理。

Result: 在40个真实错误的数据集上，BugScope的精度为87.04%，召回率为90.00%，F1分数优于现有工业工具。在Linux内核等大规模系统中发现了141个未知错误。

Conclusion: BugScope展示了在实际应用中的显著效果，能够高效检测复杂错误并推动修复。

Abstract: Detecting software bugs remains a fundamental challenge due to the extensive
diversity of real-world defects. Traditional static analysis tools often rely
on symbolic workflows, which restrict their coverage and hinder adaptability to
customized bugs with diverse anti-patterns. While recent advances incorporate
large language models (LLMs) to enhance bug detection, these methods continue
to struggle with sophisticated bugs and typically operate within limited
analysis contexts. To address these challenges, we propose BugScope, an
LLM-driven multi-agent system that emulates how human auditors learn new bug
patterns from representative examples and apply that knowledge during code
auditing. Given a set of examples illustrating both buggy and non-buggy
behaviors, BugScope synthesizes a retrieval strategy to extract relevant
detection contexts via program slicing and then constructs a tailored detection
prompt to guide accurate reasoning by the LLM. Our evaluation on a curated
dataset of 40 real-world bugs drawn from 21 widely-used open-source projects
demonstrates that BugScope achieves 87.04% precision and 90.00% recall,
surpassing state-of-the-art industrial tools by 0.44 in F1 score. Further
testing on large-scale open-source systems, including the Linux kernel,
uncovered 141 previously unknown bugs, of which 78 have been fixed and 7
confirmed by developers, highlighting BugScope's substantial practical impact.

</details>


### [33] [Do AI models help produce verified bug fixes?](https://arxiv.org/abs/2507.15822)
*Li Huang,Ilgiz Mustafin,Marco Piccioni,Alessandro Schena,Reto Weber,Bertrand Meyer*

Main category: cs.SE

TL;DR: 论文探讨了AI（特别是大语言模型）在自动程序修复（APR）中的实际效果，通过实验对比程序员使用和不使用LLM的表现，发现了一些出乎意料的结果，并提出了方法论和使用模式。


<details>
  <summary>Details</summary>
Motivation: 研究AI（尤其是LLM）在自动程序修复中的实际效果，以及程序员如何利用LLM提升修复能力。

Method: 采用随机分组实验，一组程序员使用LLM，另一组不使用，通过程序验证工具评估修复的正确性。

Result: 实验结果与预期不同，揭示了LLM在程序修复中的实际作用，并总结了7种LLM使用模式。

Conclusion: 论文为AI在程序修复中的角色提供了初步界定，并提出了方法论和使用建议。

Abstract: Among areas of software engineering where AI techniques -- particularly,
Large Language Models -- seem poised to yield dramatic improvements, an
attractive candidate is Automatic Program Repair (APR), the production of
satisfactory corrections to software bugs. Does this expectation materialize in
practice? How do we find out, making sure that proposed corrections actually
work? If programmers have access to LLMs, how do they actually use them to
complement their own skills?
  To answer these questions, we took advantage of the availability of a
program-proving environment, which formally determines the correctness of
proposed fixes, to conduct a study of program debugging with two randomly
assigned groups of programmers, one with access to LLMs and the other without,
both validating their answers through the proof tools. The methodology relied
on a division into general research questions (Goals in the Goal-Query-Metric
approach), specific elements admitting specific answers (Queries), and
measurements supporting these answers (Metrics). While applied so far to a
limited sample size, the results are a first step towards delineating a proper
role for AI and LLMs in providing guaranteed-correct fixes to program bugs.
  These results caused surprise as compared to what one might expect from the
use of AI for debugging and APR. The contributions also include: a detailed
methodology for experiments in the use of LLMs for debugging, which other
projects can reuse; a fine-grain analysis of programmer behavior, made possible
by the use of full-session recording; a definition of patterns of use of LLMs,
with 7 distinct categories; and validated advice for getting the best of LLMs
for debugging and Automatic Program Repair.

</details>


### [34] [Investigating the Use of LLMs for Evidence Briefings Generation in Software Engineering](https://arxiv.org/abs/2507.15828)
*Mauro Marcelino,Marcos Alves,Bianca Trinkenreich,Bruno Cartaxo,Sérgio Soares,Simone D. J. Barbosa,Marcos Kalinowski*

Main category: cs.SE

TL;DR: 本文提出了一种基于RAG的LLM工具，用于自动生成证据简报，并通过实验比较其与人工简报在内容保真度、易理解性和实用性上的表现。


<details>
  <summary>Details</summary>
Motivation: 证据简报对软件工程行业有价值，但人工制作成本高，阻碍其广泛应用。本文旨在探索LLM生成简报的可行性。

Method: 开发RAG-based LLM工具生成简报，设计对照实验比较LLM与人工简报的效果。

Result: 实验结果待报告。

Conclusion: 结论将基于实验结果而定。

Abstract: [Context] An evidence briefing is a concise and objective transfer medium
that can present the main findings of a study to software engineers in the
industry. Although practitioners and researchers have deemed Evidence Briefings
useful, their production requires manual labor, which may be a significant
challenge to their broad adoption. [Goal] The goal of this registered report is
to describe an experimental protocol for evaluating LLM-generated evidence
briefings for secondary studies in terms of content fidelity, ease of
understanding, and usefulness, as perceived by researchers and practitioners,
compared to human-made briefings. [Method] We developed an RAG-based LLM tool
to generate evidence briefings. We used the tool to automatically generate two
evidence briefings that had been manually generated in previous research
efforts. We designed a controlled experiment to evaluate how the LLM-generated
briefings compare to the human-made ones regarding perceived content fidelity,
ease of understanding, and usefulness. [Results] To be reported after the
experimental trials. [Conclusion] Depending on the experiment results.

</details>


### [35] [Observing Fine-Grained Changes in Jupyter Notebooks During Development Time](https://arxiv.org/abs/2507.15831)
*Sergey Titov,Konstantin Grotov,Cristina Sarasua,Yaroslav Golubev,Dhivyabharathi Ramasamy,Alberto Bacchelli,Abraham Bernstein,Timofey Bryksin*

Main category: cs.SE

TL;DR: 该论文填补了数据科学中计算笔记本动态开发过程的研究空白，通过工具集收集和分析Jupyter笔记本的代码变更，揭示了笔记本作为开发和调试工具的双重用途。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注细粒度日志分析，但缺乏对数据科学中计算笔记本开发过程的研究。

Method: 引入工具集收集Jupyter笔记本开发时的代码变更，收集20名开发者的100多小时工作数据，分析2,655个单元格和9,207次执行的动态变化。

Result: 发现笔记本中大量变更为小修复和代码迭代，表明笔记本兼具开发和调试功能。

Conclusion: 研究揭示了笔记本开发的动态特性，并提出了未来研究方向。

Abstract: In software engineering, numerous studies have focused on the analysis of
fine-grained logs, leading to significant innovations in areas such as
refactoring, security, and code completion. However, no similar studies have
been conducted for computational notebooks in the context of data science.
  To help bridge this research gap, we make three scientific contributions: we
(1) introduce a toolset for collecting code changes in Jupyter notebooks during
development time; (2) use it to collect more than 100 hours of work related to
a data analysis task and a machine learning task (carried out by 20 developers
with different levels of expertise), resulting in a dataset containing 2,655
cells and 9,207 cell executions; and (3) use this dataset to investigate the
dynamic nature of the notebook development process and the changes that take
place in the notebooks.
  In our analysis of the collected data, we classified the changes made to the
cells between executions and found that a significant number of these changes
were relatively small fixes and code iteration modifications. This suggests
that notebooks are used not only as a development and exploration tool but also
as a debugging tool. We report a number of other insights and propose potential
future research directions on the novel data.

</details>
