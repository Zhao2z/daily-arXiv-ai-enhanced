<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 32]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [CORE: Benchmarking LLMs Code Reasoning Capabilities through Static Analysis Tasks](https://arxiv.org/abs/2507.05269)
*Danning Xie,Mingwei Zheng,Xuwei Liu,Jiannan Wang,Chengpeng Wang,Lin Tan,Xiangyu Zhang*

Main category: cs.SE

TL;DR: CoRe是一个高质量、人工验证的基准测试，用于评估大型语言模型在静态分析任务中的表现，发现模型在深层语义理解和多步推理方面仍有不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注端到端结果（如代码修复或生成），而忽略了模型对程序语义推理的能力，因此需要专门的评估工具。

Method: 提出CoRe基准，包含12,553个任务实例，涵盖C/C++、Java和Python中的数据依赖、控制依赖和信息流任务，采用语义感知的多样化采样策略。

Result: 评估10种主流LLM，发现它们在依赖识别上表现良好，但在深层语义理解和多步推理任务中仍有困难。

Conclusion: CoRe揭示了LLM在复杂控制结构和反向依赖模式等挑战，为提升其代码推理能力提供了方向。

Abstract: Large language models (LLMs) have been widely adopted across diverse software
engineering domains, such as code generation, program repair, and vulnerability
detection. These applications require understanding beyond surface-level code
patterns: value propagation, control flow, and interdependence between program
elements. However, existing benchmarks primarily evaluate end-to-end outcomes,
such as whether code is correctly repaired or generated, leaving the models
ability for program semantic reasoning underexplored. This work presents CoRe,
a high-quality, human-verified benchmark designed to evaluate LLMs on
fundamental static analysis tasks. CoRe includes 12,553 task instances spanning
data dependency, control dependency, and information flow across programs
written in C/C++, Java, and Python. To ensure semantic diversity and reasoning
complexity, we propose a semantics-aware diverse sampling strategy that selects
targets and task instances based on structural coverage and dependency depth.
We evaluate 10 mainstream LLMs and show that, while they perform well at
identifying dependencies, models still struggle with tasks that require deeper
semantic understanding and multi-step reasoning. We further conduct qualitative
analyses to uncover key challenges, such as complex control structures and
backward dependency patterns, offering insights into improving LLMs code
reasoning capabilities.

</details>


### [2] [Open Source, Hidden Costs: A Systematic Literature Review on OSS License Management](https://arxiv.org/abs/2507.05270)
*Boyuan Li,Chengwei Liu,Lingling Fan,Sen Chen,Zhenlin Zhang,Zheli Liu*

Main category: cs.SE

TL;DR: 论文通过系统文献综述（SLR）分析了80篇开源软件（OSS）许可证相关研究，分类为许可证识别、风险评估和缓解三类，揭示了现有解决方案的挑战，并提出了未来研究方向和实践建议。


<details>
  <summary>Details</summary>
Motivation: 现代软件开发中集成第三方组件虽高效创新，但许可证风险可能导致法律和运营问题。现有研究和工具仍有局限，且开源许可证和生成式软件工程技术的快速发展对风险管理提出更高要求。

Method: 通过系统文献综述（SLR）分析80篇开源许可证相关论文，分类为许可证识别、风险评估和缓解三类。

Result: 揭示了现有解决方案的不足，提出了未来研究方向（如结合生成式技术）和实践建议，以促进学术界与工业界的合作。

Conclusion: 论文为开源许可证风险管理提供了系统性总结，旨在推动软件工程社区对合法风险的生态治理。

Abstract: Integrating third-party software components is a common practice in modern
software development, offering significant advantages in terms of efficiency
and innovation. However, this practice is fraught with risks related to
software licensing. A lack of understanding may lead to disputes, which can
pose serious legal and operational challenges. To these ends, both academia and
industry have conducted various investigations and proposed solutions and tools
to deal with these challenges. However, significant limitations still remain.
Moreover, the rapid evolution of open-source software (OSS) licenses, as well
as the rapidly incorporated generative software engineering techniques, such as
large language models for code (CodeLLMs), are placing greater demands on the
systematic management of software license risks. To unveil the severe
challenges and explore possible future directions, we conduct the first
systematic literature review (SLR) on 80 carefully selected OSS license-related
papers, classifying existing research into three key categories, i.e., license
identification, license risk assessment, and license risk mitigation. Based on
these, we discuss challenges in existing solutions, conclude the opportunities
to shed light on future research directions and offer practical recommendations
for practitioners. We hope this thorough review will help bridge the gaps
between academia and industry and accelerate the ecosystem-wide governance of
legitimate software risks within the software engineering community.

</details>


### [3] [FuzzFeed: An Automatic Approach to Weakest Precondition Generation using LLMs and Fuzzing](https://arxiv.org/abs/2507.05272)
*Daragh King,Vasileios Koutavas,Laura Kovacs*

Main category: cs.SE

TL;DR: 本文提出了一种结合大型语言模型（LLMs）和模糊测试的方法，用于生成程序的最弱前置条件（WP），并通过模糊指导（FG）优化LLMs的输出。


<details>
  <summary>Details</summary>
Motivation: 生成程序的最弱前置条件（WP）在验证和运行时错误检查等领域具有重要应用价值，但传统方法效率有限。

Method: 结合LLMs和模糊测试，引入模糊指导（FG）通过程序执行反馈优化LLMs生成的WP候选。

Result: 实验表明，LLMs能生成可行的WP候选，且FG能显著提升其生成质量。

Conclusion: LLMs结合FG是一种有效生成WP的方法，具有实际应用潜力。

Abstract: The weakest precondition (WP) of a program describes the largest set of
initial states from which all terminating executions of the program satisfy a
given postcondition. The generation of WPs is an important task with practical
applications in areas ranging from verification to run-time error checking.
  This paper proposes the combination of Large Language Models (LLMs) and fuzz
testing for generating WPs. In pursuit of this goal, we introduce Fuzzing
Guidance (FG); FG acts as a means of directing LLMs towards correct WPs using
program execution feedback. FG utilises fuzz testing for approximately checking
the validity and weakness of candidate WPs, this information is then fed back
to the LLM as a means of context refinement.
  We demonstrate the effectiveness of our approach on a comprehensive benchmark
set of deterministic array programs in Java. Our experiments indicate that LLMs
are capable of producing viable candidate WPs, and that this ability can be
practically enhanced through FG.

</details>


### [4] [ReservoirChat: Interactive Documentation Enhanced with LLM and Knowledge Graph for ReservoirPy](https://arxiv.org/abs/2507.05279)
*Virgile Boraud,Yannis Bendi-Ouis,Paul Bernard,Xavier Hinaut*

Main category: cs.SE

TL;DR: 介绍了一个工具，通过结合外部知识（如RAG和知识图谱）提升LLM在代码开发和Reservoir Computing领域的能力，减少幻觉并提高准确性。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在代码开发和Reservoir Computing领域中的幻觉问题，提高生成内容的准确性。

Method: 采用Retrieval-Augmented Generation (RAG)和知识图谱技术，结合ReservoirPy库，提供交互式体验。

Result: 在编码任务上表现优于ChatGPT-4o和NotebookLM，且显著优于基础模型Codestral-22B。

Conclusion: 该工具在特定领域（如ReservoirPy）中表现出色，尤其在代码开发任务上具有优势。

Abstract: We introduce a tool designed to improve the capabilities of Large Language
Models (LLMs) in assisting with code development using the ReservoirPy library,
as well as in answering complex questions in the field of Reservoir Computing.
By incorporating external knowledge through Retrieval-Augmented Generation
(RAG) and knowledge graphs, our approach aims to reduce hallucinations and
increase the factual accuracy of generated responses. The system provides an
interactive experience similar to ChatGPT, tailored specifically for
ReservoirPy, enabling users to write, debug, and understand Python code while
accessing reliable domain-specific insights. In our evaluation, while
proprietary models such as ChatGPT-4o and NotebookLM performed slightly better
on general knowledge questions, our model outperformed them on coding tasks and
showed a significant improvement over its base model, Codestral-22B.

</details>


### [5] [CoreCodeBench: A Configurable Multi-Scenario Repository-Level Benchmark](https://arxiv.org/abs/2507.05281)
*Lingyue Fu,Hao Guan,Bolun Zhang,Haowei Yuan,Yaoming Zhu,Jun Xu,Zongyu Wang,Lin Qiu,Xunliang Cai,Xuezhi Cao,Weiwen Liu,Weinan Zhang,Yong Yu*

Main category: cs.SE

TL;DR: CorePipe和CoreCodeBench旨在解决现有代码评估基准的局限性，通过自动化生成多样化的测试用例，为LLMs在工程场景中的表现提供全面评估。


<details>
  <summary>Details</summary>
Motivation: 现有代码评估基准在多样性和可控性上存在不足，无法全面反映LLMs在真实工程场景中的表现。

Method: 提出CorePipe自动化管道生成测试用例，并构建CoreCodeBench多场景可配置基准，包含原子问题和复合问题。

Result: 实验评估了16种LLMs，揭示了它们在工程场景中的不同能力和多维表现。

Conclusion: CoreCodeBench为LLMs在真实工程项目中的适用性提供了全面评估工具。

Abstract: As Large Language Models (LLMs) demonstrate increasingly sophisticated code
processing capabilities, evaluating their performance on engineering-level code
remains challenging. Existing repository-level benchmarks primarily focus on
single scenarios, such as code generation or bug fixing, without adequately
capturing the diversity and complexity of real-world software or project
engineering workflows. Furthermore, these benchmarks suffer from limited
controllability in question positioning and reliability issues in their
generated test cases. To address these limitations, we present CorePipe, a
fully automated pipeline that converts repositories into comprehensive test
cases, and introduce CoreCodeBench, a configurable multi-scenario
repository-level benchmark. To simulate real engineering scenarios, CorePipe
generates three types of atomic questions (Development, BugFix, and Test-Driven
Development) specifically targeting core code segments. These atomic questions
are further combined into three types of composite questions, with difficulty
levels flexibly adjusted through hyperparameter tuning. CoreCodeBench provides
a comprehensive and extensive repository-level benchmark to investigate the
applicability of LLMs in real-world engineering projects. Experiments with 16
LLMs across diverse scenarios reveal varying capabilities and offer
multi-dimensional insights into LLM performance in engineering contexts. The
code for CorePipe is available at
https://github.com/AGI-Eval-Official/CoreCodeBench, and the data for
CoreCodeBench can be accessed at
https://huggingface.co/collections/tubehhh/corecodebench-68256d2faabf4b1610a08caa.

</details>


### [6] [Measuring how changes in code readability attributes affect code quality evaluation by Large Language Models](https://arxiv.org/abs/2507.05289)
*Igor Regis da Silva Simoes,Elaine Venson*

Main category: cs.SE

TL;DR: 论文探讨了使用大型语言模型（LLM）评估代码可读性，通过实验验证了LLM对代码干预的敏感性及其与参考模型的对比结果。


<details>
  <summary>Details</summary>
Motivation: 代码可读性是代码质量的重要方面，但现有评估方法存在主观性和不一致性，需要一种标准化、可重复的评估方式。

Method: 通过准实验研究，测试9种LLM对三种代码干预（删除注释、替换标识符名称、重构代码）的反应，并与参考模型对比。

Result: LLM对干预敏感，与参考模型在原始和重构代码场景中一致性高，且表现出语义敏感性。响应存在一定变异性，但结果仍具统计显著性。

Conclusion: LLM在评估代码语义质量方面具有潜力，尤其在标识符名称、注释与代码目的的连贯性方面表现突出。

Abstract: Code readability is one of the main aspects of code quality, influenced by
various properties like identifier names, comments, code structure, and
adherence to standards. However, measuring this attribute poses challenges in
both industry and academia. While static analysis tools assess attributes such
as code smells and comment percentage, code reviews introduce an element of
subjectivity. This paper explores using Large Language Models (LLMs) to
evaluate code quality attributes related to its readability in a standardized,
reproducible, and consistent manner. We conducted a quasi-experiment study to
measure the effects of code changes on Large Language Model (LLM)s
interpretation regarding its readability quality attribute. Nine LLMs were
tested, undergoing three interventions: removing comments, replacing identifier
names with obscure names, and refactoring to remove code smells. Each
intervention involved 10 batch analyses per LLM, collecting data on response
variability. We compared the results with a known reference model and tool. The
results showed that all LLMs were sensitive to the interventions, with
agreement with the reference classifier being high for the original and
refactored code scenarios. The LLMs demonstrated a strong semantic sensitivity
that the reference model did not fully capture. A thematic analysis of the LLMs
reasoning confirmed their evaluations directly reflected the nature of each
intervention. The models also exhibited response variability, with 9.37% to
14.58% of executions showing a standard deviation greater than zero, indicating
response oscillation, though this did not always compromise the statistical
significance of the results. LLMs demonstrated potential for evaluating
semantic quality aspects, such as coherence between identifier names, comments,
and documentation with code purpose.

</details>


### [7] [zkSDK: Streamlining zero-knowledge proof development through automated trace-driven ZK-backend selection](https://arxiv.org/abs/2507.05294)
*William Law*

Main category: cs.SE

TL;DR: zkSDK是一个模块化框架，通过抽象后端复杂性简化ZK应用开发，核心是Presto语言，动态选择最优ZK后端。


<details>
  <summary>Details</summary>
Motivation: ZK开发者面临众多后端选择，学习曲线陡峭且体验碎片化，zkSDK旨在解决这一问题。

Method: 开发zkSDK框架，集成Presto语言和动态选择算法，自动评估并选择最优ZK后端。

Result: 通过实际工作负载分析，zkSDK能有效选择最适合的后端，提升开发体验。

Conclusion: zkSDK为ZK开发提供了无缝且用户友好的解决方案，优化了后端选择流程。

Abstract: The rapid advancement of creating Zero-Knowledge (ZK) programs has led to the
development of numerous tools designed to support developers. Popular options
include being able to write in general-purpose programming languages like Rust
from Risc Zero. Other languages exist like Circom, Lib-snark, and Cairo.
However, developers entering the ZK space are faced with many different ZK
backends to choose from, leading to a steep learning curve and a fragmented
developer experience across different platforms. As a result, many developers
tend to select a single ZK backend and remain tied to it. This thesis
introduces zkSDK, a modular framework that streamlines ZK application
development by abstracting the backend complexities. At the core of zkSDK is
Presto, a custom Python-like programming language that enables the profiling
and analysis of a program to assess its computational workload intensity.
Combined with user-defined criteria, zkSDK employs a dynamic selection
algorithm to automatically choose the optimal ZK-proving backend. Through an
in-depth analysis and evaluation of real-world workloads, we demonstrate that
zkSDK effectively selects the best-suited backend from a set of supported ZK
backends, delivering a seamless and user-friendly development experience.

</details>


### [8] [ASSURE: Metamorphic Testing for AI-powered Browser Extensions](https://arxiv.org/abs/2507.05307)
*Xuanqi Gao,Juan Zhai,Shiqing Ma,Siyi Xie,Chao Shen*

Main category: cs.SE

TL;DR: ASSURE是一个专为AI浏览器扩展设计的自动化测试框架，解决了传统方法无法应对的非确定性行为和复杂集成问题。


<details>
  <summary>Details</summary>
Motivation: AI浏览器扩展的测试和可靠性保障面临前所未有的挑战，传统方法和现有LLM测试方法均无法有效解决。

Method: ASSURE包含三个组件：模块化测试用例生成引擎、自动化执行框架和可配置验证管道。

Result: 在六个广泛使用的AI浏览器扩展中，ASSURE识别了531个问题，测试吞吐量比手动方法提高了6.4倍。

Conclusion: ASSURE为AI浏览器扩展的测试提供了高效、全面的解决方案，适合集成到开发流程中。

Abstract: The integration of Large Language Models (LLMs) into browser extensions has
revolutionized web browsing, enabling sophisticated functionalities like
content summarization, intelligent translation, and context-aware writing
assistance. However, these AI-powered extensions introduce unprecedented
challenges in testing and reliability assurance. Traditional browser extension
testing approaches fail to address the non-deterministic behavior,
context-sensitivity, and complex web environment integration inherent to
LLM-powered extensions. Similarly, existing LLM testing methodologies operate
in isolation from browser-specific contexts, creating a critical gap in
effective evaluation frameworks. To bridge this gap, we present ASSURE, a
modular automated testing framework specifically designed for AI-powered
browser extensions. ASSURE comprises three principal components: (1) a modular
test case generation engine that supports plugin-based extension of testing
scenarios, (2) an automated execution framework that orchestrates the complex
interactions between web content, extension processing, and AI model behavior,
and (3) a configurable validation pipeline that systematically evaluates
behavioral consistency and security invariants rather than relying on exact
output matching. Our evaluation across six widely-used AI browser extensions
demonstrates ASSURE's effectiveness, identifying 531 distinct issues spanning
security vulnerabilities, metamorphic relation violations, and content
alignment problems. ASSURE achieves 6.4x improved testing throughput compared
to manual approaches, detecting critical security vulnerabilities within 12.4
minutes on average. This efficiency makes ASSURE practical for integration into
development pipelines, offering a comprehensive solution to the unique
challenges of testing AI-powered browser extensions.

</details>


### [9] [OASBuilder: Generating OpenAPI Specifications from Online API Documentation with Large Language Models](https://arxiv.org/abs/2507.05316)
*Koren Lazar,Matan Vetzler,Kiran Kate,Jason Tsay,David Boaz Himanshu Gupta,Avraham Shinnar,Rohith D Vallam,David Amid Esther Goldbraich,Guy Uziel,Jim Laredo,Ateret Anaby Tavor*

Main category: cs.SE

TL;DR: OASBuilder是一个框架，将非结构化的API文档转换为机器可读的OpenAPI规范，结合了大型语言模型和基于规则的算法。


<details>
  <summary>Details</summary>
Motivation: 在线API文档多为非结构化HTML，手动转换为结构化格式耗时费力。

Method: 通过结合大型语言模型和基于规则的算法，利用文档网页结构知识，构建转换管道。

Result: 实验表明，OASBuilder能处理数百种API，生成有效的OpenAPI规范，涵盖大部分原始文档信息。

Conclusion: OASBuilder在企业环境中成功应用，节省大量手动时间，使复杂API更易于LLMs使用。

Abstract: AI agents and business automation tools interacting with external web
services require standardized, machine-readable information about their APIs in
the form of API specifications. However, the information about APIs available
online is often presented as unstructured, free-form HTML documentation,
requiring external users to spend significant time manually converting it into
a structured format. To address this, we introduce OASBuilder, a novel
framework that transforms long and diverse API documentation pages into
consistent, machine-readable API specifications. This is achieved through a
carefully crafted pipeline that integrates large language models and rule-based
algorithms which are guided by domain knowledge of the structure of
documentation webpages. Our experiments demonstrate that OASBuilder generalizes
well across hundreds of APIs, and produces valid OpenAPI specifications that
encapsulate most of the information from the original documentation. OASBuilder
has been successfully implemented in an enterprise environment, saving
thousands of hours of manual effort and making hundreds of complex enterprise
APIs accessible as tools for LLMs.

</details>


### [10] [Exploring Empathy in Software Engineering: Insights from a Grey Literature Analysis of Practitioners' Perspectives](https://arxiv.org/abs/2507.05325)
*Lidiany Cerqueira,João Pedro Bastos,Danilo Neves,Glauco Carneiro,Rodrigo Spínola,Sávio Freire,José Amancio Macedo Santos,Manoel Mendonça*

Main category: cs.SE

TL;DR: 研究探讨了软件开发中的共情，定义了其含义，识别了障碍，提出了实践方法，并构建了概念框架。


<details>
  <summary>Details</summary>
Motivation: 共情是软件开发中未被充分研究的关键社交技能，对沟通和协作至关重要。

Method: 通过分析55篇DEV和Medium文章，并辅以专家调查，进行定性内容分析。

Result: 提出了共情的定义，识别了障碍（如毒性文化和技术过度关注），提出了促进共情的实践方法，并展示了积极效果（如改善协作和减少压力）。

Conclusion: 构建的概念框架被认为清晰且有价值，未来将探索共情在软件开发中的更广泛影响。

Abstract: Context. Empathy, a key social skill, is essential for communication and
collaboration in SE but remains an under-researched topic. Aims. This study
investigates empathy in SE from practitioners' perspectives, aiming to
characterize its meaning, identify barriers, discuss practices to overcome
them, and explore its effects. Method. A qualitative content analysis was
conducted on 55 web articles from DEV and Medium, two communities widely used
by practitioners. To strengthen our findings, we conducted a follow-up survey
with empathy experts. Results. The study proposes a definition of empathy in
SE, identifies barriers such as toxic culture and excessive technical focus,
practices to foster empathy in teams, and outcomes, including improved
collaboration, communication, and reduced anxiety, frustration, and stress.
These findings are synthesized into a conceptual framework. Conclusion. Survey
results indicate the framework is clear, valuable, and raises empathy
awareness, with suggestions for improvements and integration into training.
This study paves the way for improving team dynamics by addressing barriers and
offering strategies to cultivate empathy. Future work will explore empathy's
broader implications in SE practice.

</details>


### [11] [Tool for Supporting Debugging and Understanding of Normative Requirements Using LLMs](https://arxiv.org/abs/2507.05504)
*Alex Kleijwegt,Sinem Getir Yaman,Radu Calinescu*

Main category: cs.SE

TL;DR: SLEEC-LLM利用大型语言模型（LLMs）为SLEEC规则不一致性提供自然语言解释，提升规范需求获取和一致性分析的效率与可解释性。


<details>
  <summary>Details</summary>
Motivation: 规范需求（SLEEC）的识别和管理复杂且易错，现有方法的技术性结果对非技术用户不友好，阻碍了理解与迭代效率。

Method: 提出SLEEC-LLM工具，利用LLMs将形式验证结果转化为自然语言解释。

Result: 通过两个真实案例研究验证了SLEEC-LLM在提升效率和可解释性方面的有效性。

Conclusion: SLEEC-LLM为非技术用户提供了更直观的工具，优化了规范需求的获取与分析过程。

Abstract: Normative requirements specify social, legal, ethical, empathetic, and
cultural (SLEEC) norms that must be observed by a system. To support the
identification of SLEEC requirements, numerous standards and regulations have
been developed. These requirements are typically defined by stakeholders in the
non-technical system with diverse expertise (e.g., ethicists, lawyers, social
scientists). Hence, ensuring their consistency and managing the requirement
elicitation process are complex and error-prone tasks. Recent research has
addressed this challenge using domain-specific languages to specify normative
requirements as rules, whose consistency can then be analyzed with formal
methods. Nevertheless, these approaches often present the results from formal
verification tools in a way that is inaccessible to non-technical users. This
hinders understanding and makes the iterative process of eliciting and
validating these requirements inefficient in terms of both time and effort. To
address this problem, we introduce SLEEC-LLM, a tool that uses large language
models (LLMs) to provide natural-language interpretations for model-checking
counterexamples corresponding to SLEEC rule inconsistencies. SLEEC-LLM improves
the efficiency and explainability of normative requirements elicitation and
consistency analysis. To demonstrate its effectiveness, we summarise its use in
two real-world case studies involving non-technical stakeholders.

</details>


### [12] [Search-based Selection of Metamorphic Relations for Optimized Robustness Testing of Large Language Models](https://arxiv.org/abs/2507.05565)
*Sangwon Hyun,Shaukat Ali,M. Ali Babar*

Main category: cs.SE

TL;DR: 论文提出了一种基于搜索的方法，优化MR组以最大化失败检测并最小化LLM执行成本，同时覆盖组合扰动。MOEA/D算法表现最佳，并发现了银弹MR。


<details>
  <summary>Details</summary>
Motivation: 现有LLM测试研究多限于自动生成测试用例（MRs），且仅考虑单一扰动MRs，缺乏对MR选择的优化和组合扰动的覆盖。

Method: 提出搜索过程，实现四种搜索算法（Single-GA、NSGA-II、SPEA2、MOEA/D）优化MR选择，覆盖组合扰动。

Result: MOEA/D算法在优化MR空间方面表现最佳，并发现银弹MRs能有效混淆LLM。

Conclusion: 研究为LLM稳健性评估提供了优化测试的基础问题和基于搜索的解决方案。

Abstract: Assessing the trustworthiness of Large Language Models (LLMs), such as
robustness, has garnered significant attention. Recently, metamorphic testing
that defines Metamorphic Relations (MRs) has been widely applied to evaluate
the robustness of LLM executions. However, the MR-based robustness testing
still requires a scalable number of MRs, thereby necessitating the optimization
of selecting MRs. Most extant LLM testing studies are limited to automatically
generating test cases (i.e., MRs) to enhance failure detection. Additionally,
most studies only considered a limited test space of single perturbation MRs in
their evaluation of LLMs. In contrast, our paper proposes a search-based
approach for optimizing the MR groups to maximize failure detection and
minimize the LLM execution cost. Moreover, our approach covers the
combinatorial perturbations in MRs, facilitating the expansion of test space in
the robustness assessment. We have developed a search process and implemented
four search algorithms: Single-GA, NSGA-II, SPEA2, and MOEA/D with novel
encoding to solve the MR selection problem in the LLM robustness testing. We
conducted comparative experiments on the four search algorithms along with a
random search, using two major LLMs with primary Text-to-Text tasks. Our
statistical and empirical investigation revealed two key findings: (1) the
MOEA/D algorithm performed the best in optimizing the MR space for LLM
robustness testing, and (2) we identified silver bullet MRs for the LLM
robustness testing, which demonstrated dominant capabilities in confusing LLMs
across different Text-to-Text tasks. In LLM robustness assessment, our research
sheds light on the fundamental problem for optimized testing and provides
insights into search-based solutions.

</details>


### [13] [TigAug: Data Augmentation for Testing Traffic Light Detection in Autonomous Driving Systems](https://arxiv.org/abs/2507.05932)
*You Lu,Dingji Wang,Kaifeng Huang,Bihuan Chen,Xin Peng*

Main category: cs.SE

TL;DR: 论文提出TigAug方法，通过自动增强标记的交通灯图像来测试和改进交通灯检测模型，解决了手动数据收集的不足。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统（ADS）中交通灯检测模型的测试缺乏自动化方法，手动数据收集效率低且难以覆盖多样环境。

Method: 构建基于天气、相机和交通灯特性的蜕变关系和变换家族，自动生成增强图像用于测试和模型改进。

Result: 实验表明TigAug能有效测试模型、高效合成图像，且生成图像具有可接受的自然度。

Conclusion: TigAug为交通灯检测模型的测试和改进提供了一种自动化解决方案。

Abstract: Autonomous vehicle technology has been developed in the last decades with
recent advances in sensing and computing technology. There is an urgent need to
ensure the reliability and robustness of autonomous driving systems (ADSs).
Despite the recent achievements in testing various ADS modules, little
attention has been paid on the automated testing of traffic light detection
models in ADSs. A common practice is to manually collect and label traffic
light data. However, it is labor-intensive, and even impossible to collect
diverse data under different driving environments.
  To address these problems, we propose and implement TigAug to automatically
augment labeled traffic light images for testing traffic light detection models
in ADSs. We construct two families of metamorphic relations and three families
of transformations based on a systematic understanding of weather environments,
camera properties, and traffic light properties. We use augmented images to
detect erroneous behaviors of traffic light detection models by
transformation-specific metamorphic relations, and to improve the performance
of traffic light detection models by retraining. Large-scale experiments with
four state-of-the-art traffic light detection models and two traffic light
datasets have demonstrated that i) TigAug is effective in testing traffic light
detection models, ii) TigAug is efficient in synthesizing traffic light images,
and iii) TigAug generates traffic light images with acceptable naturalness.

</details>


### [14] [Multi-Agent Debate Strategies to Enhance Requirements Engineering with Large Language Models](https://arxiv.org/abs/2507.05981)
*Marc Oriol,Quim Motger,Jordi Marco,Xavier Franch*

Main category: cs.SE

TL;DR: 研究探讨了多智能体辩论（MAD）策略如何提升大型语言模型（LLM）在需求工程（RE）任务中的准确性，提出了一种初步的MAD框架并验证其可行性。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如提示工程、模型微调）将LLM视为孤立黑箱，缺乏迭代优化和协作，限制了鲁棒性和适应性。受人类辩论启发，研究探索MAD策略是否能提升RE任务性能。

Method: 系统研究了现有MAD策略，提出分类法，并初步实现了一个基于MAD的RE分类框架进行测试。

Result: 研究总结了MAD策略的核心特征，初步评估表明MAD在RE分类中具有可行性。

Conclusion: MAD为提升LLM在RE任务中的准确性提供了新思路，为未来研究奠定了基础。

Abstract: Context: Large Language Model (LLM) agents are becoming widely used for
various Requirements Engineering (RE) tasks. Research on improving their
accuracy mainly focuses on prompt engineering, model fine-tuning, and retrieval
augmented generation. However, these methods often treat models as isolated
black boxes - relying on single-pass outputs without iterative refinement or
collaboration, limiting robustness and adaptability. Objective: We propose
that, just as human debates enhance accuracy and reduce bias in RE tasks by
incorporating diverse perspectives, different LLM agents debating and
collaborating may achieve similar improvements. Our goal is to investigate
whether Multi-Agent Debate (MAD) strategies can enhance RE performance. Method:
We conducted a systematic study of existing MAD strategies across various
domains to identify their key characteristics. To assess their applicability in
RE, we implemented and tested a preliminary MAD-based framework for RE
classification. Results: Our study identified and categorized several MAD
strategies, leading to a taxonomy outlining their core attributes. Our
preliminary evaluation demonstrated the feasibility of applying MAD to RE
classification. Conclusions: MAD presents a promising approach for improving
LLM accuracy in RE tasks. This study provides a foundational understanding of
MAD strategies, offering insights for future research and refinements in RE
applications.

</details>


### [15] [PromiseTune: Unveiling Causally Promising and Explainable Configuration Tuning](https://arxiv.org/abs/2507.05995)
*Pengzhou Chen,Tao Chen*

Main category: cs.SE

TL;DR: PromiseTune通过因果推断净化规则，指导配置调优，显著提升性能并提供解释性。


<details>
  <summary>Details</summary>
Motivation: 现代软件系统的高可配置性使配置调优成为关键，但现有方法在探索与利用间难以平衡，且缺乏对潜在优化区域的了解。

Method: 提出PromiseTune，通过学习并因果净化规则，近似反映潜在优化区域，指导调优并缓解探索与利用的权衡问题。

Result: 在12个系统和不同预算下，PromiseTune性能显著优于其他11种方法，排名提升42%，并提供更丰富的解释信息。

Conclusion: PromiseTune通过因果规则指导调优，有效提升性能并增强结果的可解释性。

Abstract: The high configurability of modern software systems has made configuration
tuning a crucial step for assuring system performance, e.g., latency or
throughput. However, given the expensive measurements, large configuration
space, and rugged configuration landscape, existing tuners suffer
ineffectiveness due to the difficult balance of budget utilization between
exploring uncertain regions (for escaping from local optima) and exploiting
guidance of known good configurations (for fast convergence). The root cause is
that we lack knowledge of where the promising regions lay, which also causes
challenges in the explainability of the results.
  In this paper, we propose PromiseTune that tunes configuration guided by
causally purified rules. PromiseTune is unique in the sense that we learn
rules, which reflect certain regions in the configuration landscape, and purify
them with causal inference. The remaining rules serve as approximated
reflections of the promising regions, bounding the tuning to emphasize these
places in the landscape. This, as we demonstrate, can effectively mitigate the
impact of the exploration and exploitation trade-off. Those purified regions
can then be paired with the measured configurations to provide spatial
explainability at the landscape level. Comparing with 11 state-of-the-art
tuners on 12 systems and varying budgets, we show that PromiseTune performs
significantly better than the others with $42\%$ superior rank to the overall
second best while providing richer information to explain the hidden system
characteristics.

</details>


### [16] [Model Cards Revisited: Bridging the Gap Between Theory and Practice for Ethical AI Requirements](https://arxiv.org/abs/2507.06014)
*Tim Puhlfürß,Julia Butzke,Walid Maalej*

Main category: cs.SE

TL;DR: 论文分析了AI模型文档（模型卡）的现状，发现开发者主要关注模型能力和可靠性，而忽略了其他伦理要求（如可解释性、用户自主权和公平性）。通过分析26份伦理指南、3个文档框架和10个实际模型卡，提出了一个包含43项伦理需求的分类法，为改进模型卡框架提供基础。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型文档实践不足，未能满足伦理法律和标准的要求，需填补这一差距。

Method: 对26份伦理指南、3个AI文档框架、3项定量研究和10个实际模型卡进行主题分析，识别并分类43项伦理需求。

Result: 开发者主要关注模型能力和可靠性，忽视了可解释性、用户自主权和公平性等伦理方面。

Conclusion: 提出了一个分类法，为全面满足伦理需求的模型卡框架奠定基础。

Abstract: Model cards are the primary documentation framework for developers of
artificial intelligence (AI) models to communicate critical information to
their users. Those users are often developers themselves looking for relevant
documentation to ensure that their AI systems comply with the ethical
requirements of existing laws, guidelines, and standards. Recent studies
indicate inadequate model documentation practices, suggesting a gap between AI
requirements and current practices in model documentation. To understand this
gap and provide actionable guidance to bridge it, we conducted a thematic
analysis of 26 guidelines on ethics and AI, three AI documentation frameworks,
three quantitative studies of model cards, and ten actual model cards. We
identified a total of 43 ethical requirements relevant to model documentation
and organized them into a taxonomy featuring four themes and twelve sub-themes
representing ethical principles. Our findings indicate that model developers
predominantly emphasize model capabilities and reliability in the documentation
while overlooking other ethical aspects, such as explainability, user autonomy,
and fairness. This underscores the need for enhanced support in documenting
ethical AI considerations. Our taxonomy serves as a foundation for a revised
model card framework that holistically addresses ethical AI requirements.

</details>


### [17] [CORE: Benchmarking LLMs Code Reasoning Capabilities through Static Analysis Tasks](https://arxiv.org/abs/2507.05269)
*Danning Xie,Mingwei Zheng,Xuwei Liu,Jiannan Wang,Chengpeng Wang,Lin Tan,Xiangyu Zhang*

Main category: cs.SE

TL;DR: CoRe是一个高质量、人工验证的基准测试，用于评估大型语言模型（LLMs）在静态分析任务中的表现，发现LLMs在深层语义理解和多步推理方面仍有不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注端到端结果（如代码修复或生成），而忽略了LLMs对程序语义推理能力的评估。

Method: 提出CoRe基准，包含12,553个任务实例，涵盖数据依赖、控制依赖和信息流，采用语义感知的多样化采样策略。

Result: 评估10种主流LLMs，发现其在依赖识别上表现良好，但在深层语义理解和多步推理任务中仍有困难。

Conclusion: 揭示了LLMs在复杂控制结构和反向依赖模式等挑战，为提升其代码推理能力提供了方向。

Abstract: Large language models (LLMs) have been widely adopted across diverse software
engineering domains, such as code generation, program repair, and vulnerability
detection. These applications require understanding beyond surface-level code
patterns: value propagation, control flow, and interdependence between program
elements. However, existing benchmarks primarily evaluate end-to-end outcomes,
such as whether code is correctly repaired or generated, leaving the models
ability for program semantic reasoning underexplored. This work presents CoRe,
a high-quality, human-verified benchmark designed to evaluate LLMs on
fundamental static analysis tasks. CoRe includes 12,553 task instances spanning
data dependency, control dependency, and information flow across programs
written in C/C++, Java, and Python. To ensure semantic diversity and reasoning
complexity, we propose a semantics-aware diverse sampling strategy that selects
targets and task instances based on structural coverage and dependency depth.
We evaluate 10 mainstream LLMs and show that, while they perform well at
identifying dependencies, models still struggle with tasks that require deeper
semantic understanding and multi-step reasoning. We further conduct qualitative
analyses to uncover key challenges, such as complex control structures and
backward dependency patterns, offering insights into improving LLMs code
reasoning capabilities.

</details>


### [18] [Open Source, Hidden Costs: A Systematic Literature Review on OSS License Management](https://arxiv.org/abs/2507.05270)
*Boyuan Li,Chengwei Liu,Lingling Fan,Sen Chen,Zhenlin Zhang,Zheli Liu*

Main category: cs.SE

TL;DR: 论文综述了第三方软件组件集成中的许可证风险问题，通过系统文献回顾（SLR）分析了80篇相关研究，分类为许可证识别、风险评估和缓解，并探讨了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 现代软件开发中广泛使用第三方组件，但许可证问题可能导致法律和运营风险，现有解决方案仍有局限，需系统性管理。

Method: 对80篇开源软件许可证相关论文进行系统文献回顾（SLR），分类为许可证识别、风险评估和缓解。

Result: 总结了现有研究的不足，提出了未来研究方向和实践建议。

Conclusion: 通过系统性综述，为学术界和工业界提供许可证风险管理的参考，推动合法软件风险的生态治理。

Abstract: Integrating third-party software components is a common practice in modern
software development, offering significant advantages in terms of efficiency
and innovation. However, this practice is fraught with risks related to
software licensing. A lack of understanding may lead to disputes, which can
pose serious legal and operational challenges. To these ends, both academia and
industry have conducted various investigations and proposed solutions and tools
to deal with these challenges. However, significant limitations still remain.
Moreover, the rapid evolution of open-source software (OSS) licenses, as well
as the rapidly incorporated generative software engineering techniques, such as
large language models for code (CodeLLMs), are placing greater demands on the
systematic management of software license risks. To unveil the severe
challenges and explore possible future directions, we conduct the first
systematic literature review (SLR) on 80 carefully selected OSS license-related
papers, classifying existing research into three key categories, i.e., license
identification, license risk assessment, and license risk mitigation. Based on
these, we discuss challenges in existing solutions, conclude the opportunities
to shed light on future research directions and offer practical recommendations
for practitioners. We hope this thorough review will help bridge the gaps
between academia and industry and accelerate the ecosystem-wide governance of
legitimate software risks within the software engineering community.

</details>


### [19] [FuzzFeed: An Automatic Approach to Weakest Precondition Generation using LLMs and Fuzzing](https://arxiv.org/abs/2507.05272)
*Daragh King,Vasileios Koutavas,Laura Kovacs*

Main category: cs.SE

TL;DR: 论文提出了一种结合大型语言模型（LLMs）和模糊测试的方法，用于生成程序的最弱前置条件（WP），并通过Fuzzing Guidance（FG）利用程序执行反馈指导LLMs生成正确的WP。


<details>
  <summary>Details</summary>
Motivation: 生成程序的最弱前置条件（WP）在验证和运行时错误检查等领域具有重要应用，但传统方法存在局限性。

Method: 结合LLMs和模糊测试，引入FG通过程序执行反馈指导LLMs生成WP，并利用模糊测试验证候选WP的有效性和弱性。

Result: 在Java确定性数组程序基准测试中，实验表明LLMs能够生成可行的候选WP，且FG能有效提升其能力。

Conclusion: LLMs结合FG的方法在生成WP方面具有实际应用潜力。

Abstract: The weakest precondition (WP) of a program describes the largest set of
initial states from which all terminating executions of the program satisfy a
given postcondition. The generation of WPs is an important task with practical
applications in areas ranging from verification to run-time error checking.
  This paper proposes the combination of Large Language Models (LLMs) and fuzz
testing for generating WPs. In pursuit of this goal, we introduce Fuzzing
Guidance (FG); FG acts as a means of directing LLMs towards correct WPs using
program execution feedback. FG utilises fuzz testing for approximately checking
the validity and weakness of candidate WPs, this information is then fed back
to the LLM as a means of context refinement.
  We demonstrate the effectiveness of our approach on a comprehensive benchmark
set of deterministic array programs in Java. Our experiments indicate that LLMs
are capable of producing viable candidate WPs, and that this ability can be
practically enhanced through FG.

</details>


### [20] [ReservoirChat: Interactive Documentation Enhanced with LLM and Knowledge Graph for ReservoirPy](https://arxiv.org/abs/2507.05279)
*Virgile Boraud,Yannis Bendi-Ouis,Paul Bernard,Xavier Hinaut*

Main category: cs.SE

TL;DR: 介绍了一个工具，通过结合外部知识（RAG和知识图谱）提升LLM在ReservoirPy库代码开发和储层计算领域问题解答中的能力，减少幻觉并提高准确性。


<details>
  <summary>Details</summary>
Motivation: 提升LLM在特定领域（如ReservoirPy和储层计算）的辅助能力，减少生成内容中的错误。

Method: 采用检索增强生成（RAG）和知识图谱结合的方法，优化模型在代码开发和领域问题解答中的表现。

Result: 在编码任务上优于ChatGPT-4o和NotebookLM，且显著优于其基础模型Codestral-22B。

Conclusion: 该工具在特定领域任务中表现出色，尤其在代码开发和领域知识解答方面具有优势。

Abstract: We introduce a tool designed to improve the capabilities of Large Language
Models (LLMs) in assisting with code development using the ReservoirPy library,
as well as in answering complex questions in the field of Reservoir Computing.
By incorporating external knowledge through Retrieval-Augmented Generation
(RAG) and knowledge graphs, our approach aims to reduce hallucinations and
increase the factual accuracy of generated responses. The system provides an
interactive experience similar to ChatGPT, tailored specifically for
ReservoirPy, enabling users to write, debug, and understand Python code while
accessing reliable domain-specific insights. In our evaluation, while
proprietary models such as ChatGPT-4o and NotebookLM performed slightly better
on general knowledge questions, our model outperformed them on coding tasks and
showed a significant improvement over its base model, Codestral-22B.

</details>


### [21] [CoreCodeBench: A Configurable Multi-Scenario Repository-Level Benchmark](https://arxiv.org/abs/2507.05281)
*Lingyue Fu,Hao Guan,Bolun Zhang,Haowei Yuan,Yaoming Zhu,Jun Xu,Zongyu Wang,Lin Qiu,Xunliang Cai,Xuezhi Cao,Weiwen Liu,Weinan Zhang,Yong Yu*

Main category: cs.SE

TL;DR: CorePipe和CoreCodeBench提出了一种自动化管道和可配置的多场景仓库级基准测试，用于评估LLM在真实工程环境中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试局限于单一场景，无法全面反映真实软件工程的多样性和复杂性，且存在可控性和可靠性问题。

Method: CorePipe将仓库转换为综合测试用例，生成三类原子问题（开发、修复、测试驱动开发），并通过超参数调整组合为复合问题。CoreCodeBench作为基准测试工具。

Result: 实验评估了16种LLM，揭示了它们在工程场景中的不同能力，提供了多维度的性能洞察。

Conclusion: CoreCodeBench为LLM在真实工程项目的适用性提供了全面评估工具，代码和数据已开源。

Abstract: As Large Language Models (LLMs) demonstrate increasingly sophisticated code
processing capabilities, evaluating their performance on engineering-level code
remains challenging. Existing repository-level benchmarks primarily focus on
single scenarios, such as code generation or bug fixing, without adequately
capturing the diversity and complexity of real-world software or project
engineering workflows. Furthermore, these benchmarks suffer from limited
controllability in question positioning and reliability issues in their
generated test cases. To address these limitations, we present CorePipe, a
fully automated pipeline that converts repositories into comprehensive test
cases, and introduce CoreCodeBench, a configurable multi-scenario
repository-level benchmark. To simulate real engineering scenarios, CorePipe
generates three types of atomic questions (Development, BugFix, and Test-Driven
Development) specifically targeting core code segments. These atomic questions
are further combined into three types of composite questions, with difficulty
levels flexibly adjusted through hyperparameter tuning. CoreCodeBench provides
a comprehensive and extensive repository-level benchmark to investigate the
applicability of LLMs in real-world engineering projects. Experiments with 16
LLMs across diverse scenarios reveal varying capabilities and offer
multi-dimensional insights into LLM performance in engineering contexts. The
code for CorePipe is available at
https://github.com/AGI-Eval-Official/CoreCodeBench, and the data for
CoreCodeBench can be accessed at
https://huggingface.co/collections/tubehhh/corecodebench-68256d2faabf4b1610a08caa.

</details>


### [22] [Measuring how changes in code readability attributes affect code quality evaluation by Large Language Models](https://arxiv.org/abs/2507.05289)
*Igor Regis da Silva Simoes,Elaine Venson*

Main category: cs.SE

TL;DR: 该论文探讨了使用大型语言模型（LLMs）评估代码可读性，通过实验发现LLMs对代码干预敏感，并显示出评估语义质量的潜力。


<details>
  <summary>Details</summary>
Motivation: 代码可读性是代码质量的重要方面，但现有评估方法存在主观性和不一致性。论文旨在探索LLMs能否提供标准化、可复现的评估方式。

Method: 通过准实验研究，测试9种LLMs对三种代码干预（删除注释、替换标识符名称、重构代码）的反应，并与参考模型对比。

Result: LLMs对干预敏感，与参考模型在原始和重构代码场景中高度一致，且显示出语义敏感性。响应存在一定变异性，但不影响统计显著性。

Conclusion: LLMs在评估代码语义质量方面具有潜力，尤其是在标识符名称、注释与代码目的的连贯性方面。

Abstract: Code readability is one of the main aspects of code quality, influenced by
various properties like identifier names, comments, code structure, and
adherence to standards. However, measuring this attribute poses challenges in
both industry and academia. While static analysis tools assess attributes such
as code smells and comment percentage, code reviews introduce an element of
subjectivity. This paper explores using Large Language Models (LLMs) to
evaluate code quality attributes related to its readability in a standardized,
reproducible, and consistent manner. We conducted a quasi-experiment study to
measure the effects of code changes on Large Language Model (LLM)s
interpretation regarding its readability quality attribute. Nine LLMs were
tested, undergoing three interventions: removing comments, replacing identifier
names with obscure names, and refactoring to remove code smells. Each
intervention involved 10 batch analyses per LLM, collecting data on response
variability. We compared the results with a known reference model and tool. The
results showed that all LLMs were sensitive to the interventions, with
agreement with the reference classifier being high for the original and
refactored code scenarios. The LLMs demonstrated a strong semantic sensitivity
that the reference model did not fully capture. A thematic analysis of the LLMs
reasoning confirmed their evaluations directly reflected the nature of each
intervention. The models also exhibited response variability, with 9.37% to
14.58% of executions showing a standard deviation greater than zero, indicating
response oscillation, though this did not always compromise the statistical
significance of the results. LLMs demonstrated potential for evaluating
semantic quality aspects, such as coherence between identifier names, comments,
and documentation with code purpose.

</details>


### [23] [zkSDK: Streamlining zero-knowledge proof development through automated trace-driven ZK-backend selection](https://arxiv.org/abs/2507.05294)
*William Law*

Main category: cs.SE

TL;DR: zkSDK是一个模块化框架，旨在简化零知识（ZK）应用开发，通过抽象后端复杂性并提供动态选择最优ZK后端的功能。


<details>
  <summary>Details</summary>
Motivation: 当前ZK开发工具众多且分散，开发者面临学习曲线陡峭和平台碎片化问题，导致他们通常只能选择并依赖单一后端。

Method: zkSDK引入Presto（一种类似Python的自定义语言）用于程序分析和计算负载评估，结合用户定义标准，动态选择最优ZK后端。

Result: 通过实际工作负载分析，zkSDK能有效选择最适合的后端，提供无缝且用户友好的开发体验。

Conclusion: zkSDK通过模块化和自动化后端选择，显著改善了ZK开发的碎片化问题，提升了开发效率。

Abstract: The rapid advancement of creating Zero-Knowledge (ZK) programs has led to the
development of numerous tools designed to support developers. Popular options
include being able to write in general-purpose programming languages like Rust
from Risc Zero. Other languages exist like Circom, Lib-snark, and Cairo.
However, developers entering the ZK space are faced with many different ZK
backends to choose from, leading to a steep learning curve and a fragmented
developer experience across different platforms. As a result, many developers
tend to select a single ZK backend and remain tied to it. This thesis
introduces zkSDK, a modular framework that streamlines ZK application
development by abstracting the backend complexities. At the core of zkSDK is
Presto, a custom Python-like programming language that enables the profiling
and analysis of a program to assess its computational workload intensity.
Combined with user-defined criteria, zkSDK employs a dynamic selection
algorithm to automatically choose the optimal ZK-proving backend. Through an
in-depth analysis and evaluation of real-world workloads, we demonstrate that
zkSDK effectively selects the best-suited backend from a set of supported ZK
backends, delivering a seamless and user-friendly development experience.

</details>


### [24] [ASSURE: Metamorphic Testing for AI-powered Browser Extensions](https://arxiv.org/abs/2507.05307)
*Xuanqi Gao,Juan Zhai,Shiqing Ma,Siyi Xie,Chao Shen*

Main category: cs.SE

TL;DR: ASSURE是一个专为AI浏览器扩展设计的自动化测试框架，解决了传统方法难以应对的非确定性行为和复杂环境集成问题。


<details>
  <summary>Details</summary>
Motivation: 传统浏览器扩展测试方法无法处理LLM驱动的扩展的非确定性和上下文敏感性，现有LLM测试方法也缺乏浏览器环境集成，亟需新框架。

Method: ASSURE包含三个组件：模块化测试用例生成引擎、自动化执行框架和可配置验证管道，系统评估行为一致性和安全性。

Result: 评估显示ASSURE在六个常用AI扩展中发现531个问题，测试吞吐量比手动方法高6.4倍，平均12.4分钟检测到关键漏洞。

Conclusion: ASSURE为AI浏览器扩展测试提供了高效、全面的解决方案，适合集成到开发流程中。

Abstract: The integration of Large Language Models (LLMs) into browser extensions has
revolutionized web browsing, enabling sophisticated functionalities like
content summarization, intelligent translation, and context-aware writing
assistance. However, these AI-powered extensions introduce unprecedented
challenges in testing and reliability assurance. Traditional browser extension
testing approaches fail to address the non-deterministic behavior,
context-sensitivity, and complex web environment integration inherent to
LLM-powered extensions. Similarly, existing LLM testing methodologies operate
in isolation from browser-specific contexts, creating a critical gap in
effective evaluation frameworks. To bridge this gap, we present ASSURE, a
modular automated testing framework specifically designed for AI-powered
browser extensions. ASSURE comprises three principal components: (1) a modular
test case generation engine that supports plugin-based extension of testing
scenarios, (2) an automated execution framework that orchestrates the complex
interactions between web content, extension processing, and AI model behavior,
and (3) a configurable validation pipeline that systematically evaluates
behavioral consistency and security invariants rather than relying on exact
output matching. Our evaluation across six widely-used AI browser extensions
demonstrates ASSURE's effectiveness, identifying 531 distinct issues spanning
security vulnerabilities, metamorphic relation violations, and content
alignment problems. ASSURE achieves 6.4x improved testing throughput compared
to manual approaches, detecting critical security vulnerabilities within 12.4
minutes on average. This efficiency makes ASSURE practical for integration into
development pipelines, offering a comprehensive solution to the unique
challenges of testing AI-powered browser extensions.

</details>


### [25] [OASBuilder: Generating OpenAPI Specifications from Online API Documentation with Large Language Models](https://arxiv.org/abs/2507.05316)
*Koren Lazar,Matan Vetzler,Kiran Kate,Jason Tsay,David Boaz Himanshu Gupta,Avraham Shinnar,Rohith D Vallam,David Amid Esther Goldbraich,Guy Uziel,Jim Laredo,Ateret Anaby Tavor*

Main category: cs.SE

TL;DR: OASBuilder是一个框架，将非结构化的API文档转换为机器可读的OpenAPI规范，结合了大型语言模型和基于规则的算法。


<details>
  <summary>Details</summary>
Motivation: 在线API文档多为非结构化HTML，需要手动转换为结构化格式，耗时且低效。

Method: 通过整合大型语言模型和基于规则的算法，结合文档网页结构的领域知识，构建了一个转换管道。

Result: 实验表明，OASBuilder能有效处理数百种API，生成包含大部分原始文档信息的有效OpenAPI规范。

Conclusion: OASBuilder在企业环境中成功应用，节省了大量手动时间，使复杂API更易于LLM使用。

Abstract: AI agents and business automation tools interacting with external web
services require standardized, machine-readable information about their APIs in
the form of API specifications. However, the information about APIs available
online is often presented as unstructured, free-form HTML documentation,
requiring external users to spend significant time manually converting it into
a structured format. To address this, we introduce OASBuilder, a novel
framework that transforms long and diverse API documentation pages into
consistent, machine-readable API specifications. This is achieved through a
carefully crafted pipeline that integrates large language models and rule-based
algorithms which are guided by domain knowledge of the structure of
documentation webpages. Our experiments demonstrate that OASBuilder generalizes
well across hundreds of APIs, and produces valid OpenAPI specifications that
encapsulate most of the information from the original documentation. OASBuilder
has been successfully implemented in an enterprise environment, saving
thousands of hours of manual effort and making hundreds of complex enterprise
APIs accessible as tools for LLMs.

</details>


### [26] [Exploring Empathy in Software Engineering: Insights from a Grey Literature Analysis of Practitioners' Perspectives](https://arxiv.org/abs/2507.05325)
*Lidiany Cerqueira,João Pedro Bastos,Danilo Neves,Glauco Carneiro,Rodrigo Spínola,Sávio Freire,José Amancio Macedo Santos,Manoel Mendonça*

Main category: cs.SE

TL;DR: 研究探讨了软件开发中同理心的定义、障碍、实践及效果，提出了一个概念框架。


<details>
  <summary>Details</summary>
Motivation: 同理心在软件开发中至关重要，但研究不足，需从实践者角度深入探讨。

Method: 对55篇DEV和Medium文章进行定性内容分析，并辅以专家调查验证。

Result: 提出了同理心的定义，识别了障碍（如毒性文化、过度技术导向），并总结了提升同理心的实践及其积极效果。

Conclusion: 框架清晰且有价值，未来将探索同理心在软件开发中的更广泛应用。

Abstract: Context. Empathy, a key social skill, is essential for communication and
collaboration in SE but remains an under-researched topic. Aims. This study
investigates empathy in SE from practitioners' perspectives, aiming to
characterize its meaning, identify barriers, discuss practices to overcome
them, and explore its effects. Method. A qualitative content analysis was
conducted on 55 web articles from DEV and Medium, two communities widely used
by practitioners. To strengthen our findings, we conducted a follow-up survey
with empathy experts. Results. The study proposes a definition of empathy in
SE, identifies barriers such as toxic culture and excessive technical focus,
practices to foster empathy in teams, and outcomes, including improved
collaboration, communication, and reduced anxiety, frustration, and stress.
These findings are synthesized into a conceptual framework. Conclusion. Survey
results indicate the framework is clear, valuable, and raises empathy
awareness, with suggestions for improvements and integration into training.
This study paves the way for improving team dynamics by addressing barriers and
offering strategies to cultivate empathy. Future work will explore empathy's
broader implications in SE practice.

</details>


### [27] [Tool for Supporting Debugging and Understanding of Normative Requirements Using LLMs](https://arxiv.org/abs/2507.05504)
*Alex Kleijwegt,Sinem Getir Yaman,Radu Calinescu*

Main category: cs.SE

TL;DR: SLEEC-LLM利用大语言模型（LLM）为SLEEC规则不一致性提供自然语言解释，提升规范需求获取和一致性分析的效率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 规范需求（SLEEC）的识别和管理复杂且易错，现有方法对非技术用户不友好，导致效率低下。

Method: 开发SLEEC-LLM工具，利用LLM将形式验证结果转化为自然语言解释。

Result: SLEEC-LLM在真实案例中有效提升了需求获取和验证的效率。

Conclusion: SLEEC-LLM为非技术用户提供了更易理解的工具，优化了规范需求的管理流程。

Abstract: Normative requirements specify social, legal, ethical, empathetic, and
cultural (SLEEC) norms that must be observed by a system. To support the
identification of SLEEC requirements, numerous standards and regulations have
been developed. These requirements are typically defined by stakeholders in the
non-technical system with diverse expertise (e.g., ethicists, lawyers, social
scientists). Hence, ensuring their consistency and managing the requirement
elicitation process are complex and error-prone tasks. Recent research has
addressed this challenge using domain-specific languages to specify normative
requirements as rules, whose consistency can then be analyzed with formal
methods. Nevertheless, these approaches often present the results from formal
verification tools in a way that is inaccessible to non-technical users. This
hinders understanding and makes the iterative process of eliciting and
validating these requirements inefficient in terms of both time and effort. To
address this problem, we introduce SLEEC-LLM, a tool that uses large language
models (LLMs) to provide natural-language interpretations for model-checking
counterexamples corresponding to SLEEC rule inconsistencies. SLEEC-LLM improves
the efficiency and explainability of normative requirements elicitation and
consistency analysis. To demonstrate its effectiveness, we summarise its use in
two real-world case studies involving non-technical stakeholders.

</details>


### [28] [Search-based Selection of Metamorphic Relations for Optimized Robustness Testing of Large Language Models](https://arxiv.org/abs/2507.05565)
*Sangwon Hyun,Shaukat Ali,M. Ali Babar*

Main category: cs.SE

TL;DR: 本文提出了一种基于搜索的方法，用于优化大语言模型（LLM）的鲁棒性测试中的变形关系（MR）选择，以最大化故障检测并最小化执行成本。实验表明MOEA/D算法表现最佳，并发现了在多种任务中表现优越的MR。


<details>
  <summary>Details</summary>
Motivation: 现有LLM鲁棒性测试方法需要大量MR且测试空间有限，亟需优化MR选择和扩展测试空间。

Method: 提出搜索方法，实现四种搜索算法（Single-GA、NSGA-II、SPEA2、MOEA/D）优化MR选择，覆盖组合扰动。

Result: MOEA/D算法表现最佳，并发现了一些在多种任务中表现优越的MR。

Conclusion: 研究为LLM鲁棒性测试提供了优化方法和搜索解决方案的见解。

Abstract: Assessing the trustworthiness of Large Language Models (LLMs), such as
robustness, has garnered significant attention. Recently, metamorphic testing
that defines Metamorphic Relations (MRs) has been widely applied to evaluate
the robustness of LLM executions. However, the MR-based robustness testing
still requires a scalable number of MRs, thereby necessitating the optimization
of selecting MRs. Most extant LLM testing studies are limited to automatically
generating test cases (i.e., MRs) to enhance failure detection. Additionally,
most studies only considered a limited test space of single perturbation MRs in
their evaluation of LLMs. In contrast, our paper proposes a search-based
approach for optimizing the MR groups to maximize failure detection and
minimize the LLM execution cost. Moreover, our approach covers the
combinatorial perturbations in MRs, facilitating the expansion of test space in
the robustness assessment. We have developed a search process and implemented
four search algorithms: Single-GA, NSGA-II, SPEA2, and MOEA/D with novel
encoding to solve the MR selection problem in the LLM robustness testing. We
conducted comparative experiments on the four search algorithms along with a
random search, using two major LLMs with primary Text-to-Text tasks. Our
statistical and empirical investigation revealed two key findings: (1) the
MOEA/D algorithm performed the best in optimizing the MR space for LLM
robustness testing, and (2) we identified silver bullet MRs for the LLM
robustness testing, which demonstrated dominant capabilities in confusing LLMs
across different Text-to-Text tasks. In LLM robustness assessment, our research
sheds light on the fundamental problem for optimized testing and provides
insights into search-based solutions.

</details>


### [29] [TigAug: Data Augmentation for Testing Traffic Light Detection in Autonomous Driving Systems](https://arxiv.org/abs/2507.05932)
*You Lu,Dingji Wang,Kaifeng Huang,Bihuan Chen,Xin Peng*

Main category: cs.SE

TL;DR: 论文提出TigAug方法，通过自动增强标记的交通灯图像来测试和改进自动驾驶系统中的交通灯检测模型。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统（ADS）中交通灯检测模型的自动化测试研究不足，传统手动数据收集方法效率低且难以覆盖多样环境。

Method: 基于天气、相机和交通灯特性，构建了两类变形关系和三类变换，通过增强图像检测模型错误行为并提升模型性能。

Result: 实验表明TigAug能有效测试模型、高效合成图像，且生成图像具有可接受的自然度。

Conclusion: TigAug为交通灯检测模型的测试和性能提升提供了自动化解决方案。

Abstract: Autonomous vehicle technology has been developed in the last decades with
recent advances in sensing and computing technology. There is an urgent need to
ensure the reliability and robustness of autonomous driving systems (ADSs).
Despite the recent achievements in testing various ADS modules, little
attention has been paid on the automated testing of traffic light detection
models in ADSs. A common practice is to manually collect and label traffic
light data. However, it is labor-intensive, and even impossible to collect
diverse data under different driving environments.
  To address these problems, we propose and implement TigAug to automatically
augment labeled traffic light images for testing traffic light detection models
in ADSs. We construct two families of metamorphic relations and three families
of transformations based on a systematic understanding of weather environments,
camera properties, and traffic light properties. We use augmented images to
detect erroneous behaviors of traffic light detection models by
transformation-specific metamorphic relations, and to improve the performance
of traffic light detection models by retraining. Large-scale experiments with
four state-of-the-art traffic light detection models and two traffic light
datasets have demonstrated that i) TigAug is effective in testing traffic light
detection models, ii) TigAug is efficient in synthesizing traffic light images,
and iii) TigAug generates traffic light images with acceptable naturalness.

</details>


### [30] [Multi-Agent Debate Strategies to Enhance Requirements Engineering with Large Language Models](https://arxiv.org/abs/2507.05981)
*Marc Oriol,Quim Motger,Jordi Marco,Xavier Franch*

Main category: cs.SE

TL;DR: 论文探讨了通过多智能体辩论（MAD）策略提升大型语言模型（LLM）在需求工程（RE）任务中的准确性，提出了一种初步框架并验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 现有方法将LLM视为孤立黑箱，缺乏迭代优化和协作，限制了鲁棒性和适应性。受人类辩论启发，研究MAD策略是否能提升RE任务的性能。

Method: 系统研究现有MAD策略，分类其核心特征，并初步实现了一个基于MAD的RE分类框架进行测试。

Result: 研究总结了MAD策略的分类体系，初步评估显示MAD在RE分类中具有应用潜力。

Conclusion: MAD是提升LLM在RE任务中准确性的有前景方法，为未来研究和应用提供了基础。

Abstract: Context: Large Language Model (LLM) agents are becoming widely used for
various Requirements Engineering (RE) tasks. Research on improving their
accuracy mainly focuses on prompt engineering, model fine-tuning, and retrieval
augmented generation. However, these methods often treat models as isolated
black boxes - relying on single-pass outputs without iterative refinement or
collaboration, limiting robustness and adaptability. Objective: We propose
that, just as human debates enhance accuracy and reduce bias in RE tasks by
incorporating diverse perspectives, different LLM agents debating and
collaborating may achieve similar improvements. Our goal is to investigate
whether Multi-Agent Debate (MAD) strategies can enhance RE performance. Method:
We conducted a systematic study of existing MAD strategies across various
domains to identify their key characteristics. To assess their applicability in
RE, we implemented and tested a preliminary MAD-based framework for RE
classification. Results: Our study identified and categorized several MAD
strategies, leading to a taxonomy outlining their core attributes. Our
preliminary evaluation demonstrated the feasibility of applying MAD to RE
classification. Conclusions: MAD presents a promising approach for improving
LLM accuracy in RE tasks. This study provides a foundational understanding of
MAD strategies, offering insights for future research and refinements in RE
applications.

</details>


### [31] [PromiseTune: Unveiling Causally Promising and Explainable Configuration Tuning](https://arxiv.org/abs/2507.05995)
*Pengzhou Chen,Tao Chen*

Main category: cs.SE

TL;DR: PromiseTune通过因果推断纯化的规则指导配置调优，有效平衡探索与利用，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现代软件系统的高可配置性使配置调优成为确保性能的关键步骤，但现有调优工具因探索与利用的平衡问题效果不佳。

Method: 提出PromiseTune，通过学习并因果纯化规则，界定有潜力的配置区域，指导调优过程。

Result: 在12个系统和不同预算下，PromiseTune表现显著优于11种现有方法，排名优势达42%。

Conclusion: PromiseTune不仅提升了调优效果，还通过纯化区域提供了配置空间的解释性。

Abstract: The high configurability of modern software systems has made configuration
tuning a crucial step for assuring system performance, e.g., latency or
throughput. However, given the expensive measurements, large configuration
space, and rugged configuration landscape, existing tuners suffer
ineffectiveness due to the difficult balance of budget utilization between
exploring uncertain regions (for escaping from local optima) and exploiting
guidance of known good configurations (for fast convergence). The root cause is
that we lack knowledge of where the promising regions lay, which also causes
challenges in the explainability of the results.
  In this paper, we propose PromiseTune that tunes configuration guided by
causally purified rules. PromiseTune is unique in the sense that we learn
rules, which reflect certain regions in the configuration landscape, and purify
them with causal inference. The remaining rules serve as approximated
reflections of the promising regions, bounding the tuning to emphasize these
places in the landscape. This, as we demonstrate, can effectively mitigate the
impact of the exploration and exploitation trade-off. Those purified regions
can then be paired with the measured configurations to provide spatial
explainability at the landscape level. Comparing with 11 state-of-the-art
tuners on 12 systems and varying budgets, we show that PromiseTune performs
significantly better than the others with $42\%$ superior rank to the overall
second best while providing richer information to explain the hidden system
characteristics.

</details>


### [32] [Model Cards Revisited: Bridging the Gap Between Theory and Practice for Ethical AI Requirements](https://arxiv.org/abs/2507.06014)
*Tim Puhlfürß,Julia Butzke,Walid Maalej*

Main category: cs.SE

TL;DR: 论文分析了AI模型文档（模型卡）的现状，发现开发者主要关注模型能力和可靠性，而忽略了其他伦理要求。通过分析26份伦理指南和实际模型卡，提出了一个包含43项伦理要求的分类法，为改进模型卡框架提供基础。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型文档实践不足，存在伦理要求与实际文档内容之间的差距，需要提供可操作的指导来弥合这一差距。

Method: 对26份伦理和AI指南、3个AI文档框架、3项模型卡定量研究和10份实际模型卡进行了主题分析。

Result: 识别出43项与模型文档相关的伦理要求，并将其分类为4个主题和12个子主题。发现开发者主要关注模型能力和可靠性，忽略了其他伦理方面。

Conclusion: 需要改进模型卡框架以全面满足伦理要求，提出的分类法为此提供了基础。

Abstract: Model cards are the primary documentation framework for developers of
artificial intelligence (AI) models to communicate critical information to
their users. Those users are often developers themselves looking for relevant
documentation to ensure that their AI systems comply with the ethical
requirements of existing laws, guidelines, and standards. Recent studies
indicate inadequate model documentation practices, suggesting a gap between AI
requirements and current practices in model documentation. To understand this
gap and provide actionable guidance to bridge it, we conducted a thematic
analysis of 26 guidelines on ethics and AI, three AI documentation frameworks,
three quantitative studies of model cards, and ten actual model cards. We
identified a total of 43 ethical requirements relevant to model documentation
and organized them into a taxonomy featuring four themes and twelve sub-themes
representing ethical principles. Our findings indicate that model developers
predominantly emphasize model capabilities and reliability in the documentation
while overlooking other ethical aspects, such as explainability, user autonomy,
and fairness. This underscores the need for enhanced support in documenting
ethical AI considerations. Our taxonomy serves as a foundation for a revised
model card framework that holistically addresses ethical AI requirements.

</details>
